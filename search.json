[
  {
    "objectID": "theme4/XS101/python-exercises.html",
    "href": "theme4/XS101/python-exercises.html",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "Wherever you see &lt;your CLASSE username&gt; below, substitute your own CLASSE username.\n\n\nFollowing the Linux exercises for CHESS users: 1. Open a terminal on lnx201 1. Change directory to your CHESS user directory: cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\n\n\nIn your CHESS user directory, type following command to copy three files (file.txt, file.data, and file.json) to your current directory (.):\ncp /nfs/chess/user/x-cite/data/python/* .\n[Optional] Instead of copying the above files, type the following commands to create your own from scratch:\n# file.txt (file with a text)\necho 'Hello world!' &gt; file.txt\n\n# file.data (file with numeric array)\necho '[1,2,3,4,5]' &gt; file.data\n\n# file.json (file with json data)\necho '{\"int\": 1, \"array\": [1,2,3], \"dictionary\": {\"key\":\"value\"}}' &gt; file.json\nNow, view and check the contents of each file using cat, less, or more.\n\n\n\nThis exercise demonstrates some of the basic steps of any data analysis: reading data, processing or interacting with data, writing data. These steps can be iterated any number of times and combined in many ways.\nNote the conceptual similarity between these data analysis pipelines and the Linux pipes demonstrated in the Linux exercises (taking one command output and pass it to another, e.g. env | grep USER`).\nThe following code will walk you through the following steps: - how to open a file in python - how to read data from a file - become familiar with different data structures: string, array, dictionary, etc. - how to manipulate the data (e.g. perform some analysis) - how to mix different data structures together - how to write data to a file\nLet’s proceed with basic analysis using python: 1. Type python (in the same directory where the files you created above are located). This will open an interactive python session. 1. Type the commands below and observe the output:\n# open file\nopen('file.txt')\n\n# open file and assign it to a file descriptor\nfds = open('file.txt')\n\n# read data from our file descriptor\ndata = fds.read()\nprint(\"text data:\", data)\n\n# close the file descriptor\nfds.close()\n\n# load data from a data file\ndata = open('file.data', 'r').read()\nprint(\"structural data:\", data)\n\n# load json data from a json data file\nimport json\ndata = json.load(open('file.json', 'r'))\nprint(\"json data:\", data)\n\n# let's perform data analysis on our data and enhance it further\nif 'array' in data:\n    arr = data['array']\n    for item in arr:\n        print(\"array item\", item)\n    data['sum'] = sum(arr)\n\n# let's create our own data and write it back to new file\nwith open('analysis.json', 'w') as ostream:\n    ostream.write(json.dumps(data))\n\nType Ctrl-D to exit the interactive python session\n\n\n\n\nA virtual environment is a self-contained directory that contains a Python installation for a particular project, along with all its packages. It isolates project dependencies, ensuring that different projects don’t interfere with each other’s libraries and versions.\nWhy use it? - Avoid dependency conflicts between projects. - Safely test upgrades without affecting system Python. - Clean, reproducible development environments.\n\n\n\n\nInstall virtualenv (optional) Python 3.x comes with venv built-in command which we can use to create and setup virtual environment\n\npython3 -m venv myenv\n\nmyenv is the name of the folder where the environment will be created.\n\n\nActivate the Virtual Environment\n\n\nOn Linux/macOS:\nsource myenv/bin/activate\nOn Windows (cmd.exe):\nmyenv\\Scripts\\activate\nOn Windows (PowerShell):\n.\\myenv\\Scripts\\Activate.ps1\n\nOnce activated, your shell prompt will change to show the environment name, e.g., (myenv).\n\n\n\n\nWhile the virtual environment is activated, you can install packages normally using pip:\npip install requests\n\nExample: Installing a specific version:\npip install requests==2.28.1\n\nAll installed packages will now live inside the myenv directory.\n\n\n\n\nWhen you’re done working inside the virtual environment, simply run:\ndeactivate\nThis will return you to the original system Python environment.\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nCreate venv\npython3 -m venv myenv\n\n\nActivate venv\nsource myenv/bin/activate (Linux/macOS)\n\n\nInstall pkg\npip install &lt;package&gt;\n\n\nDeactivate\ndeactivate"
  },
  {
    "objectID": "theme4/XS101/python-exercises.html#exercise-1-log-in-and-navigate-to-chess-user-directory",
    "href": "theme4/XS101/python-exercises.html#exercise-1-log-in-and-navigate-to-chess-user-directory",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "Following the Linux exercises for CHESS users: 1. Open a terminal on lnx201 1. Change directory to your CHESS user directory: cd /nfs/chess/user/&lt;your CLASSE username&gt;"
  },
  {
    "objectID": "theme4/XS101/python-exercises.html#exercise-2-create-input-files",
    "href": "theme4/XS101/python-exercises.html#exercise-2-create-input-files",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "In your CHESS user directory, type following command to copy three files (file.txt, file.data, and file.json) to your current directory (.):\ncp /nfs/chess/user/x-cite/data/python/* .\n[Optional] Instead of copying the above files, type the following commands to create your own from scratch:\n# file.txt (file with a text)\necho 'Hello world!' &gt; file.txt\n\n# file.data (file with numeric array)\necho '[1,2,3,4,5]' &gt; file.data\n\n# file.json (file with json data)\necho '{\"int\": 1, \"array\": [1,2,3], \"dictionary\": {\"key\":\"value\"}}' &gt; file.json\nNow, view and check the contents of each file using cat, less, or more."
  },
  {
    "objectID": "theme4/XS101/python-exercises.html#exercise-3-interact-with-files",
    "href": "theme4/XS101/python-exercises.html#exercise-3-interact-with-files",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "This exercise demonstrates some of the basic steps of any data analysis: reading data, processing or interacting with data, writing data. These steps can be iterated any number of times and combined in many ways.\nNote the conceptual similarity between these data analysis pipelines and the Linux pipes demonstrated in the Linux exercises (taking one command output and pass it to another, e.g. env | grep USER`).\nThe following code will walk you through the following steps: - how to open a file in python - how to read data from a file - become familiar with different data structures: string, array, dictionary, etc. - how to manipulate the data (e.g. perform some analysis) - how to mix different data structures together - how to write data to a file\nLet’s proceed with basic analysis using python: 1. Type python (in the same directory where the files you created above are located). This will open an interactive python session. 1. Type the commands below and observe the output:\n# open file\nopen('file.txt')\n\n# open file and assign it to a file descriptor\nfds = open('file.txt')\n\n# read data from our file descriptor\ndata = fds.read()\nprint(\"text data:\", data)\n\n# close the file descriptor\nfds.close()\n\n# load data from a data file\ndata = open('file.data', 'r').read()\nprint(\"structural data:\", data)\n\n# load json data from a json data file\nimport json\ndata = json.load(open('file.json', 'r'))\nprint(\"json data:\", data)\n\n# let's perform data analysis on our data and enhance it further\nif 'array' in data:\n    arr = data['array']\n    for item in arr:\n        print(\"array item\", item)\n    data['sum'] = sum(arr)\n\n# let's create our own data and write it back to new file\nwith open('analysis.json', 'w') as ostream:\n    ostream.write(json.dumps(data))\n\nType Ctrl-D to exit the interactive python session"
  },
  {
    "objectID": "theme4/XS101/python-exercises.html#exercise-4-set-up-python-virtual-environment",
    "href": "theme4/XS101/python-exercises.html#exercise-4-set-up-python-virtual-environment",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "A virtual environment is a self-contained directory that contains a Python installation for a particular project, along with all its packages. It isolates project dependencies, ensuring that different projects don’t interfere with each other’s libraries and versions.\nWhy use it? - Avoid dependency conflicts between projects. - Safely test upgrades without affecting system Python. - Clean, reproducible development environments.\n\n\n\n\nInstall virtualenv (optional) Python 3.x comes with venv built-in command which we can use to create and setup virtual environment\n\npython3 -m venv myenv\n\nmyenv is the name of the folder where the environment will be created.\n\n\nActivate the Virtual Environment\n\n\nOn Linux/macOS:\nsource myenv/bin/activate\nOn Windows (cmd.exe):\nmyenv\\Scripts\\activate\nOn Windows (PowerShell):\n.\\myenv\\Scripts\\Activate.ps1\n\nOnce activated, your shell prompt will change to show the environment name, e.g., (myenv).\n\n\n\n\nWhile the virtual environment is activated, you can install packages normally using pip:\npip install requests\n\nExample: Installing a specific version:\npip install requests==2.28.1\n\nAll installed packages will now live inside the myenv directory.\n\n\n\n\nWhen you’re done working inside the virtual environment, simply run:\ndeactivate\nThis will return you to the original system Python environment.\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nCreate venv\npython3 -m venv myenv\n\n\nActivate venv\nsource myenv/bin/activate (Linux/macOS)\n\n\nInstall pkg\npip install &lt;package&gt;\n\n\nDeactivate\ndeactivate"
  },
  {
    "objectID": "theme4/XS101/data-analysis.html",
    "href": "theme4/XS101/data-analysis.html",
    "title": "Hands-On Exercises",
    "section": "",
    "text": "Hands-On Exercises\nThe following hands-on exercises demonstrate various methods of analyzing and viewing x-ray detector images. All exercises (except the last one) require an activated CLASSE account and access to the CLASSE JupyterHub.\n\nBasic skills: Linux at CHESS, Python programming at CHESS\n\nSkills: Linux command line, navigating CHESS filesystms, Python\n\nData analysis (low complexity): Azimuthal integration of 2D diffraction patterns\n\nSkills: CHAP, Python, Linux command line, navigating CHESS filesystems, Jupyter notebooks, matplotlib\n\nData analysis (medium complexity): Tomographic reconstruction\n\nSkills: CHAP, Python, Linux command line, navigating CHESS filesystems, NoMachine, NeXpy, Galaxy\n\nViewing metadata (demo): Web tutorial\n\nSkills: navigating and searching for CHESS datasets\n\nScientific workflows (high complexity): Pegasus workflow management system\n\nSkills: setting up and running Pegasus workflows at CHESS"
  },
  {
    "objectID": "theme4/XS100/data-collection.html#chess-experimental-stations",
    "href": "theme4/XS100/data-collection.html#chess-experimental-stations",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "CHESS Experimental Stations",
    "text": "CHESS Experimental Stations\nData collection at all the CHESS experimental stations is supported by a complex controls, software, hardware, and cyberinfrastructure ecosystem. Having a basic understanding of this system and some relevant computing and software literacy will help prepare you for your beamtime. Historically, we have performed these trainings on the day of arrival, but preparation ahead of the beamtime will allow you to be more engaged with decisions on your experiment and focus on producing the highest quality experiment with your allocated beamtime.\nThe Cornell High Energy Synchrotron Source (CHESS) is currently home to 7 experimental stations spanning 3 sub-facilities. The diverse science, techniques, and missions of each beamline program leads to heterogeneous landscape of data collection experiences and computing literacy needed for each user. This training will cover the commonalities of CHESS data collection and resources to leverage, as well as where to expect to expect differences across beamlines and what questions to ask."
  },
  {
    "objectID": "theme4/XS100/data-collection.html#anatomy-of-an-experiment",
    "href": "theme4/XS100/data-collection.html#anatomy-of-an-experiment",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Anatomy of an experiment",
    "text": "Anatomy of an experiment\n\nMost experiments start well in advance of the awarded beamtime - and so does the computing and cyber-infrastructure needs.\nWhile CHESS provides state-of-the-art hardware, software, computing resources and trainings, users are responsible for the integrity of their experiment through thoughtful planning, experimental execution, and data handling and analysis. This includes maintaining best practices in experimental logs, metadata tracking, and recording of researcher decisions. Due to the nature of synchrotron experiments, the data integrity and intepretabilty - even within one research group - will be dependent on the practices adpoted by the research group."
  },
  {
    "objectID": "theme4/XS100/data-collection.html#beamtime-notes-and-experimental-logbook",
    "href": "theme4/XS100/data-collection.html#beamtime-notes-and-experimental-logbook",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Beamtime Notes and Experimental Logbook:",
    "text": "Beamtime Notes and Experimental Logbook:\nIt is always the responsibility of the experimenter to take detailed beamtime notes and a log of the data. Although work is ongoing to integrate metadata and capture requisite information in the data itself, automate workflows, visualizations, etc., it is imperative that experimenter notes are taken and ideally a copy is kept with the data on the CHESS system.\nJoint Experimenter Notes\nOften you will have a team of researchers taking data together - it is typically best practice to keep a collaborative log. In addition to your team - you should share these notes with your beamline scientist - they can often provide useful details you may miss if they observe some important or irregular behavior about the instrument itself that may or may not be obvious from the metadata streams.\nRemember - your beamline scientist is not responsible for memorizing the history of your data collected. They see so many experiments, if you ask 2 years later if they rememeber what you did - good luck.\nFor recording notes during the beamtime, we recommend using plain text or Markdown language (formatted text file) because it is easy to read in many systems, rather than a Microsoft Word Document which has a proprietary format. Images can be rendered in markdown formats.\nCo-Locate Your Beamtime Notes with Data\nSave your beamtime notes or a copy of your beamtime notes or (link to your google doc / equivalent) on the CHESS filesystem with your data (your beamline scientist will tell you where the best place is).\nLeverage MetaData Services\nWhenever possible, we encourage users to leverage the metadata services, beamline-specific software strategies (e.g. adding metadata to image frame headers), piping unique signals through software or hardware signals - saving these metadata with the data at the time of data collection. For unique aspects of your experiment, it is important to identify useful metadata to have associated with the x-ray data ahead of the beamtime. If signals are required to be monitored, this may need to be arranged for two weeks in advanced (see Bring Your Own Device (BYOD).\nCommenting Software\nComment any code produced at the beamtime. If this code was used to make decisions about the experiment, it should be saved and referred to in your experimental log."
  },
  {
    "objectID": "theme4/XS100/data-collection.html#station-computer-beamline-control-central",
    "href": "theme4/XS100/data-collection.html#station-computer-beamline-control-central",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Station Computer : Beamline Control Central",
    "text": "Station Computer : Beamline Control Central\nEvery experimental station has a station computer that acts as controls central. The station computer typically runs a number of processes and is responsible for orchestrating data collection, motor motions, synchronized triggers, metadata logging, and more.\n\nStation Computer ground rules:\n\nMost users directly interact with the station computer. Users associated with the beamtime will have permission to log directly into the station computer remotely through No Machine (LINK to CLASSE). Your beamline scientist will be training you in how to run your experiment - pay close attention and take ownership of your role during your beamtime. Your beamline scientist may only be training your group at a specific time(s) during the beamtime - make sure all users can be present during this training and/or take notes and be prepared to train your fellow users on the basic operations. When in doubt, always communicate with your staff scientist. Some processes staff scientists will insist that the user be trained by the scientist and not a fellow user.\nThe station computer is running many processes that enable your experiment to run. You will be asked to interact with specific processes during your beamtime. It is imperative you ONLY interact with the processes that your beamline scientist has given you permission to run. Some UI’s and data reduction processes will be running on other computers through terminals on the station computer - accidentally running these processes on the station computer itself may disrupt data collection, overwhelm the station computer, or even cause it to freeze or shut down.\nThe station computer has many special permissions, for instance it is able to write to the the CHESS DAQ (raw directory). When saving files such as beamtime notes, it is important to save these in the directories prescribed by your beamline scientist. (link to later section on CHESS file system and directories)\n\n\nThis is an example of a station computer screen shot with many processes. There are 4 desktops on the station computer, each with windows spanning 4 screens. This image is of the first desktop and shows a main controls terminal (SPEC), controls screens (MEDM screens), a data reduction GUI (HEXRD). There will typically be even more processes running than this.\nEvery beamline will have a unique version of this computer - some techniques even may be executed exclusively through a GUI.\n###1 Controls Hardware, Software, and Signal Monitoring\nThis section will discuss the hardware connections, motor configurations, and overall connectivity of signals being tracked in the beamline."
  },
  {
    "objectID": "theme4/XS100/data-collection.html#controls-software",
    "href": "theme4/XS100/data-collection.html#controls-software",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Controls Software",
    "text": "Controls Software\nThere are many controls languages and strategies across the lab. The two most common cases are SPEC and EPICS which will briefly be introduced here. Python-based controls are also very common.\nSPEC\nSPEC is a language, loosely based on C, used for instrument control and data acquisition at many synchrotrons.\nWhen using spec, you will interact with a SPEC terminal and run a combination of “standard” SPEC commands and/or a series of compiled programs for your technique\nImportantly, only ever use SPEC or edit Macros with the explicit permission of your staff scientist. This may vary from beamline to beamline SPEC commands continued.\nBelow is a video of a SPEC command window with built-in and custom macros.\n\n\n\nEPICS\nEPICS is a set of software tools and applications which provide a software infrastructure for use in building distributed control systems to operate devices\nMany of the devices at CHESS leverage EPICS drivers for operation EPICS PVs (process variables) are commonly used for signal monitoring and metadata/data logging.\nMany important metadata signals can also be tracked using “EPICS PVs.” While many of these PVs (process variables) are used throughout data collection, they can also be an important part of data monitoring. Your beamline scientist may have you observe the monitoring page depending on your experiments sensitivity to certain signals to monitor specific station signals (link to signals.chess.cornell.edu).\nAn Epics MEDM (Motif Editor and Display Manager) Screen for a Detector is shown in the annotated station view.\nPYMCA\nA common GUI used at the beamline is the PyMCA GUI. In addition to it’s original use for XRF, this GUI can be used to load in spec.log data and plot the counters at your beamline.\nPython, MATLAB, etc.\nYou may have specific other software: Python scripts (link to python tutorial), other UI’s for instrumentation, that you should receive training from your beamline scientist. See Python module if that is important."
  },
  {
    "objectID": "theme4/XS100/data-collection.html#networks-and-filesystems",
    "href": "theme4/XS100/data-collection.html#networks-and-filesystems",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Networks and Filesystems",
    "text": "Networks and Filesystems\n\nDuring data collection, raw data is written directly to the CHESS-DAQ filesystems. The CHESS-DAQ consists of approximately 2 petabytes of dedicated online storage arrays connected to the CHESS experimental stations through a high-speed 10Gb data collection network.\nTo protect the communication signals between the station computer, experimental station equipment, and other local systems, each experimental station has an isolated network with direct connections.\nDetectors often have direct fiber optic / high speed data lines to inline computing resources and/or the CHESS-DAQ.\n\nThe CHESS filesystem has different locations for storing raw data, reduced data, etc. These different locations have different backup schedules and total storage amounts. Typically best practice is as follows: 1. Raw Data that cannot be reproduced is located in RAW/DAQ 2. Reduced Data that can be reproduced from Raw Data/Other Protected Data is in REDUCED DATA 3. Metadata that is small and not reproducible should be saved in METADATA (backed up nightly) 4. Data that is being produced and does not need to be backed up and could be processed again should be done in SCRATCH. This is a good location for testing code before performing Data Reduction. 5. For Data NOT associated with a particular beamtime, USER is an appropriate place for these projects.\n\nProtected Data: Intellectual Property (IP) and Export Control\nSome data needs to be protected, e.g. data covered under Intellectual Property or Export Control agreements\nAll such data must be declared and all agreements signed before ANY data is on CHESS/Cornell computing systems (including preparatory material that falls under IP or Export Control categories).\nData Collection, Storage, and Analysis can be customized to comply with data agreements, including: - Modifying isolated networks - Mounting encrypted drives - Configuring encrypted computers - Modifying permissions on filesystem locations - Securing the experimental station with an entry password - Disconnecting streaming video to the experimental station"
  },
  {
    "objectID": "theme4/XS100/data-collection.html#data-handling-and-analysis",
    "href": "theme4/XS100/data-collection.html#data-handling-and-analysis",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Data Handling and Analysis",
    "text": "Data Handling and Analysis\n\nIf you wish to move any data from the CHESS filesystem to another location, the preferred way of doing so is through Globus. Please see here (https://wiki.classe.cornell.edu/Computing/GlobusDataTransfer) for directions on ways to transfer data from the CHESS filesystem.\nYour beamline may be producing very large quantities of data. Due to it’s size, you may not be able to take your data home or transfer it home via globus. your data in raw may only stay in hot storage for a short amount of time (6 months). Your experimental station will have best practices for how to compress or reduce this data so that it is small enough to take home or live in a different part of our filesystem.\n\nThe data is still saved, but to transfer or perform analysis on the files you will need to arrange to have it rolled back into “hot storage” - AKA take out an IT ticket: https://wiki.classe.cornell.edu/Computing/ServiceRequestTips\n\n\nBring Your Own Device (BYOD)\nUsers may need to bring their own devices to be beamline - either physically in the lab or remotely connected to the CHESS networks\nExamples include: - Controls computer for equipment they have integrated for an experiment - Analysis computer for on-the-fly analysis\nAll devices must be approved at least two weeks in advance. It may not be possible to consider integration on a shorter time period.\nBecause the CHESS-DAQ filesystems are a critical resource for data collection, write access is only granted to registered devices on the CHESS-DAQ network. If you wish to bring your own device to write data to the CHESS-DAQ, please discuss your needs with your staff scientist at least one month before your beamtime. Before your device can be registered on the CHESS-DAQ, it must undergo a cybersecurity evaluation by CLASSE-IT.\nRead access to the CHESS-DAQ filesystem may be obtained by registering your device for the LNS Protected network using this request form.\n\n\nMetaData Handling\nIdeally, all the data necessary to fully reproduce your results are recorded and disseminated in a manner that others can interpret after your experiment. Ideally the provenance remains unbroken from experiment planning.\nMetadata and parallel data streams are generated at every stage of your experiment. CHESS is continuing to develop and implement services to help with this creation. From programs like Galaxy, to our Metadata service -\nThe metadata service (https://wiki.classe.cornell.edu/bin/viewauth/CHESS/Private/CHESSMetadataService) provides tools to record and automatically ingest machine-readable metadata in a systematic way. It includes variables that historically were not recorded via a second data stream (e.g. the material processing parameters).\n\n\n\nOn-the-fly Data Processing & Visualization\nNeed to be looking at your data as it is coming off for data fidelity. At some beamlines, you will need to interact manually with a GUI to render your images and make sure the data quality is what you are expecting. At some beamlines, the new NSDF (National Data Science Fabric) Dashboard has been deployed for some datatypes which allows interactive visualization of data as it is coming off the beamline. An example can be found here: https://services.nationalsciencedatafabric.org/chess/.\nMany beamlines have an initial “data reduction” procedure that reduces the size of the data through compression or on-the-fly analysis to have smaller file sizes that are more manageable to analyze at your home institution.\nSome of these processes are software procedures performed on the data after it is written to the raw directory, while other beamlines utilize inline data processing/compression prior to writing their files to the raw directory. Overtime we will build out station specific training on these, for now it is the responsibility of the user to work with their beamline scientist and read any requisite materials they provide.\n\n\nData Reduction and Analysis\n\nCompute Farm\nThe CLASSE Compute Farm is a central resource consisting of approximately 60 enterprise-class Linux nodes (with around 400 cores), with a front-end queueing system (Son of Grid Engine, or SGE) that distributes jobs across the Compute Farm nodes. SGE supports interactive, batch, parallel, and GPU jobs, and it ensures equitable access to the Compute Farm for all users.\nData on the CHESS-DAQ filesystems can be directly accessed using the Compute Farm, and instructions for job submission are available here.\nCHESS Analysis Pipeline (CHAP)\nThe CHESS Analysis Pipeline (CHAP) is an object-oriented framework for organizing data analysis code into reusable modules. The most basic pipeline consists of the following modules: - Reader: takes an input file or data source and converts it into a standard data structure - Processor: takes a data structure from a Reader, executes a data processing algorithm, and writes an output data structure - Writer: takes a data structure from a Processor and converts it to a specific file format\n\nAn example of a concrete CHAP implementation is shown below. Here, the Processor accepts inputs from multiple Readers that provide both raw data and metadata.\n\nCHAP pipelines can be executed from a Linux command line or from the Galaxy science gateway. A third method called CHAPBook is currently under development, which presents a notebook-like coding interface for non-expert users.\nTechnique/Beamline Specific Software\nCHESS has many common X-ray software packages available on the system. Speak with your beamline scientist for the preferred software package for your experiment."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "X-CITE training materials",
    "section": "",
    "text": "X-CITE (CyberInfrastructure Training and Education for Synchrotron X-Ray Science) develops training materials for the community of scientists and researchers using the CHESS synchrotron X-ray facility and similar light sources.\nThis is the training overview:"
  },
  {
    "objectID": "index.html#theme-1-programming-fundamentals",
    "href": "index.html#theme-1-programming-fundamentals",
    "title": "X-CITE training materials",
    "section": "Theme 1: Programming Fundamentals",
    "text": "Theme 1: Programming Fundamentals\n\nPE 100: Python Programming and Jupyter notebooks\nPE 101: Using Python packages & libraries, Conda\nPE 102: Numerical data analysis with Python\nPE 103: Software version control, testing, and debugging"
  },
  {
    "objectID": "index.html#theme-2-systems-fundamentals",
    "href": "index.html#theme-2-systems-fundamentals",
    "title": "X-CITE training materials",
    "section": "Theme 2: Systems Fundamentals",
    "text": "Theme 2: Systems Fundamentals\n\nSF 100: Intro to Linux, the command line, and programming in Python\nSF 101: Containers and virtualization 🚧\nSF 200: Parallel computing concepts 🚧\nSF 201: Batch systems and compute farms with CHESS example 🚧"
  },
  {
    "objectID": "index.html#theme-3-distributed-computing-and-the-ci-ecosystem",
    "href": "index.html#theme-3-distributed-computing-and-the-ci-ecosystem",
    "title": "X-CITE training materials",
    "section": "Theme 3: Distributed Computing and the CI Ecosystem",
    "text": "Theme 3: Distributed Computing and the CI Ecosystem\n\nDC 100: Distributed computing concepts\nDC 101: Scientific workflow management\nDC 102: Using science gateways with Open OnDemand example 🚧\nDC 200: Computing with CI ecosystem - ACCESS, PATh, Campus"
  },
  {
    "objectID": "index.html#theme-4-x-ray-science-se-software",
    "href": "index.html#theme-4-x-ray-science-se-software",
    "title": "X-CITE training materials",
    "section": "Theme 4: X-Ray Science S&E Software",
    "text": "Theme 4: X-Ray Science S&E Software\n\nXS 100: Data collection, preparing input parameters, SPEC and CLI\nXS 101: Basic / on-the-fly data analysis, viewing detector images\nXS 102: Large-scale data analysis: from images to science parameters to interpretation 🚧\nXS 200: Metadata for data fidelity and systematic checks 🚧"
  },
  {
    "objectID": "index.html#theme-5-data-curation-and-fair",
    "href": "index.html#theme-5-data-curation-and-fair",
    "title": "X-CITE training materials",
    "section": "Theme 5: Data Curation and FAIR",
    "text": "Theme 5: Data Curation and FAIR\n\nCF 100: Intro to domain metadata standards, formats and repositories 🚧\nCF 101: Best practices for developing DMP (Data Mgmt Plans) 🚧\nCF 102: Metadata annotation and DOI 🚧\nCF 200: Curating data, code, workflows, and publishing 🚧\n\nMeanwhile…"
  },
  {
    "objectID": "theme5/CF102/metadata-annotation-and-doi.html",
    "href": "theme5/CF102/metadata-annotation-and-doi.html",
    "title": "Metadata annotation and DOI",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme5/CF102/metadata-annotation-and-doi.html#what-is-metadata",
    "href": "theme5/CF102/metadata-annotation-and-doi.html#what-is-metadata",
    "title": "Metadata annotation and DOI",
    "section": "What is metadata?",
    "text": "What is metadata?\n\nMirriam-Webster dictionary defines data as: “factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation”, or: “information in digital form that can be transmitted or processed.”\nFor our purposes, let us use the latter definition: data is information in digital form that can be stored, transmitted, or processed. Our experiments may consume data, produce data, transmit data between instruments and computers, and we might want to analyze data to make sense of it. We store data somewhere, for eventual use and/or sharing.\n\nWhat is metadata then?\nMetadata is data that describes data, and what that description says depends on context.\nFor example, EXIF (which is an abbreviation of “Exchangeable image file format”) is a form of image metadata. When you take a picture with cellphone, you get an image file. In addition to “pure” compressed on uncompressed image data, this image file also contains some extra data describes the image: details about the camera, lens, aperture, shutter speed, date and time at the time when the picture was taken, location if available, such things.\nIn addition to EXIF, there is the metadata that the operating system itself maintains about an image file: its ownership, date and time when the file was created and modified (as far as the operating system is concerned), and so on.\nSo when we talk about metadata, it is important to be clear about the context in which we talk about metadata.\nFor CHESS experimenters, metadata would be information about observational or experimental data that provides a “fuller picture” about the observation or experiment."
  },
  {
    "objectID": "theme5/CF102/metadata-annotation-and-doi.html#what-is-annotation",
    "href": "theme5/CF102/metadata-annotation-and-doi.html#what-is-annotation",
    "title": "Metadata annotation and DOI",
    "section": "What is annotation?",
    "text": "What is annotation?"
  },
  {
    "objectID": "theme5/CF102/metadata-annotation-and-doi.html#what-is-doi",
    "href": "theme5/CF102/metadata-annotation-and-doi.html#what-is-doi",
    "title": "Metadata annotation and DOI",
    "section": "What is DOI?",
    "text": "What is DOI?\nA DOI (Digital Object Identifier) is a unique and stable string assigned articles, books, and other works. DOIs make it easier to find and retrieve works. DOIs are designed to be used by humans as well as machines. DOIs are commonly used to identify publications and data sets.\nA DOI is meant to help us to resolve its target. The location of an online document or data set may change over time, because domain names and links change. However, since a DOI remains stable over time, with a DOI people should be able to find the current location of the target. The publisher of the document or data set is responsible for keeping the record up-to-date.\nA DOI takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash, in prefix/suffix form.\nHere is an example of a DOI: 10.1177/0306312719863494. Here the prefix 10.1177 identifies the registrant of the DOI, and the suffix 0306312719863494 identifies the article.\n\n\nHow can I resolve a DOI?\nTo resolve the article referred by 10.1177/0306312719863494, you would use a resolver. The search box at doi.org is an interface to such a resolver.\nA DOI is also resolvable as a URL using a proxy server. You could prefix the DOI with https://doi.org/ to get the URL https://doi.org/10.1177/0306312719863494, which will redirect you to the actual article, currently available at https://journals.sagepub.com/doi/10.1177/0306312719863494:\nThe https://doi.org resolver is operated by the DOI Foundation, a non-profit that governs the DOI system. The DOI system is standardized by International Organization for Standardization (ISO).\n\n\nHow do I use DOI in citations?\nHere’s how you would cite the above-mentioned article:\n\nMayernik, M. S. (2019). Metadata accounts: Achieving data and evidence in scientific research. Social Studies of Science, 49(5), 732-757. https://doi.org/10.1177/0306312719863494\n\n\n\n\nHow do I get myself a DOI?\n\n\nIn order to get a DOI for your paper or data set, you will need to work with a registration agency, such as crossref or datacite."
  },
  {
    "objectID": "theme5/CF102/metadata-annotation-and-doi.html#references",
    "href": "theme5/CF102/metadata-annotation-and-doi.html#references",
    "title": "Metadata annotation and DOI",
    "section": "references",
    "text": "references\n\nCarpentries Incubator: Data and Metadata\n\nCarpentries Incubator Training Course Material: Fundamentals of Scientific Metadata\n\nMetadata accounts: Achieving data and evidence in scientific research. Matthew S Mayernik, National Center for Atmospheric Research.\nObservational Health Data Sciences and Informatics forums: How do we define “Metadata” and “Annotation?”\nWikipedia page on Digital object identifier\nDOI Foundation: What is a DOI?\nScribbr: What is a DOI? Finding and Using Digital Object Identifiers\nLong Term Ecological Research Network: Enriching Ecological Data Using Annotated Metadata\nCool DOIs"
  },
  {
    "objectID": "theme5/CF100/domain-metadata-standards.html",
    "href": "theme5/CF100/domain-metadata-standards.html",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Wouldn’t it be great if you could quickly find data generated from other people’s experiments, combine it with your own data, and produce new, high-value output? Wouldn’t it be even better if other people could discover your data, utilize it, and cite your data in their papers? That is precisely the goal of FAIR Data - making results Findable, Accessible, Interoperable, and Reusable.\nIn this section of the X-CITE training materials, we’ll look at the motivations behind FAIR, take a more detailed look at each of the four components, and consider some of the implications of these large-scale sharing principles.\n\n\nRunning experiments and collecting data is expensive [citation needed]. Funding agencies don’t want to pay for running the same experimental design over and over beyond, perhaps, a replication study. Publishers are looking for new ways to add value to their curation of articles. And most importantly (to us, at least) there are the researchers who are producing these voluminous datasets.\nIndividual researchers have their own needs for FAIR. Speed is a big one - having Findable and Accessible data already available can lead to faster hypothosis formation, allow earlier grant proposals, and establish priority in publication. A second concern is publication. Historically it wasn’t easy to publish raw data because it’s expensive to keep and grants wouldn’t pay for it. There are repositories now where your experimental data can be stored and accessed. Some of them are very domain specific (such as Cornell’s FOXDEN, which we look at later) and others are designed more for “general purpose” use. Finally, adhering to the FAIR Data principles makes it easy (and expected!) for other researchers using your data to cite your work with unambiguous credit and with a permanent network location where the material can be found. FAIR is not restricted to data in the historical sense, but also encompasses source code and workflows.\nPublishers stand to benefit from FAIR, especially if they take full advantage of the ideas. Findable and Accessible (not to mention Interoperable and Reusable) data leads to it being used more and by a wider distribution of users. Researchers in one field probably don’t read journals in a different one. But what if that outlier of a researcher was able to discover data relevant to their work? Now the publisher may have a broader market to sell into. Discoverable data doesn’t come from outer space, with apologies to the astronomers, but is a product of research and that research is accompanied by publications. Publishers will barely care if the underlying data is cited, but they care deeply that the associated journal articles are. FAIR principles, through the “side effect” of increasing publication citations, boost the Impact Factor for journals and that benefits both the authors and the publisher.\n\n\n\nFindable, Accessible, Interoperable, and Reusable.\nIn this section we will look at the aspects that make up each of Findability, Accessibility, Interoperability, and Reusability. As you read this, keep in mind that the principles are descriptive rather than prescriptive. There is no attempt to describe how to implement an archive, for instance, but rather to discuss what the requirement is and leave the implementation details up to the implementers.\n\n\nThe first requirement for a data set to be useful is for it to be findable. If no one can find it, it may as well not exist. There has to be some kind of automatic way to describe what you are looking for and get a list of possible results in return. If this sounds like a Search Engine, then you’re on the right track. Search engines are pretty good at what they’re designed for, which is hunting through immense collections of text and determining which documents are statistically likely to be useful. The problem is the “collections of text” part: general-purpose search engines have little or no conception of structured data. Imagine trying to search for spreadsheets that contain bond angle data and find the ones that have particular angles, in a particular order, under specific circumstances. Clearly, we’re going to need something more versatile than just a text search engine (sorry, Google).\nThere are four elements that make up the “Findable” principle:\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identfier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nLooking at these elements, we see that there must be rich metadata available to describe a data set well enough to facilitate people finding the (meta)data, and both the data itself as well as the metadata need to be published in a way that facilitates indexing both. The data and metadata must each have a Globally Unique Identifier - think of it as being like a URL. If you have already seen “doi:” used to refer to journal articles, then it will come as no surprise to see this extended to (meta)data.\n\n\n\nMoving on to “Accessible”, let’s take a look at what goes into that:\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\nA2. metadata are accessible, even when the data are no longer available\n\nThere are two basic aims here. The first is to use a well-known, fully-documented, unencumbered protocol that (optionally) allow for authentication and authorization tasks (“logging in”) to be completed. The last aim is unexpected at first blush but makes sense when you think about it: metadata live forever, even after the data they describe has finally been deleted. Permanently keeping the metadata as a record of what work has gone before makes it possible to get an idea of past research directions. Interestingly, “mining” collections of metadata can produce new knowledge of its own.\n\n\n\nInteroperability simply means that the data are sufficiently described by their metadata so that researchers can create their own tools, or use the tools of others, to work with the data without having to use the same software as the original researcher. Very importantly, if at all possible the data should be preserved in a way that doesn’t require the use of proprietary software to read it. This is not simply because of cost, though that can be a major barrier, but is also a matter of historical preservation - it may be impossible to locate or run old enough software. Imagine having a really useful data set except it’s only readable by a program that only runs on a 35 year old version of MacOS. Yes, that really happens. If we design our (meta)data for interoperability then we can take that 35+ year old dataset and work with it using modern tools.\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nThe first of these elements is a challenge, honestly. The goal is to have a very rich way of representing metadata that preserves the semantics of the metadata. It’s not enough to simply say “the third column is conductance”. What is needed is a way to say “the third column is conductance, it’s an electronic measure, it’s related to resistance, and here is how”. It’s a lot of work, but luckily for us most of it has been done and we can reuse it - Resource Definition Framework (RDF) is made for this job. We’ll talk more about this in a later section.\n\n\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards\nThe crux of this principle is ensuring high-quality curation. The metadata should be thoroughly descriptive and will indicate, among other things, the rights the authors has granted to future users and a “chain of custody” that describes how the (meta)data came to be. Finally, the (meta)data should of course be of high quality to begin with, in line with the standards of the discipline. If you are dealing with human subjects, be aware that data privacy requirements (HIPPA, IRB) may restrict or prohibit the publication of data and even metadata. Enough metadata can sometimes be used to reconstruct the data itself. As an example, sometimes age, zip code, and gender is enough to identify someone - some zip codes have very people in them. 28520 has about 250 people.\n\n\n\n\nAs mentioned above, metadata is simply information that describes your data. Hidden behind that word “simply” is the slight complication that metadata can be anything from really simple to really complicated. Where it falls on that continuum is situational. Small amounts of simple data will probably have simple metadata.\n\n\nWithout a standard means of representation and agreed-upon meanings, the metadata we assemble might be more of a hinderance than a help. Selecting our data’s properties to record is domain-specific in many cases. A very general set of attributes is available at schema.org. Other ontologies exist, of course, and selection among them tends to narrow as you go.\n\n\nRDF is “Resource Description Format” and is a broadly used concept even outside of FAIR. Fundamentally, RDF’s “intention” is to describe the world in terms of triples: subject, predicate, and object. From these building blocks we can construct graph structures (in the “discrete math” sense of the term). As they grow, they can represent linkages between related items, for instance, and that is when the real power of RDF can start to be exploited. With small collections of metadata there is little choice but to handle search terms, for instance. Once the collection expands, new kinds of queries are possible. For instance, different researchers might submit data to an archive. We know the possible range of metadata descriptors and we know what each one means. At this point, we can traverse the RDF graph. It’s easy to go from the starting place to enter the graph and then follow along, hop by hop, expanding the search possibilities by adding related terms.\n\n\n\nFor any data to be useable by a computer, it has to be represented in a way that it can be understood. Metadata is no exception. RDF is both a conceptual layout for in-memory processing and also a defined way of writing out the structure. There are just two problems. One is that the format is a lot to digest when you first start working with it. The other is that, depending on the language you’re using, you might end up having to write your own parser for this. Luck is smiling on us, though, in the form of alternative ways to represent that graph structure. A very common representation, and one that is becoming increasingly popular, is JSON (Javascript Object Notation). It has its roots in Javascript, but it has spread far and wide in dozens of languages. Support for it is nearly ubiquitous now. Take a look at it.\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\nJSON is good for representing lots of structured data, but there needs to be something that can go beyond that and hold references to the schemas that apply to certain fields. For this, there is JSON-LD. The LD stands for “Linked Data”. JSON-LD stores additional data in the form of “context” fields and these fields can contain URLs to outside sites storing official, curated ontologies:\n{\n  \"@context\": \"https://json-ld.org/contexts/person.jsonld\",\n  \"@id\": \"http://dbpedia.org/resource/John_Lennon\",\n  \"name\": \"John Lennon\",\n  \"born\": \"1940-10-09\",\n  \"spouse\": \"http://dbpedia.org/resource/Cynthia_Lennon\"\n}\n\n\n\n\n\nFOXDEN (FAIR Open-Science Extensible Data Exchange Network) is a preliminary implementation of a data management system that can be used to meet the FAIR requirements. The system gets inspiration from Linux, in that it provides a collection of tools that work together in a modular fashion. It is possible to use all or some of the components.\nAccess to the FOXDEN modules is via either a web page for each module or by using a command-line tool. Besides being a perfectly reasonable way to use the tools, the command line tool is also well suited to use in scripts.\n\n\nFOXDEN’s modular architecture makes it easy to select which components of it you’d like to use and even makes it possible to substitute your own software if you have specialized needs. The FOXDEN documentation has a walkthrough of basic use. This document will instead give some brief background on each component. You’re encouraged to work through the “Quick Start Guide”.\n\n\nAs you would (likely) expect, the Frontend service generates the web pages through which users can easily interact with the system. Initially, the user is presented with a login page. Once past that, access to the other modules is a click or two away. Of particular interest is the “docs” button toward the upper-right corner. Having the documentation close at hand will prove… handy.\n\n\n\nThe command line tool (“foxden”) is both an alternative way users can access the system as well as a means to interface shell scripts to the system for automating common tasks. The “foxden” command by itself with no arguments will display a list of the available commands and also gives a link to the documentation and a reminder of how to get more detailed help.\n\n\n\nEven in a purely open research environment, it’s still necessary to keep track of who is making changes. This is both for proper attribution as well as non-repudiation (perhaps less of a factor in X-Ray Science than in other disciplines, but the system is built to be versatile). Web users will see a familiar-looking login screen. CLI users will need to authenticate via Kerberos - don’t worry, it’s well described in the introductory documentation. FOXDEN mercifully provides a way to use Kerberos that is simpler than the old-school way.\n\n\n\nThe Discovery service provides a way to query the underlying “management database” that tracks movement of files and the metadata associated with them. The query language is the same one MongoDB used (Mongo QL).\n\n\n\nThe MetaData service is one of the critical components. This module can not only query metadata, for instance finding matching schemas, but also create new schemas and manipulate existing ones.\n\n\n\nThe Provenance service provides a lot of functionality. The tracking of “provenance” is not just something art historians do. The term refers to the tracking of every movement of the data we’re managing, what tools were used to transform it and under what circumstances, and where the data came from. This last element could be, say, “from this instrument on this beamline” or it could be “Dr. J. Doe’s Sept 13th dataset, reduced by this lump of MATLAB code”.\n\n\n\nThe Data Management service abstracts data movement in and out of the underlying Object Store (AWS S3 or compatible). Functions are provided to both manage the Object Store as well as to move data in, move it out, or delete it.\n\n\n\nThe Publication service has two major sections. The first handles the creation and assignment of DOIs (Document Object Identifiers - you’ve seen these in “References” sections) and the association of that identifier with metadata and data. The second section provides a means to interact with Zenodo in a manner consistent with the rest of FOXDEN.\n\n\n\nSpecScan is pretty specific: it is used to create and manipulate records for spec scans. It does what it says on the tin.\n\n\n\nThe MLHub service allows the user to run various Machine Learning (ML) algorithms directly in the FOXDEN environment. TensorFlow is directly supported. Doing this directly inside of FOXDEN seems odd at first, but it follows the paradigm of “moving the compute to the data”, preventing time-consuming retrievals.\n\n\n\nThe CHAP service simplifies running the CHESS-developed CHAP algorithms on data stored in FOXDEN.\n\n\n\nDesigned for novice programmers, the CHAP Notebook service simplifies data analysis by giving users a Jupyter-like interface for writing code modules that are inserted into pre-defined workflows. These modules are also deposited in a code repository for future dissemination."
  },
  {
    "objectID": "theme5/CF100/domain-metadata-standards.html#why-fair",
    "href": "theme5/CF100/domain-metadata-standards.html#why-fair",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Running experiments and collecting data is expensive [citation needed]. Funding agencies don’t want to pay for running the same experimental design over and over beyond, perhaps, a replication study. Publishers are looking for new ways to add value to their curation of articles. And most importantly (to us, at least) there are the researchers who are producing these voluminous datasets.\nIndividual researchers have their own needs for FAIR. Speed is a big one - having Findable and Accessible data already available can lead to faster hypothosis formation, allow earlier grant proposals, and establish priority in publication. A second concern is publication. Historically it wasn’t easy to publish raw data because it’s expensive to keep and grants wouldn’t pay for it. There are repositories now where your experimental data can be stored and accessed. Some of them are very domain specific (such as Cornell’s FOXDEN, which we look at later) and others are designed more for “general purpose” use. Finally, adhering to the FAIR Data principles makes it easy (and expected!) for other researchers using your data to cite your work with unambiguous credit and with a permanent network location where the material can be found. FAIR is not restricted to data in the historical sense, but also encompasses source code and workflows.\nPublishers stand to benefit from FAIR, especially if they take full advantage of the ideas. Findable and Accessible (not to mention Interoperable and Reusable) data leads to it being used more and by a wider distribution of users. Researchers in one field probably don’t read journals in a different one. But what if that outlier of a researcher was able to discover data relevant to their work? Now the publisher may have a broader market to sell into. Discoverable data doesn’t come from outer space, with apologies to the astronomers, but is a product of research and that research is accompanied by publications. Publishers will barely care if the underlying data is cited, but they care deeply that the associated journal articles are. FAIR principles, through the “side effect” of increasing publication citations, boost the Impact Factor for journals and that benefits both the authors and the publisher."
  },
  {
    "objectID": "theme5/CF100/domain-metadata-standards.html#the-principles-of-fair-ness",
    "href": "theme5/CF100/domain-metadata-standards.html#the-principles-of-fair-ness",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Findable, Accessible, Interoperable, and Reusable.\nIn this section we will look at the aspects that make up each of Findability, Accessibility, Interoperability, and Reusability. As you read this, keep in mind that the principles are descriptive rather than prescriptive. There is no attempt to describe how to implement an archive, for instance, but rather to discuss what the requirement is and leave the implementation details up to the implementers.\n\n\nThe first requirement for a data set to be useful is for it to be findable. If no one can find it, it may as well not exist. There has to be some kind of automatic way to describe what you are looking for and get a list of possible results in return. If this sounds like a Search Engine, then you’re on the right track. Search engines are pretty good at what they’re designed for, which is hunting through immense collections of text and determining which documents are statistically likely to be useful. The problem is the “collections of text” part: general-purpose search engines have little or no conception of structured data. Imagine trying to search for spreadsheets that contain bond angle data and find the ones that have particular angles, in a particular order, under specific circumstances. Clearly, we’re going to need something more versatile than just a text search engine (sorry, Google).\nThere are four elements that make up the “Findable” principle:\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identfier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nLooking at these elements, we see that there must be rich metadata available to describe a data set well enough to facilitate people finding the (meta)data, and both the data itself as well as the metadata need to be published in a way that facilitates indexing both. The data and metadata must each have a Globally Unique Identifier - think of it as being like a URL. If you have already seen “doi:” used to refer to journal articles, then it will come as no surprise to see this extended to (meta)data.\n\n\n\nMoving on to “Accessible”, let’s take a look at what goes into that:\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\nA2. metadata are accessible, even when the data are no longer available\n\nThere are two basic aims here. The first is to use a well-known, fully-documented, unencumbered protocol that (optionally) allow for authentication and authorization tasks (“logging in”) to be completed. The last aim is unexpected at first blush but makes sense when you think about it: metadata live forever, even after the data they describe has finally been deleted. Permanently keeping the metadata as a record of what work has gone before makes it possible to get an idea of past research directions. Interestingly, “mining” collections of metadata can produce new knowledge of its own.\n\n\n\nInteroperability simply means that the data are sufficiently described by their metadata so that researchers can create their own tools, or use the tools of others, to work with the data without having to use the same software as the original researcher. Very importantly, if at all possible the data should be preserved in a way that doesn’t require the use of proprietary software to read it. This is not simply because of cost, though that can be a major barrier, but is also a matter of historical preservation - it may be impossible to locate or run old enough software. Imagine having a really useful data set except it’s only readable by a program that only runs on a 35 year old version of MacOS. Yes, that really happens. If we design our (meta)data for interoperability then we can take that 35+ year old dataset and work with it using modern tools.\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nThe first of these elements is a challenge, honestly. The goal is to have a very rich way of representing metadata that preserves the semantics of the metadata. It’s not enough to simply say “the third column is conductance”. What is needed is a way to say “the third column is conductance, it’s an electronic measure, it’s related to resistance, and here is how”. It’s a lot of work, but luckily for us most of it has been done and we can reuse it - Resource Definition Framework (RDF) is made for this job. We’ll talk more about this in a later section.\n\n\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards\nThe crux of this principle is ensuring high-quality curation. The metadata should be thoroughly descriptive and will indicate, among other things, the rights the authors has granted to future users and a “chain of custody” that describes how the (meta)data came to be. Finally, the (meta)data should of course be of high quality to begin with, in line with the standards of the discipline. If you are dealing with human subjects, be aware that data privacy requirements (HIPPA, IRB) may restrict or prohibit the publication of data and even metadata. Enough metadata can sometimes be used to reconstruct the data itself. As an example, sometimes age, zip code, and gender is enough to identify someone - some zip codes have very people in them. 28520 has about 250 people."
  },
  {
    "objectID": "theme5/CF100/domain-metadata-standards.html#metadata",
    "href": "theme5/CF100/domain-metadata-standards.html#metadata",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "As mentioned above, metadata is simply information that describes your data. Hidden behind that word “simply” is the slight complication that metadata can be anything from really simple to really complicated. Where it falls on that continuum is situational. Small amounts of simple data will probably have simple metadata.\n\n\nWithout a standard means of representation and agreed-upon meanings, the metadata we assemble might be more of a hinderance than a help. Selecting our data’s properties to record is domain-specific in many cases. A very general set of attributes is available at schema.org. Other ontologies exist, of course, and selection among them tends to narrow as you go.\n\n\nRDF is “Resource Description Format” and is a broadly used concept even outside of FAIR. Fundamentally, RDF’s “intention” is to describe the world in terms of triples: subject, predicate, and object. From these building blocks we can construct graph structures (in the “discrete math” sense of the term). As they grow, they can represent linkages between related items, for instance, and that is when the real power of RDF can start to be exploited. With small collections of metadata there is little choice but to handle search terms, for instance. Once the collection expands, new kinds of queries are possible. For instance, different researchers might submit data to an archive. We know the possible range of metadata descriptors and we know what each one means. At this point, we can traverse the RDF graph. It’s easy to go from the starting place to enter the graph and then follow along, hop by hop, expanding the search possibilities by adding related terms.\n\n\n\nFor any data to be useable by a computer, it has to be represented in a way that it can be understood. Metadata is no exception. RDF is both a conceptual layout for in-memory processing and also a defined way of writing out the structure. There are just two problems. One is that the format is a lot to digest when you first start working with it. The other is that, depending on the language you’re using, you might end up having to write your own parser for this. Luck is smiling on us, though, in the form of alternative ways to represent that graph structure. A very common representation, and one that is becoming increasingly popular, is JSON (Javascript Object Notation). It has its roots in Javascript, but it has spread far and wide in dozens of languages. Support for it is nearly ubiquitous now. Take a look at it.\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\nJSON is good for representing lots of structured data, but there needs to be something that can go beyond that and hold references to the schemas that apply to certain fields. For this, there is JSON-LD. The LD stands for “Linked Data”. JSON-LD stores additional data in the form of “context” fields and these fields can contain URLs to outside sites storing official, curated ontologies:\n{\n  \"@context\": \"https://json-ld.org/contexts/person.jsonld\",\n  \"@id\": \"http://dbpedia.org/resource/John_Lennon\",\n  \"name\": \"John Lennon\",\n  \"born\": \"1940-10-09\",\n  \"spouse\": \"http://dbpedia.org/resource/Cynthia_Lennon\"\n}"
  },
  {
    "objectID": "theme5/CF100/domain-metadata-standards.html#foxden---a-pilot-prototype-example",
    "href": "theme5/CF100/domain-metadata-standards.html#foxden---a-pilot-prototype-example",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "FOXDEN (FAIR Open-Science Extensible Data Exchange Network) is a preliminary implementation of a data management system that can be used to meet the FAIR requirements. The system gets inspiration from Linux, in that it provides a collection of tools that work together in a modular fashion. It is possible to use all or some of the components.\nAccess to the FOXDEN modules is via either a web page for each module or by using a command-line tool. Besides being a perfectly reasonable way to use the tools, the command line tool is also well suited to use in scripts.\n\n\nFOXDEN’s modular architecture makes it easy to select which components of it you’d like to use and even makes it possible to substitute your own software if you have specialized needs. The FOXDEN documentation has a walkthrough of basic use. This document will instead give some brief background on each component. You’re encouraged to work through the “Quick Start Guide”.\n\n\nAs you would (likely) expect, the Frontend service generates the web pages through which users can easily interact with the system. Initially, the user is presented with a login page. Once past that, access to the other modules is a click or two away. Of particular interest is the “docs” button toward the upper-right corner. Having the documentation close at hand will prove… handy.\n\n\n\nThe command line tool (“foxden”) is both an alternative way users can access the system as well as a means to interface shell scripts to the system for automating common tasks. The “foxden” command by itself with no arguments will display a list of the available commands and also gives a link to the documentation and a reminder of how to get more detailed help.\n\n\n\nEven in a purely open research environment, it’s still necessary to keep track of who is making changes. This is both for proper attribution as well as non-repudiation (perhaps less of a factor in X-Ray Science than in other disciplines, but the system is built to be versatile). Web users will see a familiar-looking login screen. CLI users will need to authenticate via Kerberos - don’t worry, it’s well described in the introductory documentation. FOXDEN mercifully provides a way to use Kerberos that is simpler than the old-school way.\n\n\n\nThe Discovery service provides a way to query the underlying “management database” that tracks movement of files and the metadata associated with them. The query language is the same one MongoDB used (Mongo QL).\n\n\n\nThe MetaData service is one of the critical components. This module can not only query metadata, for instance finding matching schemas, but also create new schemas and manipulate existing ones.\n\n\n\nThe Provenance service provides a lot of functionality. The tracking of “provenance” is not just something art historians do. The term refers to the tracking of every movement of the data we’re managing, what tools were used to transform it and under what circumstances, and where the data came from. This last element could be, say, “from this instrument on this beamline” or it could be “Dr. J. Doe’s Sept 13th dataset, reduced by this lump of MATLAB code”.\n\n\n\nThe Data Management service abstracts data movement in and out of the underlying Object Store (AWS S3 or compatible). Functions are provided to both manage the Object Store as well as to move data in, move it out, or delete it.\n\n\n\nThe Publication service has two major sections. The first handles the creation and assignment of DOIs (Document Object Identifiers - you’ve seen these in “References” sections) and the association of that identifier with metadata and data. The second section provides a means to interact with Zenodo in a manner consistent with the rest of FOXDEN.\n\n\n\nSpecScan is pretty specific: it is used to create and manipulate records for spec scans. It does what it says on the tin.\n\n\n\nThe MLHub service allows the user to run various Machine Learning (ML) algorithms directly in the FOXDEN environment. TensorFlow is directly supported. Doing this directly inside of FOXDEN seems odd at first, but it follows the paradigm of “moving the compute to the data”, preventing time-consuming retrievals.\n\n\n\nThe CHAP service simplifies running the CHESS-developed CHAP algorithms on data stored in FOXDEN.\n\n\n\nDesigned for novice programmers, the CHAP Notebook service simplifies data analysis by giving users a Jupyter-like interface for writing code modules that are inserted into pre-defined workflows. These modules are also deposited in a code repository for future dissemination."
  },
  {
    "objectID": "theme2/SF100/index.html",
    "href": "theme2/SF100/index.html",
    "title": "Linux, Command Line, and Scripting",
    "section": "",
    "text": "The following notes assume that you are all set up to use your accounts on the CLASSE Linux systems.\nDepending on your level of familiarity with the system, you might know enough commands to find your way around. Since it probably is not a good idea to make such assumptions right off the bat, let us see what you might need to know to in order to become a proficient user of the systems."
  },
  {
    "objectID": "theme2/SF100/index.html#linux",
    "href": "theme2/SF100/index.html#linux",
    "title": "Linux, Command Line, and Scripting",
    "section": "Linux",
    "text": "Linux\nLinux is a free and open source operating system known for stability, security, and versatility. Linux runs on a variety of machines from small embedded systems to powerful servers. A great deal of software runs on Linux.\nYou probably know all this already, so let us skip ahead."
  },
  {
    "objectID": "theme2/SF100/index.html#the-command-line",
    "href": "theme2/SF100/index.html#the-command-line",
    "title": "Linux, Command Line, and Scripting",
    "section": "The command line",
    "text": "The command line\nTo perform certain kinds of tasks, using the command line is often quicker and more efficient. You can “chain” or compose separate programs together, each of them specializing in doing different things. You can save longer tasks in the form of scripts for later use, and share them with your colleagues.\nHere’s a quick example. You will find documentation for the software installed on lnx201 in the directory /usr/share/doc. Many of those are named README, or README.md, or README.rst, or readme.txt, or some such variation. How many such files are there in /usr/share/doc?\nWe can find that out by using find (a program for searching for files under a directory tree) and wc (a word count program):\n[ssasidharan@lnx201 ~]$ find /usr/share/doc/ -iname \"readme*\" | wc -l\n1589\nMany of the files in /usr/share/doc mention the word “license” or “LICENSE” or some variation thereof. How many such lines are there? In order to find that out, we can use grep (a program that matches patterns), and wc together:\n[ssasidharan@lnx201 ~]$ grep -ir license /usr/share/doc/ | wc -l\n84089\nLearning to use the command line well will leave more power on your hands."
  },
  {
    "objectID": "theme2/SF100/index.html#using-the-shell-prompt",
    "href": "theme2/SF100/index.html#using-the-shell-prompt",
    "title": "Linux, Command Line, and Scripting",
    "section": "Using the shell prompt",
    "text": "Using the shell prompt\nThe [ssasidharan@lnx201 ~]$ thing with a blinking cursor at the end is called a shell prompt. You type commands at the shell prompt, hit enter, and then something happens in response to that.\nThe examples in these notes are my shell prompt: it contains my username on lnx201, followed by @ character, followed by the name of the computer (or “hostname”), followed by the current directory. Your prompt will be different, because it will contain your username.\nAfter entering the first few characters of a command, you can use the tabtab key for auto-completing commands.\n[ssasidharan@lnx201 /]$ ssh&lt;tab&gt;\nssh          ssh-agent    sshd         sshfs        ssh-keyscan  \nssh-add      ssh-copy-id  sshd-keygen  ssh-keygen   sshpass   \n[ssasidharan@lnx201 /]$ condor_&lt;tab&gt;\nDisplay all 119 possibilities? (y or n)\nBash offers some helpful methods for editing and navigating the history of commands you have previously executed.\n\nYou can use up/down arrow keys to navigate history.\nhistory command will print a list of recently used commands.\nYou can use Ctrl-RCtrl-R to search command history.\nCtrl-ACtrl-A will make the cursor to the beginning of the line.\nCtrl-ECtrl-E will go to the end of the line.\nCtrl-KCtrl-K will “kill” (cut) text from current position to end of line to a buffer called the “kill-ring”.\nCtrl-YCtrl-Y will “yank” (paste) most recently killed text from the kill ring to current cursor position.\nAlt-YAlt-Y will cycle through the kill-ring.\n\nTo exit the shell, you can use exit command or Ctrl-DCtrl-D.\nIf you are using ssh to connect to lnx201, exiting the shell will end your ssh session. If you had opened a terminal window, exiting the shell will close the window."
  },
  {
    "objectID": "theme2/SF100/index.html#how-does-bash-set-up-the-environment",
    "href": "theme2/SF100/index.html#how-does-bash-set-up-the-environment",
    "title": "Linux, Command Line, and Scripting",
    "section": "How does bash set up the environment?",
    "text": "How does bash set up the environment?\nThere are two kinds of shell sessions: login and non-login. login session starts when you enter a username and password, such as when using ssh. A non-login session starts when you open a terminal window from a desktop.\nDepending on how the session was started, a few shell scripts are read and executed when starting a shell.\nFor login shells these will be:\n\n/etc/profile is a global script that applies to all users.\n\n~/.bash_profile is a script in your home directory, and it is applied when you start a shell.\nIf ~/.bash_profile was not found, bash will attempt to read ~/.bash_login and ~/.profile in order.\n\nFor non-login shells:\n\n/etc/bashrc is the script that applies to everyone.\n~/.bashrc is the script that applies to you.\n\nNon-login shells also inherit the environment from their parent process, which is usually a login shell.\nSystems vary on how they are set up. You should look around lnx201 to find out how this is done there. These files are some examples of shell scripts, which is a topic we’ll visit later in these notes."
  },
  {
    "objectID": "theme2/SF100/index.html#changing-environment-variables",
    "href": "theme2/SF100/index.html#changing-environment-variables",
    "title": "Linux, Command Line, and Scripting",
    "section": "Changing environment variables",
    "text": "Changing environment variables\nYou can also use export command to overwrite existing environment variables, or add new ones. For example:\n[ssasidharan@lnx201 /]$ export HISTSIZE=2000\n[ssasidharan@lnx201 /]$ echo $HISTSIZE\n2000\nNote that this change in HISTSIZE applies only to the current shell. It will be forgotten when you exit the shell.\nIn order to make the change permanent, you will need to add the line export HISTSIZE=2000 to your ~/.bash_profile file."
  },
  {
    "objectID": "theme2/SF100/index.html#absolute-and-relative-paths",
    "href": "theme2/SF100/index.html#absolute-and-relative-paths",
    "title": "Linux, Command Line, and Scripting",
    "section": "Absolute and relative paths",
    "text": "Absolute and relative paths\nPaths can be specified in two ways: absolute or relative. An absolute pathname begins with the root directory, /, and contains every directory name, branch by branch.\nAbsolute path to the Desktop directory in my home directory on lnx201 is /home/ssasidharan/Desktop/.\nIn comparison, a relative pathname starts from the working directory. When I’m in my home directory, I can simply use the relative pathname, Desktop.\nEvery directory contains two special directory names, . and .., in which . refers to the current directory, and .. refers to the parent directory of the current directory.\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\n[ssasidharan@lnx201 ~]$ cd .\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\n[ssasidharan@lnx201 ~]$ cd ..\n[ssasidharan@lnx201 home]$ pwd\n/home\n[ssasidharan@lnx201 home]$ cd ..\n[ssasidharan@lnx201 /]$ pwd\n/"
  },
  {
    "objectID": "theme2/SF100/index.html#wildcards",
    "href": "theme2/SF100/index.html#wildcards",
    "title": "Linux, Command Line, and Scripting",
    "section": "Wildcards",
    "text": "Wildcards\nThe shell gives special treatment to some characters, known as wildcards. Using wildcard characters, we can quickly specify groups of files.\nThe wildcard character * stands for any set of characters. For example, you can list the names of all programs in /usr/bin that start with ab with ls /usr/bin/ab*:\n[ssasidharan@lnx201 ~]$ ls /usr/bin/ab*\n/usr/bin/ab  /usr/bin/abs2rel\nThe wildcard character ? stands for any single character. So if you want to list the filenames in /usr/bin that starts with any character, followed by abc, followed by any characters:\n[ssasidharan@lnx201 ~]$ ls /usr/bin/?abc*\n/usr/bin/kabc2mutt  /usr/bin/kabcclient"
  },
  {
    "objectID": "theme2/SF100/index.html#the-current-working-directory",
    "href": "theme2/SF100/index.html#the-current-working-directory",
    "title": "Linux, Command Line, and Scripting",
    "section": "The current working directory",
    "text": "The current working directory\nAt any time in the shell, we are “inside” a single directory, known as the current working directory. When you list files with ls, a list of files and directories of the current working directory will be printed on the output.\nWhen you log in to lnx201, initially you will be in a directory named /home/${USER}, where ${USER} is your username on lnx201. This is what is known as your /home directory/. When you log in first, your home directory will be your current working directory.\nTo find where you are, use the command pwd.\nThe below commands are useful:\n\nmkdir test will create a directory named test\ncd test will change the working directory to test.\nls will list files and directories in the current working directory.\nrm will remove a file.\nrm &lt;name of directory&gt; will not remove a directory; you have to remove it recursively, like so: rm -r &lt;name of directory&gt;.\n\ncd - is useful: it will switch you to the directory that you were previously in:\n[ssasidharan@lnx201 /]$ cd /usr/\n[ssasidharan@lnx201 usr]$ pwd\n/usr\n[ssasidharan@lnx201 usr]$ cd share/\n[ssasidharan@lnx201 share]$ pwd\n/usr/share\n[ssasidharan@lnx201 share]$ cd -\n/usr\n[ssasidharan@lnx201 usr]$ pwd\n/usr\nRunning cd ~ (or simply cd) will drop you back in your home directory:\n[ssasidharan@lnx201 ~]$ cd /usr/share/doc/\n[ssasidharan@lnx201 doc]$ pwd\n/usr/share/doc\n[ssasidharan@lnx201 doc]$ cd ~\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\nIt is worth noting that the shell will substitute ~ for your home directory.\n. and .. are special directory names: . means the current directory, and .. means its parent directory, or the directory above it in the directory hierarchy.\ntouch command is used to change file timestamps. You can also use touch to create an empty file, like so: touch test.txt.\n\nSymbolic links\nOn my home directory on lnx201, when I do an ls -l (which is for ls with long file listing format), I would see something like this:\nlrwxrwxrwx  1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\nThe first letter of the listing is l and the entry kind of suggests that my Downloads directory is a reference to another directory, /cdat/tem/ssasidharan/Downloads. The Downloads directory in my home directory is what is called a symbolic link, which also known as a soft link or “symlink”.\nWith symbolic links, we can have shortcuts to other files or directories."
  },
  {
    "objectID": "theme2/SF100/index.html#users-and-groups",
    "href": "theme2/SF100/index.html#users-and-groups",
    "title": "Linux, Command Line, and Scripting",
    "section": "Users and groups",
    "text": "Users and groups\nLinux is a multi-user operating system. Since many people can be using the system, there needs to be mechanisms in place to ensure separation between them, while ensuring that they can access shared resources when necessary.\nThe basic mechanism is the concept of users and groups.\nThe root user is a special user that has all the permissions. They can change most things about the system. The root user can change system configuration, add and remove users and groups, etc.\nMost of the time, we do not need neither the power nor the responsibilities of the root user. So we have a non-root, regular user account in lnx201.\nYour account also belongs to certain groups. Groups are the way to grant permission to a group of accounts. You can find the groups you belong to using groups command:\n[ssasidharan@lnx201 ~]$ groups\nchess classeuser\n[ssasidharan@lnx201 ~]$\nUsers and groups have distinct numerical identifiers too. You can find them with id command:\n[ssasidharan@lnx201 ~]$ id\nuid=63499(ssasidharan) gid=262(chess) groups=262(chess),750(classeuser)\nIf you run ls -l (-l is for long listing format) command to list files and folders in your home directory, the result will be something like this:\n[ssasidharan@lnx201 ~]$ ls -l\ntotal 4\ndrwxr-xr-x 2 ssasidharan chess   28 Mar 28 09:36 bin\ndrwxr-xr-x 2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x 2 ssasidharan chess   30 Mar 26 15:22 Desktop\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Documents\nlrwxrwxrwx 1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r-- 1 ssasidharan chess 3254 Mar  7 15:55 helloworld.ipynb\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Music\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Pictures\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Public\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Templates\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Videos\nLet us see what the above columns means:\n\nThe first column lists permissions on the file/folder. (We will see what this means in the next section.)\nThe second column shows number of links to it.\nThe third one shows the user who owns it.\nThe fourth one shows the group that owns the file.\nThe fifth one is the size of the file in bytes. Note that directories are a little special here – what you see here is not the total size of all the files and folders under the directory, but the space the directory itself uses on disk.\nThe next column (the whole Mar 26 15:21 segment) shows a timestamp when the file/folder was last modified.\nFinally, the name of the file/folder. Note that Downloads -&gt;   /cdat/tem/ssasidharan/Downloads is a bit special: it means that Downloads folder is in fact a link to /cdat/tem/ssasidharan/Downloads."
  },
  {
    "objectID": "theme2/SF100/index.html#permissions-and-ownership",
    "href": "theme2/SF100/index.html#permissions-and-ownership",
    "title": "Linux, Command Line, and Scripting",
    "section": "Permissions and ownership",
    "text": "Permissions and ownership\nLet us see what a string like drwxr-xr-x from the above example means. This string, sometimes called “permission bits” or “file mode bits”, is ten characters long. Each of the characters are shorthand signifying something.\n\nThe first d stands for directory. (For files, this will be a -.)\nThe next three rwx are for user’s permissions.\nThe next three r-x are for group permissions.\nThe final three r-x are for permissions for the rest of the users.\n\nNow, what do those r and w and x mean?\n\nr means permission to read.\nw means permission to write.\nx means permission to execute, in the case of files. In the case of directories, x means that you can cd into them.\n\n\nChanging permissions with chmod\nYou can use chmod command to change permissions. If you create a shell script named test.sh, for example, it won’t be executable by default. You will have to change the file mode bits using chmod:\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ ./test.sh\n-bash: ./test.sh: Permission denied\n[ssasidharan@lnx201 ~]$ chmod +x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ ./test.sh\nYou can remove the x bit like so:\n[ssasidharan@lnx201 ~]$ chmod -x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nYou can also grant permission to just the user, or group, or others:\n[ssasidharan@lnx201 ~]$ chmod u+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod g+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod o+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nYou can also combine u, g, o bits and r, w, x bits with + or -:\n[ssasidharan@lnx201 ~]$ chmod ugo-r test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n--wx--x--x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nI just made the file unreadable by everyone, even me!\n[ssasidharan@lnx201 ~]$ cat test.sh\ncat: test.sh: Permission denied\nOf course you can restore the permission with chmod ugo+r test.sh\nNote that when invoking chmod, a (or all) is equivalent of ugo (user + group + others). You can also omit a or ugo if you want everyone to have the same permissions. So the below all are equivalent:\n[ssasidharan@lnx201 ~]$ chmod ugo+r test.sh\n[ssasidharan@lnx201 ~]$ chmod a+r test.sh\n[ssasidharan@lnx201 ~]$ chmod +r test.sh\n(If you want to change owner or group of a file/folder, you can do that with chown and chgrp. This probably is not immediately useful; it is enough to know that these commands exist.)"
  },
  {
    "objectID": "theme2/SF100/index.html#noteworthy-facts-about-file-names",
    "href": "theme2/SF100/index.html#noteworthy-facts-about-file-names",
    "title": "Linux, Command Line, and Scripting",
    "section": "Noteworthy facts about file names",
    "text": "Noteworthy facts about file names\n\nFile/folder names that begin with a . (period character) are “hidden”: meaning that they will not be listed in the output of ls command by default. You can list them with ls -a. They are also called dotfiles.\nConfiguration files for the programs you use (such as .bashrc for bash configuration) are often saved in hidden files. This way they usually stay out of your way without creating a clutter.\nFile/folder names and commands are case sensitive in Linux. Thus Notes.txt and notes.txt and NOTES.TXT are all distinct files.\nAs a matter of convenience, it is better to avoid spaces and special characters in file/folder names, as it will make tasks a little more difficult. If you need to represent spaces between words, you can replace spaces with _ (the underscore character)."
  },
  {
    "objectID": "theme2/SF100/index.html#io-redirection",
    "href": "theme2/SF100/index.html#io-redirection",
    "title": "Linux, Command Line, and Scripting",
    "section": "I/O redirection",
    "text": "I/O redirection\nI/O redirection lets us to change where standard output gets printed. To redirect standard output, we use the &gt; operator.\n[ssasidharan@lnx201 ~]$ ls -l &gt; ls-output.txt\nAs a result of redirection, a new file named ls-output.txt will be created. You can view its contents using cat command.\n[ssasidharan@lnx201 ~]$ ls -l ls-output.txt\n-rw-r--r-- 1 ssasidharan chess 807 Apr  1 17:32 ls-output.txt\n[ssasidharan@lnx201 ~]$ cat ls-output.txt\ntotal 4\ndrwxr-xr-x 2 ssasidharan chess   28 Mar 28 09:36 bin\ndrwxr-xr-x 2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x 2 ssasidharan chess   30 Mar 26 15:22 Desktop\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Documents\nlrwxrwxrwx 1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r-- 1 ssasidharan chess 3254 Mar  7 15:55 helloworld.ipynb\n-rw-r--r-- 1 ssasidharan chess    0 Apr  1 17:32 ls-output.txt\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Music\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Pictures\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Public\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Templates\n-rwxr-xr-x 1 ssasidharan chess    0 Mar 28 13:39 test.sh\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Videos\nNote that if there already was a file named ls-output.txt, the redirection above would have overwritten its contents. You want to be careful about this.\nWhat if you want to discard stdout completely? You can redirect it to the special file /dev/null:\n[ssasidharan@lnx201 ~]$ ls -l &gt; /dev/null\nIf you want to append stdout to a file instead of overwriting it, you can use &gt;&gt; operator:\n[ssasidharan@lnx201 ~]$ ls -l &gt;&gt; ls-output.txt\nThe &lt; operator is a sort of inverse of the &gt; operator:\n[ssasidharan@lnx201 ~]$ echo \"Shall I compare thee to a summer’s day?\" &gt; sonnet18.txt\n[ssasidharan@lnx201 ~]$ cat sonnet18.txt\nShall I compare thee to a summer’s day?\n[ssasidharan@lnx201 ~]$ cat &lt; sonnet18.txt\nShall I compare thee to a summer’s day?"
  },
  {
    "objectID": "theme2/SF100/index.html#pipes",
    "href": "theme2/SF100/index.html#pipes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Pipes",
    "text": "Pipes\nPrograms can write to standard output. Programs can also read from standard input. This means we can “chain” them together, such that one programs standard output is “piped” into another program’s standard input.\nThe operator to do this is | (vertical bar), also known as a pipe, and it is used in this manner: command1 | command2.\n[ssasidharan@lnx201 ~]$ ls -l /bin/ | less\nThe output of ls -l /bin is fairly large, so we pipe it into less, which allows you to scroll the output backward and forward, using up and down keyboard keys.\nYou can form longer pipes like this:\n[ssasidharan@lnx201 ~]$ ls /bin /usr/bin /sbin /usr/sbin | sort | uniq | wc\n   4289    4288   46820\n\nsort will sort lines of text files.\nuniq is used to filter adjacent matching lines the output of sort.\nwc is a word count program. It counts lines, words, and bytes present in its input."
  },
  {
    "objectID": "theme2/SF100/index.html#listing-processes",
    "href": "theme2/SF100/index.html#listing-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Listing processes",
    "text": "Listing processes\nYou can list running processes using ps command:\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 694411 pts/81   00:00:00 ps\n3479688 pts/81   00:00:00 bash\nBy default, ps prints processes of the current user and terminal in four columns:\n\nPID is process id.\nTTY is the terminal associated with the process.\nTIME is the elapsed CPU time for the process.\nCMD is the command that created the process.\n\nUsually there are many more processes running in the system, and sometimes they were started by other users. You can list them, with more detail, by passing some options to ps:\n[ssasidharan@lnx201 ~]$ ps -ef | head\nUID          PID    PPID  C STIME TTY          TIME CMD\nroot           1       0  0 Jan10 ?        03:14:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 22\nroot           2       0  0 Jan10 ?        00:01:12 [kthreadd]\nroot           6       2  0 Jan10 ?        00:12:16 [ksoftirqd/0]\nroot           7       2  0 Jan10 ?        00:01:10 [migration/0]\nroot           8       2  0 Jan10 ?        00:00:00 [rcu_bh]\nroot           9       2  0 Jan10 ?        11:14:26 [rcu_sched]\nroot          10       2  0 Jan10 ?        00:00:00 [lru-add-drain]\nroot          11       2  0 Jan10 ?        00:05:22 [watchdog/0]\nroot          12       2  0 Jan10 ?        00:00:24 [watchdog/1]\nRun man ps for details.\nPrograms like top will list processes in friendlier, fancier format."
  },
  {
    "objectID": "theme2/SF100/index.html#background-and-foreground-processes",
    "href": "theme2/SF100/index.html#background-and-foreground-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Background and foreground processes",
    "text": "Background and foreground processes\nBy default, commands run in the foreground: they do their thing, use the terminal (to read input, print output), and finally exit. You need to wait for a foreground process to end before you start the next one, or use another terminal.\nWhen have a long-running process, you have the option of sending it to the background, using the & operator:\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 949751\nYou can use Ctrl-ZCtrl-Z to stop a foreground process and send it to the background:\n[ssasidharan@lnx201 ~]$ sleep 100\n^Z\n[1]+  Stopped                 sleep 100\nYou can list background processes using jobs command:\n[ssasidharan@lnx201 ~]$ jobs\n[1]-  Running                 sleep 100 &\n[2]+  Stopped                 sleep 100\nYou can bring a background process to foreground using fg command, and you can terminate it using Ctrl-CCtrl-C:\n[ssasidharan@lnx201 ~]$ fg 2\nsleep 100\n^C\n[ssasidharan@lnx201 ~]$\nYou can use bg command to resume a stopped background process:\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 1746205\n[ssasidharan@lnx201 ~]$ sleep 100\n^Z\n[2]+  Stopped                 sleep 100\n[ssasidharan@lnx201 ~]$ jobs\n[1]-  Running                 sleep 100 &\n[2]+  Stopped                 sleep 100\n[ssasidharan@lnx201 ~]$ bg %2\n[2]+ sleep 100 &"
  },
  {
    "objectID": "theme2/SF100/index.html#terminating-processes",
    "href": "theme2/SF100/index.html#terminating-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Terminating processes",
    "text": "Terminating processes\nSometimes you might want to terminate a program, perhaps because it is using too much CPU or memory. You can find out the offending program’s ID using ps or top or htop, and then you can use kill command to end the process.\nBy default, kill sends a signal called SIGTERM (more on signals later). If SIGTERM is unable to terminate the process (such as when the program is ignoring SIGTERM), you can try SIGKILL:\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 796679 pts/116  00:00:00 bash\n1185454 pts/116  00:00:00 ps\n1748299 pts/116  00:00:00 sleep\n[ssasidharan@lnx201 ~]$ kill 1748299\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 796679 pts/116  00:00:00 bash\n1203470 pts/116  00:00:00 ps\n1748299 pts/116  00:00:00 sleep\n[ssasidharan@lnx201 ~]$ kill -SIGKILL 1748299\n[2]+  Killed                  sleep 100\nYou can use killall command to kill processes by name:\n[ssasidharan@lnx201 ~]$ killall sleep\nsleep(1469283): Operation not permitted\nsleep(1509215): Operation not permitted\nsleep: no process found\nIn the above example, you are not running a sleep process, but some other users are, and you are not allowed to terminate them."
  },
  {
    "objectID": "theme2/SF100/index.html#signals",
    "href": "theme2/SF100/index.html#signals",
    "title": "Linux, Command Line, and Scripting",
    "section": "Signals",
    "text": "Signals\nAs mentioned above, kill command sends signals to running processes, and we’ve already seen SIGTERM and SIGKILL. Signals are a process control mechanism. They are used to stop, resume, or terminate processes, and more.\nWhen we use Ctrl-CCtrl-C or Ctrl-ZCtrl-Z, we are sending signals to process – SIGINT (or “keyboard interrupt”) and SIGTSTP (or “terminal stop”), respectively.\nSignals have numbers: SIGKILL is 9, so you can use kill -9 &lt;pid&gt; instead of kill -SIGKILL &lt;pid&gt;. You can also omit the SIG prefix, and use kill -KILL &lt;pid&gt;.\nHere are some common signals:\n\n       Signal     Value     Action   Comment\n       ──────────────────────────────────────────────────────────────────────\n       SIGHUP        1       Term    Hangup detected on controlling terminal\n                                     or death of controlling process\n       SIGINT        2       Term    Interrupt from keyboard\n       SIGQUIT       3       Core    Quit from keyboard\n       SIGILL        4       Core    Illegal Instruction\n       SIGABRT       6       Core    Abort signal from abort(3)\n       SIGFPE        8       Core    Floating point exception\n       SIGKILL       9       Term    Kill signal\n       SIGSEGV      11       Core    Invalid memory reference\n       SIGPIPE      13       Term    Broken pipe: write to pipe with no\n                                     readers\n       SIGALRM      14       Term    Timer signal from alarm(2)\n       SIGTERM      15       Term    Termination signal\nRun the command man 7 signal to read signal command’s manual page."
  },
  {
    "objectID": "theme2/SF100/linux-commandline-scripting.html",
    "href": "theme2/SF100/linux-commandline-scripting.html",
    "title": "Intro to Linux, the command line, and programming in Python",
    "section": "",
    "text": "Welcome to Systems Fundamentals! In this section, we’re going to learn how to use the computing resources available at CHESS. In particular, we’ll focus on the Linux cluster available to researchers. How to access these computers remotely, how to use them via their command-line interface, and how to write actual, legitimate Python programs are the three main topics. In addition, a “bonus” section at the end will look deeper into what is really happening on the servers when we do things like access files or run programs.\nThere is a lot of material to cover, and in truth we’ll still just be scratching the surface. The goal here is to give you enough information to get started and to find more information as you need it. We recommend you explore each of the following sections in order.\n\nGetting Started (logging in and out with NoMachine and ssh)\nCommand Line Basics\nWriting Standalone Python Programs\nHow Linux Really Works\n\nShall we begin? Go ahead, click on the link for “Getting Started”, and let’s connect to a server!"
  },
  {
    "objectID": "theme2/SF101/containers-and-virtualization.html",
    "href": "theme2/SF101/containers-and-virtualization.html",
    "title": "Containers and Virtualization",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme1/PE101/PE101-02CondaRepos.html",
    "href": "theme1/PE101/PE101-02CondaRepos.html",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "",
    "text": "As we’ve mentioned, there is an awful lot of Python code out in the world completely free for us to use. There packages as broadly useful as “SciPy” (numerical methods for the sciences) and as narrowly interesting as “bosch-thermostat-client” for setting values in Bosch Thermostats. With so many packages available, if there is something you need to do in a Jupyter notebook or in a Python program, there is a good chance someone else has done at least part of it and made it available as a package.\nPublicly-available packages have to be kept somewhere to be useful. If programmers can’t find them, then they might as well not be made public. Fortunately, the Python world has a central repository - www.pypi.org. The repository (usually shortened to just “repo”, both vowels are long) is searchable.\nOne thing to notice in the repository is that packages there have version numbers. It’s also pretty common for one package to require another - scikit-learn requires scipy, for instance. Sometimes the requirements also have version numbers. There can be cases where Package A requires Package B, and specifically Package B has to have a version number greater than or equal to 11. When you have a lot of packages to import it can get cumbersome to check all the dependencies and make sure you have a combination that satisfies all the constraints.\nIn fact, it’s more than just cumbersome. Automatic checking is the only practical solution once the problem gets much size to it, and a piece of software that does this is called a SAT solver."
  },
  {
    "objectID": "theme1/PE101/PE101-02CondaRepos.html#conda---hard-problems-made-solvable",
    "href": "theme1/PE101/PE101-02CondaRepos.html#conda---hard-problems-made-solvable",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Conda - hard problems made solvable",
    "text": "Conda - hard problems made solvable\nFortunately, there is Conda, a software tool and a repository of its own. The conda developers keep a subset of the half billion packages that are available and they ensure that their repository reflects a combination of versions that should work together. They do the hard work, we take advantage of it. They stay in business by selling their tools to commercial users but, being a research organization, we’re not required to pay.\nConda also has another useful trick: it can take advantage of Python’s virtual environments to let you load outside packages into a completely private space. This way, when you download and install the “instantnobelprize” package (I made that up), it’s only written to your own directories. Other users, and the system as a whole, are protected from whatever it might contain.\nSetting up Conda and using it with Jupyter notebooks takes a little bit of work and has to be done from the command line, but so often it’s worth it. If you haven’t used the command line yet, take a look at the training units in SF100 on the Linux command line and scripting.\nWhat follows is taken directly from the CLASSE wiki entry for JupyterHub with just a few modifications."
  },
  {
    "objectID": "theme1/PE101/PE101-02CondaRepos.html#python-environments",
    "href": "theme1/PE101/PE101-02CondaRepos.html#python-environments",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Python Environments",
    "text": "Python Environments\nA Python environment is a local, unique to a user, repository plus a copy of the Python interpreter itself. Having a private environment is how we can load specific versions of packages even when the server has a different one. It even lets us install specific versions of python without affecting anyone else.\nWhen you launch a new notebook, you are presented with a dropdown to select your desired python kernel. The default Python 3 kernel is a CLASSE-IT maintained conda environment in /nfs/opt/anaconda3/envs/python3"
  },
  {
    "objectID": "theme1/PE101/PE101-02CondaRepos.html#adding-new-environments",
    "href": "theme1/PE101/PE101-02CondaRepos.html#adding-new-environments",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Adding New Environments",
    "text": "Adding New Environments\nIn addition, you can install your own python environments and have them added as an option when creating new notebooks.\nCreate your own python environment using your desired python installation. Please see LinuxSoftwareDevelopment for a list of centrally maintained python environments, and further down LinuxSoftwareDevelopment for tips on creating your own conda installation.\nInstall anything you like in the environment, but you MUST at least install ipykernel. For example\npip install ipykernel\nActivate the new environment. If using conda, this would look something like:\nsource /path/to/conda/install/bin/activate conda activate my-python-env\nAdd the virtual environment as a jupyter kernel using\npython -m ipykernel install --user --name=my-python-env --display-name \"My Python Env\"\nThis adds the kernel to ~/.local/share/jupyter/kernels/ and now it can be used by Jupyter. When you create a new notebook now, “My Python Env” will be one of your choices."
  },
  {
    "objectID": "theme1/PE101/PE101-01Packages.html",
    "href": "theme1/PE101/PE101-01Packages.html",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "By itself, Python provides everything you need to write programs. These programs won’t have a fancy user interface and they may not run very fast, but they’ll work. If that’s all Python offered, it might have become a popular language but it wouldn’t have taken over most of the world the way it has. No, what Python has going for it is a simple way to take commonly-used chunks of code, wrap them up neatly into sharable budles, and distribute those bundles far and wide. The mechanism for doing this in Python is called packages.\nIn this training unit, PE101-01, we’re going to look at some of the packages that come with Python. These are packages that you can count on being available anywhere you can run Python. In the next unit, PE101-02, we’ll look at how to find and use packages hosted in repositories available to anyone but not necessarily already installed where you’re running your programs.\nPython is, by itself, a rather simple language. The PE100 series of units has introduced you to almost all of the language. The language is kept small by moving the “nice to have, but not really necessary” parts into their own independent packages. Let’s start with an example:\nimport math\n\nprint(\"pi equals\", math.pi)\nprint(\"There are\", math.perm(52,2), \"possible outcomes when drawing two cards from a deck\")\nprint(\"The natural logarithm of 7.994 is\", math.log(7.994))\n\npi equals 3.141592653589793\nThere are 2652 possible outcomes when drawing two cards from a deck\nThe natural logarithm of 7.994 is 2.0786912602891316"
  },
  {
    "objectID": "theme1/PE101/PE101-01Packages.html#packages",
    "href": "theme1/PE101/PE101-01Packages.html#packages",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Packages",
    "text": "Packages\nThere are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\nimport random as rand\n\nprint(rand.random())\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details."
  },
  {
    "objectID": "theme1/PE101/PE101-01Packages.html#modules",
    "href": "theme1/PE101/PE101-01Packages.html#modules",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Modules",
    "text": "Modules\nSo far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\nimport os.path as op\n\nif op.exists(\"/usr/bin\"):\n    print(\"all is good.\")\nelse:\n    print(\"I don't even know how the server booted!\")\n    \n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory."
  },
  {
    "objectID": "theme1/PE101/PE101-01Packages.html#coming-up-next-packages-from-the-outside-world",
    "href": "theme1/PE101/PE101-01Packages.html#coming-up-next-packages-from-the-outside-world",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Coming up next: Packages from the outside world",
    "text": "Coming up next: Packages from the outside world\nAs we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks."
  },
  {
    "objectID": "theme1/PE103/vcs-testing-debugging.html",
    "href": "theme1/PE103/vcs-testing-debugging.html",
    "title": "Version Control, Testing and Debugging",
    "section": "",
    "text": "Version Control, Testing and Debugging\n\nVersion Control\nTesting\nDebugging"
  },
  {
    "objectID": "theme1/PE103/testing.html",
    "href": "theme1/PE103/testing.html",
    "title": "Testing",
    "section": "",
    "text": "Testing proves that your code works as expected in response to the inputs that it is expected to receive.\nOf course you can “manually” run your code and check that it works as expected. Since frequent manual testing will soon become tedious, you can also write code that tests your code, so that much of your testing is automated.\nWhen you test your code, you will be more confident about the correctness of your code. As your project evolves, the tests you have written will help you to test the changes more confidently. Your tests will also serve as a sort of documentation about how to use the code.\nThere are several ways to test your code:\n\nUnit tests test the smaller units of your code, namely: functions, classes, and methods.\nIntegration tests are used to prove that your code works correctly when they interface with externals systems (for example: you read weather data from NOAA).\nSystem tests are used to test that your application as a whole works as expected.\n\nThis is a big topic. In the interest of practicality, we will limit the discussion to unit tests here.\n\n\nPython standard library ships a unittest module, which provides some tools for testing your code. Let us see how this works in practice with a quick example.\nSuppose you have written a method for temperature conversion, in a module named temperature.py:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"\n    Convert temperature from Celsius to Fahrenheit.\n    \n    :param celsius (float): Temperature in Celsius\n    \n    :returns: Temperature converted to Fahrenheit\n    \"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\nYou will write tests for your code in a corresponding module named test_temperature.py:\n\n\ntest_temperature.py\n\nimport unittest\n\nfrom temperature import celsius_to_fahrenheit\n\nclass TestCelsiusToFahrenheit(unittest.TestCase):\n    def test_conversion(self):\n        # Test conversion for 0°C\n        self.assertEqual(celsius_to_fahrenheit(0), 32)\n\n        # Test conversion for 100°C\n        self.assertEqual(celsius_to_fahrenheit(100), 212)\n\n        # Test conversion for negative temperature -10°C\n        self.assertEqual(celsius_to_fahrenheit(-10), 14)\n\nif __name__ == '__main__':\n    unittest.main()\n\nAnd then you will run your tests:\n$ python3 test_temperature.py \n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nAs you see, unittest module provides:\n\nA TestCase class, which has an assertEqual() method (and several other assert methods) to check that the code has executed as expected.\n\n\nA unittest.main() function method which provides a way to run the test.\n\nThe TestCase class also provides methods to handle test fixtures to set things up before tests are run, and tear them down later, namely setUp() and tearDown().\n\n\nWhat would a failing test look like? Let us add a new test case to our TestCelsiusToFahrenheit class:\n    def test_conversion_bad_input(self):\n        # Test with bad input\n        self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\nThis will of course fail, and leave us enough hints about why it failed:\n$ python3 test_temperature.py \n.E\n======================================================================\nERROR: test_conversion_bad (__main__.TestCelsiusToFahrenheit.test_conversion_bad)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/test_temperature.py\", line 18, in test_conversion_bad\n    self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 14)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/temperature.py\", line 9, in celsius_to_fahrenheit\n    fahrenheit = (celsius * 9/5) + 32\n                  ~~~~~~~~~~~^~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nFAILED (errors=1)\nNow you can decide how to fix the code (or the test), or whether to fix anything at all. You may want to reject inputs other than numbers in your celsius_to_fahrenheit() function. Or you may decide that failing on unexpected inputs is fine, and change the test accordingly:\n    def test_conversion_bad_input_expect_exception(self):\n        # Test with bad input, and expect an exception\n        with self.assertRaises(TypeError):\n            celsius_to_fahrenheit(\"not temperature\")\n\n\n\n\nPyTest is a framework for writing and running tests. You can use PyTest along with unittest module.\nYou will need to install pytest with:\n$ pip install pytest\nPyTest provides a commandline program named pytest, which will discover the tests you have written in your project and run them. The output is a little fancier and probably more helpful than simply running the test module directly:\n$ pytest \n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\nplugins: anyio-4.3.0\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9/5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.08s ==========================\n\n\n\nWhen you share your code with your colleagues, you will want to make sure that your code works in environments other than your own too. Tox will help you to test that your project builds and installs correctly under several environments. You might want to test your code with several versions of Python, and various dependencies, for example.\nYou will write a configuration file named tox.ini:\n\n\ntox.ini\n\n[tox]\nrequires =\n    tox&gt;=4\nenv_list = py{38,39,310,311}\n\n[testenv]\ndescription = run unit tests\ndeps =\n    pytest&gt;=7\ncommands =\n    pytest {posargs:tests}\n\nYou can install tox like so:\n$ pip install tox\nAnd then run tox like so:\n$ tox\npy38: skipped because could not find python interpreter with spec(s): py38\npy38: SKIP ⚠ in 0.01 seconds\npy39: skipped because could not find python interpreter with spec(s): py39\npy39: SKIP ⚠ in 0 seconds\npy310: skipped because could not find python interpreter with spec(s): py310\npy310: SKIP ⚠ in 0 seconds\npy311: commands[0]&gt; pytest\n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\ncachedir: .tox/py311/.pytest_cache\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:19: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9 / 5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.04s ==========================\npy311: exit 1 (0.29 seconds) /home/sajith/projects/x-cite/X-CITE/theme1/PE103&gt; pytest pid=935973\n  py38: SKIP (0.01 seconds)\n  py39: SKIP (0.00 seconds)\n  py310: SKIP (0.00 seconds)\n  py311: FAIL code 1 (0.32=setup[0.04]+cmd[0.29] seconds)\n  evaluation failed :( (0.41 seconds)"
  },
  {
    "objectID": "theme1/PE103/testing.html#writing-unit-tests",
    "href": "theme1/PE103/testing.html#writing-unit-tests",
    "title": "Testing",
    "section": "",
    "text": "Python standard library ships a unittest module, which provides some tools for testing your code. Let us see how this works in practice with a quick example.\nSuppose you have written a method for temperature conversion, in a module named temperature.py:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"\n    Convert temperature from Celsius to Fahrenheit.\n    \n    :param celsius (float): Temperature in Celsius\n    \n    :returns: Temperature converted to Fahrenheit\n    \"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\nYou will write tests for your code in a corresponding module named test_temperature.py:\n\n\ntest_temperature.py\n\nimport unittest\n\nfrom temperature import celsius_to_fahrenheit\n\nclass TestCelsiusToFahrenheit(unittest.TestCase):\n    def test_conversion(self):\n        # Test conversion for 0°C\n        self.assertEqual(celsius_to_fahrenheit(0), 32)\n\n        # Test conversion for 100°C\n        self.assertEqual(celsius_to_fahrenheit(100), 212)\n\n        # Test conversion for negative temperature -10°C\n        self.assertEqual(celsius_to_fahrenheit(-10), 14)\n\nif __name__ == '__main__':\n    unittest.main()\n\nAnd then you will run your tests:\n$ python3 test_temperature.py \n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nAs you see, unittest module provides:\n\nA TestCase class, which has an assertEqual() method (and several other assert methods) to check that the code has executed as expected.\n\n\nA unittest.main() function method which provides a way to run the test.\n\nThe TestCase class also provides methods to handle test fixtures to set things up before tests are run, and tear them down later, namely setUp() and tearDown().\n\n\nWhat would a failing test look like? Let us add a new test case to our TestCelsiusToFahrenheit class:\n    def test_conversion_bad_input(self):\n        # Test with bad input\n        self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\nThis will of course fail, and leave us enough hints about why it failed:\n$ python3 test_temperature.py \n.E\n======================================================================\nERROR: test_conversion_bad (__main__.TestCelsiusToFahrenheit.test_conversion_bad)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/test_temperature.py\", line 18, in test_conversion_bad\n    self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 14)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/temperature.py\", line 9, in celsius_to_fahrenheit\n    fahrenheit = (celsius * 9/5) + 32\n                  ~~~~~~~~~~~^~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nFAILED (errors=1)\nNow you can decide how to fix the code (or the test), or whether to fix anything at all. You may want to reject inputs other than numbers in your celsius_to_fahrenheit() function. Or you may decide that failing on unexpected inputs is fine, and change the test accordingly:\n    def test_conversion_bad_input_expect_exception(self):\n        # Test with bad input, and expect an exception\n        with self.assertRaises(TypeError):\n            celsius_to_fahrenheit(\"not temperature\")"
  },
  {
    "objectID": "theme1/PE103/testing.html#pytest",
    "href": "theme1/PE103/testing.html#pytest",
    "title": "Testing",
    "section": "",
    "text": "PyTest is a framework for writing and running tests. You can use PyTest along with unittest module.\nYou will need to install pytest with:\n$ pip install pytest\nPyTest provides a commandline program named pytest, which will discover the tests you have written in your project and run them. The output is a little fancier and probably more helpful than simply running the test module directly:\n$ pytest \n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\nplugins: anyio-4.3.0\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9/5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.08s =========================="
  },
  {
    "objectID": "theme1/PE103/testing.html#tox",
    "href": "theme1/PE103/testing.html#tox",
    "title": "Testing",
    "section": "",
    "text": "When you share your code with your colleagues, you will want to make sure that your code works in environments other than your own too. Tox will help you to test that your project builds and installs correctly under several environments. You might want to test your code with several versions of Python, and various dependencies, for example.\nYou will write a configuration file named tox.ini:\n\n\ntox.ini\n\n[tox]\nrequires =\n    tox&gt;=4\nenv_list = py{38,39,310,311}\n\n[testenv]\ndescription = run unit tests\ndeps =\n    pytest&gt;=7\ncommands =\n    pytest {posargs:tests}\n\nYou can install tox like so:\n$ pip install tox\nAnd then run tox like so:\n$ tox\npy38: skipped because could not find python interpreter with spec(s): py38\npy38: SKIP ⚠ in 0.01 seconds\npy39: skipped because could not find python interpreter with spec(s): py39\npy39: SKIP ⚠ in 0 seconds\npy310: skipped because could not find python interpreter with spec(s): py310\npy310: SKIP ⚠ in 0 seconds\npy311: commands[0]&gt; pytest\n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\ncachedir: .tox/py311/.pytest_cache\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:19: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9 / 5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.04s ==========================\npy311: exit 1 (0.29 seconds) /home/sajith/projects/x-cite/X-CITE/theme1/PE103&gt; pytest pid=935973\n  py38: SKIP (0.01 seconds)\n  py39: SKIP (0.00 seconds)\n  py310: SKIP (0.00 seconds)\n  py311: FAIL code 1 (0.32=setup[0.04]+cmd[0.29] seconds)\n  evaluation failed :( (0.41 seconds)"
  },
  {
    "objectID": "theme1/PE100/PE100-01Introduction.html",
    "href": "theme1/PE100/PE100-01Introduction.html",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "",
    "text": "Welcome to the Programming Essentials - Python Programming and Jupyter Notebooks component of the X-CITE training materials. The intent behind this unit is to show new CHESS users how to write programs in Python for experimental data analysis.\nPrerequisites? None.\nThe training materials before you are designed both for scientists who may not have any programming experience whatsoever and for those who have at least some basic programming capability but in a language other than Python.\nPython’s adoption has exploded in the last decade. Much of its success can be attributed to productivity. Many programming languages force the programmer to deal with very small details to do even the simplest things. Python’s attitude is to just take care of all the minutae so we don’t have to. On top of that, Python’s popularity has resulted in hundreds of thousands of packages of useful code for specific tasks. If there is something you need to write a program for, it’s almost definite that someone else has had the same problem. There’s a good chance at least one of those people neatly wrapped up some of their code and made it available in one of the repositories on the internet. There’s no reason for you to reinvent the proverbial wheel again - take advantage of their work (don’t forget to cite it!) and get back to doing actual science that much sooner.\nYou’re currently looking at PE100-01 Introduction. If you are new to Python and especially if you’re new to programming, you should work through each of the modules in order. More experienced programmers might benefit from skipping directly to topics that interest them. Select one of the following:\n\nPE100-02 Types, Variables, and Operators - the heart of any programming language.\nPE100-03 Decision Structures - conditional statements (“if” statements”) change the program flow.\nPE100-04 Repetition - “while” and “for” loops let us do things over and over.\nPE100-05 Functions - Python comes with a lot of functions, but we can write even more.\nPE100-06 Files - Reading input from and storing your results to disk.\nPE100-07 Exceptions - Dealing with unexpected contingencies.\nPE100-08 Lists - Another kind of variable, and the key to structuring data storage.\nPE100-09 Strings - More details on working with text.\nPE100-10 Dictionaries - Like a simple database, look up information quickly."
  },
  {
    "objectID": "theme1/PE100/PE100-10Dictionaries.html",
    "href": "theme1/PE100/PE100-10Dictionaries.html",
    "title": "PE100-10: Dictionaries",
    "section": "",
    "text": "Dictionaries are, conceptually, a special type of list. A list has an order to it. Elements are placed into a list in a particular order. Removing and inserting items changes the order in a well understood way. Because of this, yoiu can always access a list by looking into it at a specific integer index. None of this is possible with a dictionary. Instead, dictionaries have a better trick: all access is by searching.\nPython’s dictionaries store each of their elements as a pair of things, a key and a value. The key is the thing that can be searched for, and the value is the information that will be retured when that element is read.\nAn example will help:\n\nfavorite_food = { \"Alice\" : \"Apple Pie\", \\\n                 \"Bob\" : \"Ice Cream\",\\\n                 \"Charlie\" : \"Pizza\" }\nprint(favorite_food[\"Bob\"])\n                 \n\nIce Cream\n\n\nLet’s take that apart and see how it works.\n\nWe have the creation of the dictionary itself. We create a dictionary literal much like we do a list, only here we use curly brackets instead of square brackets.\nWe have key:value pairs. The keys we’re using are all strings (people’s names, in fact) but they could just as well be numbers. The values are also strings, but they can be anything we want. These keys and values are separated by colons (:).\nBetween each of the key:value pairs there is a comma.\nWe see how to look up information in a dictionary. It looks like indexing into a list to extract a particular element, except in this case we don’t give a positional index number but rather we give it a key to look up.\n\nWhen you think about it, looking for Bob’s favorite food by using favorite_food[\"Bob\"] is a pretty powerful tool. Rather than having to specify where to get something from, we can just specify what to get. This is why you’ll occasionally see dictionaries refered to as “Content Addressable Memory”. It’s nice to just get the data we want without having to step through every element of a list looking for it. It’s also faster: Python’s dictionaries use clever indexing so they can straight to what you’re looking for.\nDictionaries get their name, by the way, from real-world physical dictionaries. Suppose you have Webster’s 9th New Collegiate Dictionary in front of you. There are a lot of keys in there - each one of those words in alphabetical order is a key. There’s not really any way to look up something by page number alone - there’s no algorithm to tell me what page the definition for brisance is on. There’s no way to look up a word by knowing which word number it is. If brisance is the 8000th word in the dictionary, that knowledge does me no good. On the other hand, there is an algorithm for finding that definition by looking up the key word. I go to the “B”s, look for the “Br” part, and so forth until I find brisance. I can only look up things by key, not by position.\nOne of the great things about Python is its flexibility with data types. Lists can contain any data type. You can have lists of tuples of lists of strings if you want to. Dictionaries are similarly versatile. You can have dictionaries that contain, say, lists:\n\nfamily_info = { 'ages' : [6,8,36,38],\\\n               'names' : ['Jane', 'John', 'Alice', 'Bob'] }\n\nprint(family_info[\"names\"])\n\n['Jane', 'John', 'Alice', 'Bob']\n\n\nYou can even have dictionaries of dictionaries. They’re quite useful, in fact.\n\nfast_food = { \"McAwful\" : { \"address\" : \"1012 Western Blvd\",\\\n                            \"sanitation\" : 92} ,\\\n            \"Davids\" : { \"address\" : \"201 S Fayetteville St\",\\\n                         \"sanitation\" : 99.5 } }\n\nprint (fast_food[\"Davids\"][\"sanitation\"])\n\n99.5\n\n\nDictionaries give us the beginnings of a database. It’s not as powerful as a “real” database, but it’s good enough for a lot of things. Of course, a dictionary is like any iother variable: it only lasts as long as your program is running. You would have to combine a dictionary with some file access to have any permanent storage.\nWhat happens if we try to look up a key:value pair, and the key isn’t in the dictionary? Let’s see!\n\nprint(fast_food[\"Ruth's Chris\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(fast_food[\"Ruth's Chris\"])\n\nKeyError: \"Ruth's Chris\"\n\n\n\nYeah, we expected that by now, didn’t we? It threw a “KeyError” exception. What should we do about this? We could always wrap the access up in a try/except structure to catch the KeyError, but this is such a common problem that Python gives us a friendlier way to do it: the in operator.\nRemember using in to see if something was in a string? This is philosophically similar. Let’s try it:\n\nif \"Ruth's Chris\" in fast_food:\n    print(\"Surprised to see such an expensive place here!\")\nelse:\n    print(\"that's a relief, actually.\")\n\nthat's a relief, actually.\n\n\nAdding to a dictionary is even easier than adding to a list. All you do is just act like the key was already there and assign it a value:\n\nprint(favorite_food)\nprint()\nfavorite_food[\"Dan\"] = 'Fish'\nprint(\"Now we have:\")\nprint(favorite_food)\n\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Pizza', 'Dan': 'Fish'}\n\nNow we have:\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Pizza', 'Dan': 'Fish'}\n\n\nWhat happens if we try to overwrite some data?\n\nfavorite_food[\"Charlie\"] = \"Soup\"\nprint(favorite_food)\n\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Soup', 'Dan': 'Fish'}\n\n\nWe can change what is stored in the “value” part of the key:value pair any time we want. We can’t change the key, though. At least, we can’t change it directly. We can always delete the existing key:value pair and replace it with a new one. Let’s say Charlie really wants to be known as Chuck. He has his reasons. So let’s fix the favorite_food dictionary:\n\nmunchie = favorite_food[\"Charlie\"]\ndel favorite_food[\"Charlie\"]\nfavorite_food[\"Chuck\"]=munchie\nprint(favorite_food)\n\nWhat we did to accomplish that was 1. Look up Charlie’s favorite food and save that value. 2. use the built-in del operator to remove Charlie as a key and whatever value was associated with him. 3. Insert a new key:value pair whose key is “Chuck” and whose value is whatever we looked up before.\nSince every other Python data type that holds more than one thing can work with the built-in len() function, it stands to reason that dictionaries can, too. And as you would imagine, len() returns the number of entries in the dictionary.\n\nlen(favorite_food)\n\n4\n\n\n\nLooping and Iteration\n\nA dictionary is another type of iterable. This means we can write loops that traverse the entire dictionary, start to finish, and do something useful.\n\nfor key in favorite_food:\n    print(key)\n\nAlice\nBob\nDan\nChuck\n\n\nNotice that a traversal of a dictionary retrieves the keys. If you want to retrieve the values, just use the keys to look up the values.\nfor key in favorite_food: print(favorite_food[key])"
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html",
    "href": "theme1/PE100/PE100-05Functions.html",
    "title": "PE100-05: Functions",
    "section": "",
    "text": "Functions in Python are very, very similar to functions in mathematics. Our functions take one or more input values and transform them into precisely one output. Let’s start with an example.\ndef inductiveReactance(l, f):\n    reactance = 2 * 3.14159 * f * l\n    return reactance\n\nprint (inductiveReactance(1e-3, 2e6))\n\n12566.36\nThe above code defines a function named “inductiveReactance” that accepts two input parameters. The first one, l, is the amount of inductance a coil has in henrys. The second one, f, is the frequency of interest (in hertz). We can call that function with parameters of one millihenry and two megahertz. The function computes the value 12566.36 (the unit is ohms) and returns that to the code that called it. In this case, it was a print statement that called it.\nTake a look at the first line. The very first thing is the keyword def (shortened from the word “define”). After the “def” is where you specify the name of the new function you want to create. In this case, it’s “inductiveReactance”. Next is a list of names of parameters, enclosed in parentheses. In our example, the parameters are “l” and “f”. Finally at the end of the line is a colon.\nOnce that first line is done, the next step is to write the body of the function. Just like an “if”, “while”, or “for” statement, the code block has to be indented consistently. Our example function computes the reactance of the device in question. On the last line of the function, the return statement is how the function sends its computed value back to the code that called the function in the first place. Every function should have at least one return statement. I won’t get drawn into this debate: some say a function should have precisely one return statement and utilize whatever logical means necessary to make sure that all code paths eventually lead to it. Others say it’s not a problem at all for a function to have multiple return statements (and hence multiple ways for a function to end) if it makes the logic more understandable. Personally, I try to minimize the number of return statements in my functions but I’m by no means a zealot on this one. If I need seven different places to exit the function and return a value then so be it."
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#encapsulation",
    "href": "theme1/PE100/PE100-05Functions.html#encapsulation",
    "title": "PE100-05: Functions",
    "section": "Encapsulation",
    "text": "Encapsulation\nFunctions are useful in programming for the same reason they’re useful in math - ours encapsulate a chunk of code so you don’t have to think about what is in it every time. Imagine how tedious it would be to write a program that needed to compute cosine in a lot of different places in the code. You could, I suppose, type in a Taylor series expansion for cosine in each of the places where we need to compute a cosign. That would be irritating, error prone, and confusing to anyone else who has to read it. Instead, we can write a function exactly once to compute cosine and then call that function from many places in our code. Once we have the function tested and debugged, we don’t have to think about it again. That frees up mental energy for more productive uses.\nFunctions can be classified into one of two types. Void Functions exist for encapsulation and don’t actually return a value. print() is an example of a void function. Value-Returning Functions, as the name strongly implies, return a value to the calling code. inductiveReactance() is an example of one.\nHere’s another example. This time, we’ll define a function that calls another function.\n\ndef squared(x):\n    return x ** 2\n\ndef circle_area(radius):\n    area = 3.14159 * squared(radius)\n    return area\n\n\nprint(\"Area of a circle with a radius of 2 is\", circle_area(2))\n\nArea of a circle with a radius of 2 is 12.56636\n\n\nWe defined a function to compute the area of a circle. It needed to square a number and so we decided to write a function to do that. Functions can call other functions ad infinitum. In fact, functions can even call themselves! When that happens the function is said to be recursive. Recursive functions are very useful for solving some hard problems but they’re a little beyond an introductory module like this one."
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#function-and-variable-naming",
    "href": "theme1/PE100/PE100-05Functions.html#function-and-variable-naming",
    "title": "PE100-05: Functions",
    "section": "Function (and Variable) Naming",
    "text": "Function (and Variable) Naming\nWhat kinds of names can we use for functions? The same ones we can use for variables! More specifically, * No keywords (e.g., False is invalid) * No spaces (e.g., my function is invalid) * The first character must be: * a-z, A-Z, or _ (the underscore character) * No numbers (e.g., 1st_function is invalid) * After the first character, the following are allowed: * a-z, A-Z, _, and 0-9 * No other symbols (e.g., get_room&board is invalid)\nAs a widely agreed upon best practice, names should be meaningful and be composed of lowercase characters with underscores as separators."
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#function-arguments",
    "href": "theme1/PE100/PE100-05Functions.html#function-arguments",
    "title": "PE100-05: Functions",
    "section": "Function Arguments",
    "text": "Function Arguments\nInput Parameters to functions are called arguments. They are the primary and best way to put information into a function, and definitely the way that causes the fewest problems. Arguments to a function in Python are mostly analagous to what we’re used to in math, but of course Python has some extensions.\nA function can have any number of arguments, including zero. “A function of zero arguments” might sound like a mathematician’s idea of “humor”, but it can actually make sense in programming. Sometimes you just need to encapsulate part of your code so you don’t have to worry with it again. For instance:\n\ndef say_hi():\n    print(\"==============================\")\n    print(\"==============================\")\n    print(\"Greetings, User. I'll start \")\n    print(\"loading the instrument config\")\n    print(\"files and opening connections\")\n    print(\"to them. It'll take a minute.\")\n    print(\"==============================\")\n    print(\"==============================\")\n\nsay_hi()\n\n==============================\n==============================\nGreetings, User. I'll start \nloading the instrument config\nfiles and opening connections\nto them. It'll take a minute.\n==============================\n==============================\n\n\nNow the code to print that banner is hidden away inside a function we’ll never have to look at again. Less mental clutter means fewer bugs.\nAnd for the sake of completeness, functions can also take one or more arguments:\n\ndef convert_to_miles(kilometers):\n    return kilometers * .6213712\n\ndef interesting_polynomial(a, b, c, d):\n    result = 2*(c**3) + 3.91*(c**2) + 1.1*c + d\n    return result\n\nprint(\"The race was\", convert_to_miles(10), \"miles long and my ankles were hurting the ENTIRE way.\")\n\nprint(\"The polynomial evaluates to:\", interesting_polynomial(7,4,8,1))\n\nThe race was 6.213712 miles long and my ankles were hurting the ENTIRE way.\nThe polynomial evaluates to: 1284.04\n\n\nWhen arguments are passed into a function, they become parameter variables and can be referred to inside the function just like any other variable. This handy because the variables inside a function are called local variables and they have special properties: nothing outside of the function can modify their value, they’re destroyed and re-created every time the function is called, and these local variables supercede any outside variables with the same name.\nTake a look for yourself:\n\ndef show_twice_the_wavelength(wavelength):\n    wavelength = wavelength * 2\n    print(\"Twice the wavelength is\", wavelength)\n\n# a good wavelength to listen for synchrotron radiation\n# emitted from Jupiter (the planet, not the software), in meters\nwavelength = 20\nshow_twice_the_wavelength(wavelength)\nshow_twice_the_wavelength(wavelength)\nshow_twice_the_wavelength(wavelength)\n\n    \n\nTwice the wavelength is 40\nTwice the wavelength is 40\nTwice the wavelength is 40\n\n\nDoes that seem odd to you? What happened is this: four lines from the bottom we created a variable named “wavelength” and set it to 20. We then called the function to print it out doubled. We passed the global variable “wavelength” to our function which took it as its only argument. That argument became a parameter variable that was coincidentally named “wavelength”. That “wavelength” parameter variable has nothing to do with the “wavelength” variable in the main part of the program. Our function doubles that parameter variable and prints it out. At that point, the function completes and the flow of control goes back to the main body.\nThe next time our function is called an entirely new, fresh set of variables and parameter variables is created. This is important - it means that if we call the function with the same value every time then we always get the same result. Functions are unable to save their “state”. Like a football player on a stretcher, they have no memory of what happened before.\n(OK, yes, there are ways for them to save their state. Sometimes it’s unavoidable and you just have to do it, but doing so makes more places for bugs to creep into your programs and makes it harder to understand later. Try to avoid it. We’ll talk about it later.)"
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#variable-scope",
    "href": "theme1/PE100/PE100-05Functions.html#variable-scope",
    "title": "PE100-05: Functions",
    "section": "Variable Scope",
    "text": "Variable Scope\nThe degree to which your programs can “see” a variable is called scope. There are two levels of scope in most Python programming:\n\nGlobal Scope\n\nDefined in main Python file\nOutside of ANY function\nTry to avoid these!\n\nConsidered poor design\nDangerous to use: any part of the program anywhere can change these\nBug Magnet!\n\n\nLocal Scope\n\nVariables defined within a function\nOnly visible and useable from inside their own function!\nUse these if at all possible.\n\n\nThe danger in global variables comes from two things. The first is the fact that the value can be changed anywhere in your program, either in the main program or inside of a function, and it’s devilishly hard to keep track of where that might be.\nThe second danger is more subtle. When a function saves a value into a global variable, the function is now said to have side effects. Side effects break the idea of isolation that functions are meant to give us. Imagine a mathematical function, such as tangent, if it had side effects. Calling tan(.0125) would not only result in the tangent of .0125, but it would have some other effect on some unrelated part of math. Imagine if calling tan caused your coordinate system to change every time? That would be insane.\nIt gets worse, though. What if our tangent function also read from a global variable and changed its behavior based on that. Then each time we called tan(.0125) we might get a different value.\nIn other words, we basically broke math.\nSimilarly, when we write programs, if our functions have side effects then we’ve complicated them tremendously. And more complication means more places for bugs to sneak into our code and they’ll also be harder to find.\nAs an aside, there is a style of programming that eliminates global variables and, to an extent, even local variables. It’s called functional programming, and Python has some support for that style. There is usually more than one way to do anything in Python, and experienced Pythonistas will usually try to choose the most Pythonic way. Part of being in Pythonic style means to use (at least partially) a functional style."
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#constants",
    "href": "theme1/PE100/PE100-05Functions.html#constants",
    "title": "PE100-05: Functions",
    "section": "Constants",
    "text": "Constants\nThere is an exception to the “no globals” rule: Constants. Just like in math, a constant is given a value once and never changed again. “Never changing” means “no side effects” so everything is OK. It is good practice to define your constants using ALL CAPITAL LETTERS.\n\nPLANCK = 6.626e-34\n\ndef photon_energy(freq):\n    gumption = PLANCK*freq\n    return gumption\n\nprint(photon_energy(3e14))\n\n1.9878e-19"
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#abstraction",
    "href": "theme1/PE100/PE100-05Functions.html#abstraction",
    "title": "PE100-05: Functions",
    "section": "Abstraction",
    "text": "Abstraction\nA valuable property of functions is how they isolate the code and variables inside of them from being manipulated elsewhere in your software. A consequence of that is their ability to “hide” detail from us. We’ve already talked about writing a function, debugging it, and never having to look at the code inside of it again. What is every bit as useful, if not more so, is using functions to provide abstraction.\nAbstraction is something we’ve used every day even if we haven’t thought it. Remember learning math? You started off counting things, and yes, that counts as math. If you had four bottle caps in one hand and three in the other, you could toss them all on the table, count them, and know that you have seven in total.\nThere are two problems with having to count everything. One is that the amount of stuff can get big in a hurry. Try using two hands and table to count sand grains. The other problem is that if there are any insights to be had, it’s hard to find them when you’re stuck down in the details. Fortunately, we learned arithmetic.\nArithmetic is great. We don’t have to deal with handfuls of stuff anymore. We can just use numbers and operators and get an answer without a bunch of messing around. We can start to see patterns we never would have just tossing bottle caps on the table. If we need to add 12 to something, we can instead add 10 and then add 2 more. This is so handy. Of course, it would be nice if we could just do something to analyze entire families of arithmetic problems.\nAlgebra lets us analyze entire families of arithmetic problems. We don’t have to fool with numbers if we don’t have to - we can just substitute variables in their place. We’ve hidden some of the complexity, like the petty little details of numbers, and abstracted that complexity away.\nSimilarly, a lot of problem solving is perfectly amenable to using abstraction. Let’s write a bit of code to run an experiment…\n\ndef run_experiment():\n    safe=is_it_safe_to_run()\n    if safe == True:\n        light_em_up()\n        put_facility_back_in_a_safe_state()\n        print(\"Better than Ghostbusters, huh?\")\n        \n\nThat function is a (admittedly fanciful) representation of running an experiment. It makes sense, anyone can understand it, and if there’s a bug in there then it’s going to be really obvious. The only problem: if we try to run it, it’ll crash because those other functions haven’t been defined yet. Shall we fix that?\n\ndef is_it_safe_to_run():\n    if badges_swiped_in() == 0 and shutter_closed() and not red_light_illuminated():\n        return True\n    else:\n        return False\n\ndef shutter_closed():\n    # put some code to interface with the solenoid sense switch here\n    return True;\n\ndef red_light_illuminated():\n    # more code, this time to see if the light is on...\n    return False\n\ndef badges_swiped_in():\n    # do some database lookups or something to see if we\n    # think anyone is still in the room.\n    return 0;\n\n\ndef light_em_up():\n    turn_on_red_light()\n    high_voltage(True)\n    shutter_open()\n    # a few seconds delay here, perhaps?\n\ndef put_facility_back_in_safe_state():\n    shutter_closed()\n    high_voltage(False)\n    turn_off_red_light()\n\nNotice how the program is broken up into several functions? The best part is that you don’t have to keep everything in your head. All you have to remember is the part you’re working on. Smaller pieces, fewer bugs."
  },
  {
    "objectID": "theme1/PE100/PE100-05Functions.html#modules",
    "href": "theme1/PE100/PE100-05Functions.html#modules",
    "title": "PE100-05: Functions",
    "section": "Modules",
    "text": "Modules\nOne reason Python has become so popular is the sheer amount of code that has been written in it and made available for public use. We’ve seen a few functions already that were built in to Python - int(), float(), and str(), for example - but there are many tens of thousands of modules that are freely available for use in your own software. Just picking five common ones at random:\n\nmath\nrandom\nos\nPyMySql\npsycopg\n\nThe first two contain functions for general-purpose math and for producing random numbers. The “os” module interfaces Python with the operating system the code is running on. PyMySql and psycopg provide connectivity to relational databases.\nRemember at the beginning of this lesson when we wrote a function to calculate inductive reactance? I put the value of pi in there as 3.14159, but that really isn’t anywhere near enough digits for some problems. Let’s fix that:\n\nimport math\nprint(math.pi)\n\n3.141592653589793\n\n\nThere are two things to note here. First, the keyword import is used to tell Python to go find a module with the right name and load it. The name we want it to find is the word right after the import. And secondly, just looking at the output we can see that there are a lot more digits than when we did something by hand in our Inductive Impedance example (top of this page). In general, using a module that was (a) written by someone else and (b) is widely used and has been checked by a lot of people is going to avoid a lot of bugs. For instance, I would never code my own Fast Fourier Transform. Instead, I would use the one in the “numpy” module. I know how easy it is to make a mistake and I trust their work a lot more than my own. They have tens or hundreds of thousands of users and scores of developers. I have… a copy of Numerical Recipes that’s old enough to run for President.\nSince we used the “math” module already, here’s a very incomplete list of what is in there: * sin(), cos(), tan(), acos(), asin(), atan()… - “acos” is “arc cosine”, etc. * log(), log10(), sqrt() - square root * radians(), degrees() - converts between them\nAnd lots more stuff. How do you know what’s in it? Go to the online documentation: https://docs.python.org/3/library/math.html\n\nRandom Numbers\nAnother module that is heavily used is “random”. It generates random numbers, yes, but it can also do things like take a list of things and shuffle them randomly.\n\nimport random\ninteger_number = random.randint(10,100)\nreal_number = random.random()\n\nprint(\"The random integer between 10 and 100 was:\", integer_number)\nprint(\"The random float between 0 and 1 is:\", real_number)\n\nThe random integer between 10 and 100 was: 20\nThe random float between 0 and 1 is: 0.6474502367565016\n\n\nThere are more functions available in the “random” module, including ones to select a real number from a non-uniform distribution. Take a look at https://docs.python.org/3/library/random.html\nHere’s a slightly more complicated example:\n\n# for 25 hypothetical proposals, use random() to decide\n# if it gets funded.\n\nfor proposal in range(0,25):\n    if random.random() &lt; 0.12:\n        print(\"Proposal number\", proposal,\"was funded!\")\n    else:\n        print(\"Don't feel bad... proposal number\", researcher, \"didn't get funded either.\")\n\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nProposal number 23 was funded!\nDon't feel bad... proposal number 24 didn't get funded either.\n\n\n\nLet’s try out what we’ve learned so far. Use the next code cell to write a bit of Python that simulates rolling a pair of dice and adds the two values. Print the value out.\nLet’s add to that… add a loop so that we keep doing that over and over until we get the same sum twice in a row. Some questions to ask yourself are “What kind of loop do I need?” and “How can I compare what happened between two different loop iterations?”"
  },
  {
    "objectID": "theme1/PE100/PE100-06Files.html",
    "href": "theme1/PE100/PE100-06Files.html",
    "title": "PE100-05: Files",
    "section": "",
    "text": "You can write a good deal of software that runs entirely inside Jupyter notebooks or that runs on the command line and only communicates through the screen and the keyboard. Sometimes, though, you have to do with files. It may not be practical to hardcode all your data into assignment statements in Python, or maybe you have to deal with a number of files and therefore you can’t use pipes. link to Sys Fundamentals page here\nWe’ve already seen two basic ways to do Input and Output (often referred to as “I/O”). We’ve used input() to read from the keyboard and print() to send output to the screen. Those functions work quite well, except you might have to do a lot of typing or deal with your output scrolling off the screen. In neither case is the data durable - it goes away as soon as the program is done or you close the Jupyter notebook.\nThe input() and print() functions are just the tip of the proverbial iceberg in terms of getting information in and out of running Python code. Some of our other options include: * GUI controls: text box, menu, dialog box… * Networks: HTTP, TCP/IP sockets, Infiniband… * Databases: Relational (SQL) and NoSQL * Other: cameras, microphones, speakers, LabView"
  },
  {
    "objectID": "theme1/PE100/PE100-06Files.html#files",
    "href": "theme1/PE100/PE100-06Files.html#files",
    "title": "PE100-05: Files",
    "section": "Files",
    "text": "Files\nPractically everyone is more-or-less familiar with the idea of a file, even if fairly few people know how they work. We’re going to ignore a lot of details for the moment and say this: a file is a long-lasting collection of bytes. It has a first byte, a last byte, and every one in between stays in the same order.\nThis begs the question “What is a byte?” A byte is just a small number from 0 to 255 (inclusive). We can assign meaning to those numbers, and if we’re smart about how we do it then we can represent any information a computer can process as long as we use enough of these bytes.\nWe like to think of files as being one of two types: binary files and text files. Binary files are pure data. We decide how to write bytes to a file to represent data. Then when we’re ready to read it in again, we read the bytes, process them somehow, and reconstruct the original data. It’s a great technique - it’s fast and efficient.\nWe won’t be talking about binary files in this notebook or even in this module. Fifteen years ago we wouldn’t have had a choice, we would have had to. These days, it’s unusual to have to deal with binary files, especially in Python, because there is so often a library function already available to do the work for us.\nText files, on the other hand, are probably something you’re already familar with - they are what you get when you edit a “plain text” file in “notepad” or “textedit”. In a text file, every one of the letter, number, and punctuation mark characters is assigned its own number. For instance, capital “A” is 65. “B” is 66. Not that it should ever matter, but here’s a complete list and then some!\nLet’s say you open an editor and type “CAT”. When you save that to a file, there will be a file that is three bytes long and contains the three bytes 67, 65, and 84. Actually there will usually be a fourth byte, 10, which is the character you get when you press “Enter” or “Return”.\nFor now, at least for a few minutes, we’re going to pretend the only language on earth is English. We’ll talk about other languages when we talk about networks.\nIt’s about time for an example, don’t you think?\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First Post!\")\nmy_file_object.close()\n\nThree lines of code was all it took to create a file, write to it, and tidy up after ourselves. What does each of those lines do?\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object is an object variable. Think of an object as a way to store data in a variable along with some functions that only make sense to that data. They hide a lot of complexity from us. A file object is one that keeps track of a filename, how to get to it, and how to use it. It has some functions built in to it to help us do things to the file.\nPython gives us the function “open”. It gets a file ready to be used by our code. It takes two arguments. The first is the file’s name, and the second is the mode we want to use the file in. In our example, we specified that the file’s name was “first_file.txt” and that it was in the “/tmp” directory. Then in the second argument we specified “w”, meaning we wanted to write to the file. The “w” mode will cause the file to be created if it didn’t already exist. If it did already exist, on the other hand, all the contents of it will be deleted and we’ll start writing from the beginning just as if the file was created from scratch. We’ll see more modes as we go.\nmy_file_object.write(\"First Post!\")\nThis line uses one of those functions that are tucked away inside an object. In this case, we’re calling the file object’s “write” function. It does what we expect - it takes its argument, in this case “First Post!”, and causes it to be written to disk byte by byte.\nmy_file_object.close()\nFinally, we call one more of the file object’s functions: close. When we run this, Python tells the operating system “Hey, we’re done with the file. You can get rid of any of the tedious housekeeping data that operating systems keep behind the scenes!”\nClosing files is considered “good programming hygene”. You’re allowed 1024 file objects to be open and connected to files in one program on the CLASSE cluster of computers. I’ll say from my experience: if you think you need that many, you’re probably doing something the wrong way.\nWriting files, then, is fairly easy. What about reading files? I’m glad you asked.\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n'First Post!'\n\n\nYou can probably tell mostly how that worked just by looking at. We used the open() function again, but this time with a “r” for our mode. This means “read”. Also, this time we used read() instead of write(). The read() function reads in an entire file and saves it a string variable. Finally, we call close() again to close the file and tidy up after ourselves.\nNote that if the file is, say, 500 megabytes long, the string variable is going to be very, very large - roughly half a gigabyte. Python can handle this, but it may not be terribly convenient. If the file is more than 100-200 gigabytes, the CLASSE servers are probably not going to be able to handle. I say “probably” because there are a lot of factors at play.\nJust writing one line to a file is probably not very useful. Let’s try writing two lines:\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First line written.\")\nmy_file_object.write(\"This is my second line.\")\nmy_file_object.close()\n\nWhen we run that, it will open /tmp/first_file.txt for writing and it will delete anything already in it (that’s what the “w” means, remember?). Then it will write “First line written.” and “This is my second line.”.\nLet’s read the file again and prove to ourselves that it worked…\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n'First line written.This is my second line.'\n\n\nOh no! The two lines ran together!\nAnd that is one of the first differences we’ll see between write() and print(). Print() always adds a newline character after it prints out anything. Remember when I said there would usually be a byte at the end of a line, represented by the number 10? This character is called “newline” and it, as the name implies, marks where a new line starts.\nIn all likelihood, when we do two write() statements like we did, we want to put a newline character in the file to make it into two lines. Fortunately, there are several ways to do that. Here are two of them.\nThe first way is simple and direct - call write() three times instead of two and put a newline in there “by hand”, as it were:\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First line written.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"This is my second line.\")\nmy_file_object.close()\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n\n\n'First line written.\\nThis is my second line.'\n\n\nThe output looks a little strange. We put an extra write() function call, but we gave it an odd looking argument - . That is a backslash (usually between the Enter and the backspace keys on a US keyboard) immediately followed by a lowercase “n”. The combination together means “newline character”. This much is fairly straightforward.\nNext we read the contents of the file. This is just like before.\nFinally, and this is where things take an unexpected turn, we evaluate the_contents and let Jupyter print that out for us. And when Jupyter does that, we see the “” there. It seems like Python didn’t convert those two characters to a newline, just sticking them in there as-is, and still left us with one long line. But is that true? Has Python foresaken us?\nRun the code in the next cell:\n\nprint(the_contents)\n\nFirst line written.\nThis is my second line.\n\n\nSalvation! print() did the right thing. This is a key difference between just typing a variable or an expression at the end of a cell and letting Python evaluate it versus putting a print() in there and having absolute control over what gets sent to the notebook and on to the screen.\nThis also illustrates something else important and useful: all of the code cells in this notebook are being run by the same Python “interpreter”. This means if we set a variable to a value in one cell, we will see the same value stored in that variable in other cells. That’s how we were able to print what was stored in the_contents in the cell above even though we had set its value to the file contents two cells above that.\nIf a file only has a line or two, it’s not a big deal dealing with that with string functions. If a file has millions of lines, then it becomes a bit of a hassle. We need a way to read a file one line at a time. Fortunately, there’s readline():\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nline_one = input_file.readline()\nline_two = input_file.readline()\ninput_file.close()\n\nprint(line_one)\nprint(line_two)\n\nFirst line written.\n\nThis is my second line.\n\n\nThis does almost what we expect: it reads both lines from the file, one at a time, and prints them out. The only snag is that blank space between the lines. What has happened? It turns out readline() reads the entire line, even the newline character at the end. We can see this if we evaluate the string instead of just printing it:\n\nline_one\n\n'First line written.\\n'\n\n\nThere’s that \\n again! What about the second line?\n\nline_two\n\n'This is my second line.'\n\n\nWhen readline() reads a line, it includes the newline character at the end unless it reaches the end of the file and the file didn’t end with a newline.\nIt’s rare that we would want to read a bunch of lines in a file with the newlines included. That’s just not something we do very often, and practically never in scientific software. We’ll almost always want to trim off the newline character. And for that, we have the rstrip() function. It takes a string, strips off any newlines on the right side of it, and returns that cleaned-up string. rstrip() does that for the right side of the string, lstrip() cleans up the left side (the beginning of the string) and strip() goes crazy and does both ends at the same time.\nLet’s try it:\n\nclean_first_line = line_one.rstrip('\\n')\nclean_second_line = line_two.rstrip('\\n')\n\nprint(clean_first_line)\nprint(clean_second_line)\n\nFirst line written.\nThis is my second line.\n\n\nWhat’s going on here? A couple of things. The first thing to note is that rstrip() and its close companions lstrip() and strip() take one argument, which is the character to be stripped. Practically always we’ll want to get rid of the trailing newline character.\nThe other interesting things is how we called the rstrip() function in the first place. We gave the name of the string variable, a period, and the name of the function we were calling. This is just like how we called the close() function on a file object. And in fact, strings are another kind of object in Python. We’ll see a lot more on this later.\nHistorical note: The original programming language that had objects was named “Smalltalk”. In Smalltalk, the functions that were inside of objects were called “methods”. You’ll still hear people call them that. Later, the “C++” language came along and it called methods “member functions”. When programmers talk about the functions that are contained in objects, we’ll use either term interchangably, sometimes even switching in the middle of a sentence. We now return to your Python tutorial, already in progress…\nWe read both lines in the file we created. We were able to call readline() twice and know that we had all of our lines in the file because (1) we created the file ourselves and (2) we therefore knew it had precisely two lines. It wasn’t even too bad having to type those readline() and rstrip() lines twice. But what if we had a lot more lines? We would certainly want to use a loop.\nFor example, what do we do with a five-line file?\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nmy_file_object.write(\"Line 1.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 2.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 3.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 4.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 5.\")\nmy_file_object.write('\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor i in range(5):\n    input_line = input_file.readline()\n    print(input_line.rstrip('\\n'))\n\ninput_file.close()\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nNo problem - we just use a for loop and do the readline() inside of it. It repeats the five times we asked for. In this case, after we read each line we cleaned it up a little and printed it.\nBut what if we can’t know the number of lines ahead of time? One approach is to have whatever program that creates the file write the number of lines that will be in it first. I won’t say this is a common approach in scientific software, but it isn’t exactly rare either.\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nmy_file_object.write(\"5\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 1.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 2.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 3.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 4.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 5.\")\nmy_file_object.write('\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nfirst_line = input_file.readline()\nhow_many_lines = int(first_line.rstrip('\\n'))\n\nfor i in range(how_many_lines):\n    input_line = input_file.readline()\n    print(input_line.rstrip('\\n'))\n\ninput_file.close()\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe overall scheme for this is probably obvious by now. In the first half, when we’re writing the file, we write a “5” on its own line, and then write five more lines. In the second part, we 1. Read the first line. 2. rstrip() to get rid of the trailing newline 3. Use the results of that as the argument to int(), converting that string (“5”) to an actual integer (5). 4. and finally go through a for loop that many times just like before\nMost of the time we won’t have the luxury of knowing how many lines are in a file, though. We need a way to read all of the lines, line by line, without limit. For that, we can loop through the file and quit when Python returns an empty string with not even a newline character.\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nline = input_file.readline()\nwhile line != '':\n    print(line.rstrip('\\n'))\n    line = input_file.readline()\ninput_file.close()\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe while loop behaved just like we expected - strat by reading a line, and then every time the line isn’t empty, print it out and read another line. When you finally hit a line that is completely empty, exit the while loop and close the file.\nLooping through a file all the way to the end is such a common thing to do, Python has a shortcut for doing it. Remember when we talked about a for loop iterating over an ordered set? A file can be thought of as an ordered set of strings. They’re not in alphabetical order, but rather they are ordered by line number. That means we can:\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nfor line in input_file:\n    print(line.rstrip('\\n'))\n\ninput_file.close()\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nAs you can imagine, reading isn’t the only file operation you can do with a loop. You can also write to a file that way. For instance,\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nfor i in range(7):\n    output_string = str(i)\n    my_file_object.write(output_string + '\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor line in input_file:\n    print(line.rstrip('\\n'))\ninput_file.close()\n\n0\n1\n2\n3\n4\n5\n6\n\n\nFinally, we don’t have to erase the contents of a file every time we write to it. It’s perfectly normal to append to an existing file, and for that the “a” mode can be used with open().\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"a\")\nfor i in range(7,10):\n    output_string = str(i)\n    my_file_object.write(output_string + '\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor line in input_file:\n    print(line.rstrip('\\n'))\ninput_file.close()\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWhen you use the append mode, the write() calls will either add to the existing file or, if it doesn’t already exist, it will be created and then written to as though we used the “w” mode.\nSo far in this lesson we’ve acted like everything just works perfectly every time. In reality, it’s not that neat. Filenames get typed in wrong, didks get full, and lines that are supposed to be numbers might contain text instead. Any of these problems is enough to bring our Python code to a grinding halt. Our next lesson is all about how to handle these problems and many, many more like them. We’re going to learn about Exceptions!"
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "",
    "text": "Niklaus Wirth was one of the founding giants of Computer Science. He wrote an introductory textbook whose title neatly summed up the act and art of programming: Algorithms + Data Structures = Programs. Data Structures are how information is stored in a computer, and algorithms are the instructions the computer applies to transform that data.\nTo run the code in a cell, first click in the cell to select it. Then you can either: 1. Go to the “Run” menu and choose “Run Selected Cells”, or 1. Just press Shift + Enter.\nLet’s do this now: click just below where it says “print (403.616”), then Go to the “Run” menu and choose “Run Selected Cells”.\nprint (403.616)\n\n403.616\nWhen it ran, it printed “403.616” on a line by itself. That was the output from the print.\nClick in the next cell (where it says “103.5”) to select that cell. Then hold down the Shift key while you press Enter.\n103.5\nIn Python, and in Jupyter notebooks, if the last (or only!) line evaluates to some value then it will be printed out. That’s how “103.5” got printed - a literal number evaluates to that number when it’s run. A “literal number” means you look at it in your code and you literally see a number.\nTake a look at a string literal. Run each of the next two cells…\nprint (\"the quick brown fox\")\nprint ('jumped over the lazy dogs')\nAt this point, we can use Python and Jupyter Lab as a scientific calculator. We have some literals of different types (int, real, and string, so far) and we can print them out with the print() function. If we don’t explicitly print anything at the end of a cell, Python will show us the last value that was computed."
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#operators",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#operators",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Operators",
    "text": "Operators\nLike any programming language, Python lets you “do math” and lots of other things. Let’s take a look at some of the basic “operators”. In all of the code-containing cells through this course, try to predict what will happen first, and then run the code.\n\n2+2\n\n\n2*8\n\n\n6-4\n\n2\n\n\n\n7*6\n\n\n16/3\n\nBesides the “classic” operators, there are some handy extras:\n\n16//3\n\nWhat happened there? The // operator does integer division - it returns the whole number part of the answer, just like when we learned division in elementary school.\n\n16%3\n\nThe % operator returns the remainder. This is also called “modulo”, and the above would be pronounced “sixteen mod 3”.\n\n2**8\n\n256\n\n\n\n4**2.718281828459045\n\nThe ** operator does exponentiation. The arguments can be integers or they can be real numbers. Naturally, operators can be combined into arbitrarily long expressions.\n\n3*4*5\n\n\n6+4*5\n\nNotice what happens when we use different operators. They are applied in the “My Dear Aunt Sally” order of precendence (multiplication, division, addition, subtraction).\nOrder of operations: * Exponentiation: ** * Multiplication, Division, Remainder: * / // % * Addition and Subtraction: + -\nWithin the same level, operators are applied left-to-right. 8-5+2 is evaluated as 3+2 and yields 5. The exception is exponentiation: 2 ** 3 ** 4 is treated as 2 ** 81 and yeilds an annoyingly large number\n\nprint (2**3**4)\n\n\nprint (2**81)"
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#variables",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#variables",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Variables",
    "text": "Variables\nUnless we just use Jupyter as a big, expensive scientific calculator, we need a way to store data. Variables were invented for just that purpose, and virtually every language has them. Think of them as a place to store data of some kind, and that place has a name. They behave in Python just like you’d expect.\n\nanswer = 42\nprint(answer)\n\n42\n\n\nWe just created a variable named answer and gave it the value 42. Variables are long-lived - later we’ll talk about just how long when we start writing our own functions, but until then our variables last as long as Python (or in our case, Jupyter) is running. Take a look - answer is still there.\n\nprint(answer)\n\nThe value stored in a variable can change. It can even change type:\n\nweight = 60\nweight = 70\nprint(weight)\nweight = \"not very much.\"\nprint(weight)\n\nWe can declare many variables, and we can “do things” with them just like we can when we type in numbers or strings.\n\nvolts = 120\namps = 4\nwatts = volts * amps\nwatts\n\nIn the last line, we just put watts because Jupyter automatically prints what the last line evaluates to.\nWe can use variables to change the order of operations. Let’s see the average price of two people’s meals:\n\ntotal = 22.41 + 19.45\naverage = total / 2\naverage\n\nThat’s the right answer. If we hadn’t done that, we would have gotten\n\n22.41 + 19.45 / 2\n\nwhich is utterly wrong. Beware of the order of operations… it is a frequent source of bugs in scientific programming.\n\nVariable Naming Rules\nFor the most part, you can pick whatever name makes sense for a variable, but there are some rules. When choosing a name: 1. No keywords (False won’t work.) 1. No spaces (sample thickness is invalid) 1. The first character must be one of * a-z, or A-Z, or _. (the underscore character) * As a result, no numbers (3rd_sample_holder is invalid) 1. After the first character, you can then have numbers (sample_holder_3 is perfectly valid) 1. No other symbols are allowed (exploded&destroyed_spectrometers is invalid, and probably suggests it’s time to review lab safety procedures).\nNote: Uppercase vs. Lowercase matters! Bevatron is not the same variable as bevatron"
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#types",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#types",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Types",
    "text": "Types\nWe’ve hinted that variables have a “type”, and that the type can change if it needs to. The way it works is that variables keep track of what type they are (integer, real number, or string) and what their “value” is. We can even interrogate a variable as see what type it is:\n\nreading=7.2\nprint(\"reading:\")\nprint(type(reading))\nreading=\"rainbow\" # changes type of reading to string\nprint(type(reading))\n\nreading:\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\n\nmy_number = 42\nmy_string = \"was that really the right answer?\"\n\nfirst_type = type(my_number)\nsecond_type = type(my_string)\nprint(first_type, second_type)\n\nThe type of a variable matters. Let’s create a variable with an integer in it and another with a string. Then let’s do some math:\n\nfirst_thing = 6\nsecond_thing = \"7\"\n\nprint (first_thing + second_thing)\n\nHow do we handle situations like that, where second_thing held a string representing a seven, but because it was a string variable it couldn’t be used as an integer? Python provides a few functions to convert values from one type to another. The str() function takes a variable and converts it to a string. The float() and int() functions convert their arguments to floating-point and to integer numbers, respectively.\n\nprint(first_thing + int(second_thing))\nprint(first_thing + float(second_thing))\n\n13\n13.0\n\n\nBeing able to convert values from one type to another is often called type coercion. These conversions are extremely important for situation where you need to get input from a user, even more so if you need to do it repetitively."
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#continuation-character",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#continuation-character",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Continuation Character",
    "text": "Continuation Character\nSometimes the expressions we need to evaluate can be very long. It would be nice if we could split up a long expression and spread it out over a few lines. As a small example, we’ll take a look at 4+2+3. Many programming languages will let us split an expression anywhere we want, such as:\n\n4+2\n+3\n\n…but that result isn’t right in Python. The last line, +3, was evaluated and printed as the result of running that cell. In Python,it turns out, if we need to continue an expression on the next line we must end the current line with a backslash \\ and press enter. It has to be a backslash, by the way, and cannot be the forward slash like we use for division.\n\n4+2\\\n+3\n\n9\n\n\nTime for an exercise! Try to predict what will be printed when you run the next cell. Then, run the next cell and see how you did. If you miss one, make sure you figure out what happened before you go. I know, we’re professionals, I shouldn’t have to say that…\n\nprint(1 + 3 + 5 * 4 / 2)\nprint(7 % 2 * 10)\nbig_num = 1 + 2 + 3 + 4 \\\n     + 5 + 6\nprint(big_num)\n\nNow write an expression to average three numbers (12, 14, and 66), divide the result by three, and square it. You can use the code cell right below here:"
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#the-string-type",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#the-string-type",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "The String Type",
    "text": "The String Type\nAt the beginning of this notebook, we casually mentioned “strings” without saying what they are. They’re just “sequences of characters”. And these can be any kind of characters - the English alphabet, the Hungarian alphabet, hiragana… it doesn’t matter.\n\nprint(\"I'll see you mañana, assuming I don't get irradiated to death.\")\n\nSome, probably most, languages contain strings inside “double quotes”, \", which is shift+apostrophe on US English keyboards. Other languages (SQL and Pascal are the only two I can think of) use single quotes: '. Python lets you use either one. You do have to be consistent in each string, but it can vary from one string to the next:\n\nprint(\"double quotes work\")\nprint('single quotes also work')\nprint('but do not try to mix the two in one string!\"\n\nBecause we can use either type of quotation mark, we can exploit that to let us put quotation marks into strings:\n\nprint(\"Don't put explosive mixtures in the spectrometer, please.\")\nprint('Of course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.')\n\nDon't put explosive mixtures in the spectrometer, please.\nOf course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.\n\n\nThat lets us embed whichever kind of quotation mark we need into a string.\nBut what if we need to embed both kinds of quotes into one string? We’re in luck: we can use the backslash character again to “quote” our quotation mark. In fact, we can quote any character with it if we need to.\n\nprint(\"We told him \\\"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\\\", but we're pretty sure he ignored us.\")\n\nWe told him \"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\", but we're pretty sure he ignored us.\n\n\nThat sentence contains three things, inside the string itself: 1. Double Quotes to surround a direct quotation 2. A single quote, also called an apostrophe depending on how it’s used, to make a contraction, and 3. A totally awesome/terrifying molecule you have to google to believe.\nOK, I’ll save you the trouble. Prepare to lose most of a day’s productivity. You’re welcome.\n(Derek has written gobs of articles on fun substances. Here are some more. )\nThere is one last kind of string literal. Sometimes you need a string that is several lines long. The “triple quote” is a way to do it. You have to use three double-quotes in a row:\n\ngigantic = \"\"\"This is the first line,\nThis is the second,\nand this is the third and final line of my string.\"\"\"\nprint(gigantic)\n\nTriple quotes are also an easier way to embed mixed kinds of quotation marks into strings:\n\nmovie_opinion = \"\"\"I know people who say \"The Avengers\" isn’t a good movie, but I don’t agree.\"\"\"\nprint(movie_opinion)\n\nI know people who say \"The Avengers\" isn’t a good movie, but I don’t agree."
  },
  {
    "objectID": "theme1/PE100/PE100-02TypesVarsAndOperators.html#coming-up-next",
    "href": "theme1/PE100/PE100-02TypesVarsAndOperators.html#coming-up-next",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Coming Up Next",
    "text": "Coming Up Next\nWe just looked at enough of Python and Jupyter notebooks to use it as a basic calculator, but so far we can’t do any real, general-purpose programming with it. The “flow of control” sob far as been a straight line from top to bottom and we can’t change what we’re doing in response to different inputs. That’s about to change. In the next section we’ll look at the if statement and how to use it."
  },
  {
    "objectID": "theme1/PE102/PE102-02NumPy.html",
    "href": "theme1/PE102/PE102-02NumPy.html",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "In the previous section, modules for Python was introduced. In this section, we’ll take a much more detailed look at one of the most useful to scientists: NumPy. This module contains numerous routinues and support frameworks for numerical computing. The routinues in it are very carefully tested for correctness and are crafted for speed. Any time you can use something from this package, it’s a good idea to.\nPython is built for versatility and ease of programming. Unfortunately, it is not built for speed. Over the years Python has gotten faster and faster but there is still a speed penalty compared to classic compiled languages like C, C++, or Fortran.\nEnter NumPy: a package of mathematical routines written in C and Fortran and made to work with Python via a “glue” or “shim” layer. This interface is invisible to the programmer. NumPy looks and behaves just like any other Python package. Under the surface, though, lies a very fast and efficient library of algorithms."
  },
  {
    "objectID": "theme1/PE102/PE102-02NumPy.html#a-first-glimpse",
    "href": "theme1/PE102/PE102-02NumPy.html#a-first-glimpse",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "A first glimpse",
    "text": "A first glimpse\nLet’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\n\nimport numpy as np\n\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\na\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\nz = np.zeros(3)\nz\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\nm = np.ones((3,3))\nm\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\nbig_m = np.ones((3,3,3,3))\nbig_m\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\nprint(\"z:\")\nprint(np.shape(z))\nprint()\nprint(\"m:\")\nprint(np.shape(m))\nprint()\nprint(\"big_m\")\nprint(np.shape(big_m))\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)"
  },
  {
    "objectID": "theme1/PE102/PE102-02NumPy.html#lets-do-some-actual-math-shall-we",
    "href": "theme1/PE102/PE102-02NumPy.html#lets-do-some-actual-math-shall-we",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "Let’s do some actual math, shall we?",
    "text": "Let’s do some actual math, shall we?\nThe trivial example: add a scalar (“a single number”) to every element of the matrix:\n\nprint(z)\nz_plus_three = z + 3\nprint(z_plus_three)\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\nprint (a % 2)\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\nprint(a)\nprint()\nprint(a &gt; 5)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]"
  },
  {
    "objectID": "theme1/PE102/PE102-02NumPy.html#linear-algebra-anyone",
    "href": "theme1/PE102/PE102-02NumPy.html#linear-algebra-anyone",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "Linear algebra, anyone?",
    "text": "Linear algebra, anyone?\nLet’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\n\nimport numpy.linalg as nl\n\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\nk = np.array([[1,1,1], [1,1,0], [1,0,0]])\nprint(k)\n\nkinv = nl.inv(k)\nkinv\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\nk @ kinv\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "theme3/DC100/distributed-computing.html",
    "href": "theme3/DC100/distributed-computing.html",
    "title": "Distributed Computing Concepts",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThese notes give you a brief overview of concepts of distributed computing. If you are at this chapter, it is assumed that you are familiar with writing scientific code for your analysis and are able to run it locally on your machine (laptop/desktop).\nFor purposes of this discussion, it is important to review some terminology."
  },
  {
    "objectID": "theme3/DC100/distributed-computing.html#serial-computing",
    "href": "theme3/DC100/distributed-computing.html#serial-computing",
    "title": "Distributed Computing Concepts",
    "section": "Serial Computing",
    "text": "Serial Computing\nSerial computing can be defined as executing instructions in a sequential order as laid out in your program. The order in which the instructions are executed is important and critical for correctness.\nIn serial computing your program runs on a single CPU/processor and speed at which your programs runs is directly proportional to the CPU’s processing power which is usually measured in Clock cycles per second (Hz). A 1 Hz CPU does 1 clock cycles per second. You can consider for discussion purposes\n\n1 instruction takes 1 cycle to to get executed (not always true)\nThe clock speed defines how fast your CPU can execute instructions.\nSo a 1Gz CPU can execute 1 billion instructions per second!\n\n\n\n\nSerial Computing"
  },
  {
    "objectID": "theme3/DC100/distributed-computing.html#parallel-computing",
    "href": "theme3/DC100/distributed-computing.html#parallel-computing",
    "title": "Distributed Computing Concepts",
    "section": "Parallel Computing",
    "text": "Parallel Computing\nIn parallel computing you can leverage multiple CPU’s or cores to execute your program. It involves breaking down a problem into smaller, independent sub-tasks that can be processed simultaneously by multiple processors or cores. Remember modern day processors/CPU’s are now comprised of multiple cores which can act independently, allowing use to execute instructions in parallel.\n\n\n\nParallel Computing\n\n\nParallel computing can often be achieved by either leveraging\n\nThreads: Threads are light-weight sub-processes, that have access to your program global memory. Threads allow you to leverage the multiple cores on a processor/CPU. Threading needs be done carefully, as data is shared across threads and a thread can easliy modify data that another thread maybe using resulting in crashes or deadlocks.\nMPI: Message Passing Interface(MPI) allows independent processes to access a shared global memory via a message interface. This allows processes to control\nwhat data is shared and which is private to the process. Using MPI you can either use cores on a single CPU, or also use cores across multiple CPU’s in a high performance computing cluster, where the CPU’s are connected via a high speed network such as infiniband.\n\nEven though you can acheive serious performance gains using parallel computing, it is generally considered hard and error prone. Various reasons contribute to it such as harder debugging, memory coherence issues etc. Also as humans, we are better at thinking sequentially."
  },
  {
    "objectID": "theme3/DC100/distributed-computing.html#distributed-computing",
    "href": "theme3/DC100/distributed-computing.html#distributed-computing",
    "title": "Distributed Computing Concepts",
    "section": "Distributed Computing",
    "text": "Distributed Computing\nDistributed computing involves leveraging multiple computers that are connected via a local area network or a wide area network to solve a problem that you cannot solve on a local computer. For example, if you do a google search the search request gets broken down and distributed amongst various nodes to get you the results back.\nParallel and Distributed computing are often changed interchangeably. However, there are subtle differences between the two. For example, in distributed computing there is no shared memory to use. If anything, Distributed Computing should be viewed as the superset (that could include Parallel Computing).\n\n\n\nParallel vs Distributed Computing\n\n\n\nHigh Performance Computing\nHigh Performance Computing involves using high performance computing clusters or supercomputing clusters (when sufficiently large) that consist of tightly coupled compute nodes (often thousands) interconnected with a high performance shared filesystem and high speed interconnects.\nA typical layout of a HPC cluster is shown below.\n\n\n\nHPC Cluster Layout\n\n\nImage Credit: https://docs.ycrc.yale.edu/clusters-at-yale\nUsers largely use HPC resources to run parallel codes such as MPI applications. However, HPC clusters are also used to run long-running serial jobs, or jobs that require only one node.\n\n\nHigh Throughput Computing\nHigh Throughput Computing (HTC) involves leveraging many computing resources over long periods of time to accomplish a computational task. These computing resources are often geographically distributed and inter-connected via the internet.\nHTC paradigm focused on executing a large number of tasks over a long period of time, often using distributed resources. Instead of maximizing performance for a single job (as in High Performance Computing, HPC), HTC aims to maximize the total number of jobs completed.\nHTC workloads are often comprised of hundreds or thousands of serial jobs that are mainly independent or loosely coupled, and don’t need tight coupling the way HPC jobs require.\nHTC focuses on increasing the overall throughput of the system by running many smaller size jobs (confined to a single core or a single) in parallel over a distributed computing infrastructure."
  },
  {
    "objectID": "theme3/DC101/scientific-workflow-management.html",
    "href": "theme3/DC101/scientific-workflow-management.html",
    "title": "Scientific Workflow Management",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThis module provides an overview on how a CHESS can use notion of workflows to automate their data processing tasks and in the process shortening the turn around time for the data processing that needs to be done on data collected at a beamline.\nThe content in this module is gathered from author’s various presentations over the years and their group’s work on workflows."
  },
  {
    "objectID": "theme3/DC101/scientific-workflow-management.html#what-are-scientific-workflows",
    "href": "theme3/DC101/scientific-workflow-management.html#what-are-scientific-workflows",
    "title": "Scientific Workflow Management",
    "section": "What are Scientific Workflows",
    "text": "What are Scientific Workflows\nScientific workflows allow users to easily express multi-step computational tasks, for example retrieve data from an instrument or a database, reformat the data, and run an analysis. A scientific workflow describes the dependencies between the tasks and in most cases the workflow is described as a directed acyclic graph (DAG), where the nodes are tasks and the edges denote the task dependencies.\nA defining property for a scientific workflow is that it manages data flow. The tasks in a scientific workflow can be everything from short serial tasks to very large parallel tasks (MPI for example) surrounded by a large number of small, serial tasks used for pre- and post-processing.\n\n\n\nExample Workflow\n\n\nIn the above example figure, the nodes/vertices in indicate the job that needs to be run, while the edges indicate the dependencies between the jobs.\nThe dependencies can be both control flow or data driven i.e. - a job can be run only after the parent has finished successfully OR - a job requires data sets to run that are generated as outputs by a parent node/job. - the rectangles in the figure indicate datasets that a node/job require and generate.\nUsing scientific workflows for your data processing in general provides you the following benefits\n\nReproducibility - Scientific workflows allow you to document and reproduce your analyses, ensuring their validity.\nAutomation - Workflows automate repetitive and time-consuming tasks, thereby reducing the workload of researchers.\nScalability - Workflows allow you to scale up your data processing to handle large data sets and complex analyses, enabling you to solve large research problems in your field. It helps you run your analysis in parallel over distributed resources.\nReusability - Once your data processing pipeline is modelled as a workflow, typically it is possible to reuse the workflow in different ways. For example, you could use the workflow as part of a bigger science analysis (that maybe faclity wide), or even share the workflow with another CHESS researcher.\n\n\nHigh Level Steps on Identifying a Workflow\nHere are some tips on how you can identify a workflow for your data processing\n\nStart on a whiteboard and think about the various steps that your data processing entails. Especially when you already have existing monolithic code for data processing, it is important to think about your processing at a high level in a logical manner.\nThink about the various steps that make sense logically, and would also make sense another CHESS researcher when discussing the problem.\nThink about the granularity in context of input data. If a particular step in your identified workflow, can benefit on working on smaller chunks of data; then consider breaking down that step in smaller parallel sub steps. For example if you are working on time series dataset, and you have a dataset collected for a month, you can break down processing in chunks of\n\nper day\nper hour\nper minute\nand so forth.\n\nWhat granularity makes sense is subjective. A good rule of thumb is that the step can easily run on a wide variety of compute nodes with reasonable memory. You don’t want to model a workflow that can only run on a few extremely powerful nodes with lots of memory. In some cases this maybe unavoidable. However, it is always good to think along these lines.\nOften, each step (type of step) in your workflow would map to a different executable.\n\n\n\nWorkflow Constructs\nThere are some core different types of workflow patterns that underpin most of the complex workflows. The jobs in the examples, below map to simple commonly available linux executables and is for illustration purposes only. For your actual workflows, you use your actual scientific code.\nProcess\n\n\n\nProcess Workflow\n\n\nIn the above example, workflow consists of a single job that runs the ls command and generates a listing of the files in the / directory.\nPipeline\n\n\n\nPipeline Workflow\n\n\nIn the above example, workflow consists of two jobs linked together in a pipeline. The first job runs the curl command to fetch the Pegasus home page and store it as an HTML file. The result is passed to the wc command, which counts the number of lines in the HTML file.\nSplit\n\n\n\nSplit Workflow\n\n\nIn the above example, workflow consists of a job that downloads the Pegasus home page using the curl command, then uses the split command to divide it into 4 pieces. The result is passed to the wc command to count the number of lines in each piece.\nMerge\n\n\n\nMerge Workflow\n\n\nIn the above example, workflow consists of 3 jobns that execute the ls command on several */bin directories and passes the results to the cat command, which merges the files into a single listing.\n\n\nWorkflow Challenges\nWhen running workflows independent of what workflow system you are using, there are some common challenges that you may encounter.\n\nHow to describe your complex workflows in a simple way? This is important in relation to sharing your workflows with your colleagues. Can the workflow that you just ran locally on your resource, can be run by another researcher on the\n\nsame resource\ndifferent resource / cluster\nwhat changes if any are required?\n\nHow do you get your workflows to access distributed, heterogeneous data and resources (heterogeneous interfaces). For example, you may want to distribute your analysis across different clusters.\nHow to deal with resources/software that change over time? The resource on which you are currently running may go away (no longer operational), software dependencies on which your code requires change.\nHow to have ease of use? Ability to debug and monitor large workflows.\n\n\n\nWorkflow Creation\nThere are a variety of workflow systems available that you can use in general. Some of them allow you to create workflows using\n\nA graphical interface\nProgrammatically using an API in a main stream programming language such as Python\n\nCHESS researchers have access to the following systems for running workflows\n\nCHESS Workflow Runner\nGalaxy (to be covered in DC102)\nPegasus Workflows"
  },
  {
    "objectID": "theme3/DC101/scientific-workflow-management.html#pegasus-workflows",
    "href": "theme3/DC101/scientific-workflow-management.html#pegasus-workflows",
    "title": "Scientific Workflow Management",
    "section": "Pegasus Workflows",
    "text": "Pegasus Workflows\nPegasus WMS allows users to model their computational pipelines as workflows, that can execute in a number of different environments including\n\ndesktops,\ncampus clusters,\ndistributed computing environments such as PATh and OSG OSPool,\nsupercomputing CI such as ACCESS and clouds.\n\nPegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware (HTCondor, SLURM, or Amazon EC2).\nThe clean separation of how the user defines a Pegasus workflow (in a resource agnostic way) called the Abstract workflow and the Executable workflow that actually runs on your target resource is illustrated below.\n\n\n\nPegasus Translation of User Defined Abstract Workflow to Executable Workflow\n\n\nAs a user when you define a Pegasus workflow, you\n\nidentify both input and data sets that a job requires by their logical identifiers called LFN.\nidentify the executable that needs to be invoked for a job by it’s logical identifier called transformation.\n\nThe Abstract workflow when given to Pegasus gets transformed to an Executable workflow that can execute on your target resource. As part of this transformation of the workflow Pegasus will figure out\n\nhow to get/stage the input data required for your workflow. add data stage-in jobs\nmaps your jobs to a compute resource\nhow to stage-out data that your workflow generates\n\nAdditionally, as part of this transformation Pegasus can do a lot of optimizations on your workflow such as\n\nadd data cleanup nodes, that clean up data from cluster when it is not required\ncluster short running jobs together\ndata reuse (delete jobs whose datasets already exist)\n\n\nGetting Started with Pegasus @ CHESS\nPegasus is already installed and configured to run on the CHESS cluster. In order to use Pegasus, you need to login to the node lnx201.classe.cornell.edu .\nX-CITE vahi$ ssh  lnx201.classe.cornell.edu\n(user@lnx201.classe.cornell.edu) Password: \nSource Pegasus binaries in your PATH\nIn order to run Pegasus you need to make sure that Pegasus executables are in your PATH. You can do this by sourcing the following setup file.\n[kvahi@lnx201 ~]$ source /nfs/chess/sw/pegasus/setup.sh\n(pegasus-env) [kvahi@lnx201 ~]$ pegasus-version \n5.1.0dev\n\n\nSetup a test diamond workflow and run it\nIn this example, we will try and run a test diamond workflow through Pegasus.\n\n\n\nDiamond Abstract Workflow\n\n\nTo setup a test workflow that runs on the CHESS SGE cluster, you can use the pegasus-init executable and answer the questions asked.\nYou should do the following selections when prompted\n\nSelect an execution environment [1]: 5\nWhat’s the execution environment’s queue: chess.q\nWhat’s your project’s name []:\nSelect an example workflow [1]: 1\n\n[kvahi@lnx201 pegasus]$ pegasus-init diamond\n###########################################################\n###########   Available Execution Environments   ##########\n###########################################################\n1) Local Machine Condor Pool\n2) Local SLURM Cluster\n3) Remote SLURM Cluster\n4) Local LSF Cluster\n5) Local SGE Cluster\n6) OLCF Summit from OLCF Headnode\n7) OLCF Summit from OLCF Hosted Kubernetes Pod\n\nSelect an execution environment [1]: 5\nWhat's the execution environment's queue: chess.q\nWhat's your project's name []: \nWhat's the location of the PEGASUS_HOME dir on the compute nodes [/nfs/chess/user/kvahi/software/pegasus/default]: \n###########################################################\n###########     Available Workflow Examples      ##########\n###########################################################\n1) pegasus-isi/diamond-workflow\n2) pegasus-isi/hierarichal-sample-wf\n3) pegasus-isi/merge-workflow\n4) pegasus-isi/pipeline-workflow\n5) pegasus-isi/split-workflow\n\nSelect an example workflow [1]: 1\nFetching workflow from https://github.com/pegasus-isi/diamond-workflow.git\nGenerating workflow based on pegasus-isi/diamond-workflow\nThis workflow will target queue \"chess.q\"\nThe PEGASUS_HOME location is \"/nfs/chess/user/kvahi/software/pegasus/default\"\nCreating workflow properties...\nCreating transformation catalog...\nCreating replica catalog...\nCreating diamond workflow dag...\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000001, transformation=preprocess)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000002, transformation=findrange)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000003, transformation=findrange)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000004, transformation=analyze)\nINFO:Pegasus.api.workflow:inferring diamond dependencies\nINFO:Pegasus.api.workflow:workflow diamond with 4 jobs generated and written to workflow.yml\nCreating properties file...\nCreating site catalog for SitesAvailable.SGE...\nThe example workflow gets generated in the diamond directory.\n[kvahi@lnx201 pegasus]$ cd diamond/\n(pegasus-env) [kvahi@lnx201 diamond]$ ls\ndiamond-workflow  generate.sh  pegasus.properties  plan.sh  replicas.yml  sites.yml  transformations.yml  workflow.yml\nIn this directory, you will see the following yaml files\n\nworkflow.yml - the abstract workflow generated for the diamond workflow that we will run.\nreplicas.yml - a yaml formatted file that lists the locations of the inputs for the workflow.\ntransformations.yml - a yaml formatted file that lists the locations of executables used by the workflows.\nsites.yml - a yaml formated file that describe the CHESS SGE cluster to Pegasus.\n\nThe above catalog files are described in the Pegasus documentation.\nTo plan the workflow, run the ./plan.sh script.\n(pegasus-env) [kvahi@lnx201 diamond]$ ./plan.sh\n./plan.sh \n2025.05.13 14:56:10.755 EDT:    \n2025.05.13 14:56:10.761 EDT:   ----------------------------------------------------------------------- \n2025.05.13 14:56:10.766 EDT:   File for submitting this DAG to HTCondor           : diamond-0.dag.condor.sub \n2025.05.13 14:56:10.771 EDT:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out \n2025.05.13 14:56:10.777 EDT:   Log of HTCondor library output                     : diamond-0.dag.lib.out \n2025.05.13 14:56:10.782 EDT:   Log of HTCondor library error messages             : diamond-0.dag.lib.err \n2025.05.13 14:56:10.787 EDT:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log \n2025.05.13 14:56:10.792 EDT:    \n2025.05.13 14:56:10.798 EDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: \n2025.05.13 14:56:10.808 EDT:   ----------------------------------------------------------------------- \n2025.05.13 14:56:23.174 EDT:   Database version: '5.1.0dev' (sqlite:////home/kvahi/.pegasus/workflow.db) \n2025.05.13 14:56:28.480 EDT:   Pegasus database was successfully created. \n2025.05.13 14:56:28.485 EDT:   Database version: '5.1.0dev' (sqlite:////nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001/diamond-0.replicas.db) \n2025.05.13 14:56:28.693 EDT:   Output replica catalog set to jdbc:sqlite:/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001/diamond-0.replicas.db \n2025.05.13 14:56:28.693 EDT:   \n\n\nI have concretized your abstract workflow. The workflow has been entered \ninto the workflow database with a state of \"planned\". The next step is \nto start or execute your workflow. The invocation required is\n\n\npegasus-run  /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n \n2025.05.13 14:56:29.864 EDT:   Time taken to execute is 28.297 seconds \nThe above command generated the executable workflow that you can run.\n\n\n\nDiamond Executabe Workflow\n\n\nTo do this copy the pegasus-run command invocation that you see in your terminal.\n(pegasus-env) [kvahi@lnx201 diamond]$ pegasus-run  /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\nSubmitting to condor diamond-0.dag.condor.sub\nSubmitting job(s).\n1 job(s) submitted to cluster 693.\n\nYour workflow has been started and is running in the base directory:\n\n/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n*** To monitor the workflow you can run ***\n\npegasus-status -l /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n*** To remove your workflow run ***\n\npegasus-remove /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\nTo check the status of the worklfow we can run the command pegasus-status with a -w 30 option to watch it every 30 seconds.\nAgain make sure you copy the pegasus-status invocation that you see in your terminal and add the -w 30 option after -l option.\n(pegasus-env) [kvahi@lnx201 diamond]$ pegasus-status -l -w 30 /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n   ID        SITE      STAT  IN_STATE  JOB                      \n  693        local      Run    03:06   diamond-0 (/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001)\n  696         sge      Done    00:04   ┗━untar_diamond_0_sge    \nSummary: 2 Condor jobs total (R:1)\n\nUNREADY READY  PRE  IN_Q  POST  DONE  FAIL %DONE  STATE  DAGNAME                  \n  13      0     0    0     1     2     0    12.5 Running diamond-0.dag           \nOnce the workflow completes you will see somthing similar to the following\npegasus-env) [kvahi@lnx201 diamond]$ pegasus-status -l -w 30 /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\nPress Ctrl+C to exit                                    (pid=2380412)                                 Tue May-13-2025 15:07:51\n\n   ID        SITE      STAT  IN_STATE  JOB                      \n  693        local      Run    08:58   diamond-0 (/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001)\nSummary: 1 Condor job total (R:1)\n\nUNREADY READY  PRE  IN_Q  POST  DONE  FAIL %DONE  STATE  DAGNAME                  \n   0      0     0    0     0     16    0   100.0 Success diamond-0.dag            \nSummary: 1 DAG total (Success:1)\nYou can also run additional pegasus commands after your worklfow finishes. The  should be replaced by the directory you passed to the pegasus-status command.\n\npegasus-analyser  - This allows you to debug your workflow if it fails. Will tell you what jobs failed and why.\npegasus-statistics  - This allows you to generate runtime statistics about your workflow run such how long it run, how much resources were used etc.\n\n\n\nSupport\nIf you would like to use Pegasus to run your pipeline you can contact via\n\nEmail - Support requests and bug reports can be emailed to pegasus-support@isi.edu.\nSlack - We encourage you to join the Slack Workspace as it is an\non-going, open forum for all Pegasus users to share ideas, experiences, and talk out issues with the Pegasus Development team. Please ask for an invite by trying to join pegasus-users.slack.com in the Slack app, or email pegasus-support@isi.edu and request an invite."
  },
  {
    "objectID": "theme3/DC101/scientific-workflow-management.html#chess-workflow-runner",
    "href": "theme3/DC101/scientific-workflow-management.html#chess-workflow-runner",
    "title": "Scientific Workflow Management",
    "section": "CHESS Workflow Runner",
    "text": "CHESS Workflow Runner\nTo be added. Coming soon in FALL 2025."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About X-CITE",
    "section": "",
    "text": "This site hosts material meant for CyberInfrastructure Training and Education for Synchrotron X-Ray Science (X-CITE). X-CITE is developed for the community of scientists and researchers using the CHESS synchrotron X-ray facility and similar light sources.\n\nAbout this site\nThe sources of this site are available at https://github.com/RENCI-NRIG/X-CITE.\nThe content in this website is available under Creative Commons Attribution-ShareAlike 4.0 International license. Code snippets may be used under CC0 1.0 Universal license.\nCopyrights for logos are owned by the respective organizations.\nThis site is generated using Quarto."
  },
  {
    "objectID": "newsite/theme4/XS101/python-exercises.html",
    "href": "newsite/theme4/XS101/python-exercises.html",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "Wherever you see &lt;your CLASSE username&gt; below, substitute your own CLASSE username.\n\n\nFollowing the Linux exercises for CHESS users: 1. Open a terminal on lnx201 1. Change directory to your CHESS user directory: cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\n\n\nIn your CHESS user directory, type following command to copy three files (file.txt, file.data, and file.json) to your current directory (.):\ncp /nfs/chess/user/x-cite/data/python/* .\n[Optional] Instead of copying the above files, type the following commands to create your own from scratch:\n# file.txt (file with a text)\necho 'Hello world!' &gt; file.txt\n\n# file.data (file with numeric array)\necho '[1,2,3,4,5]' &gt; file.data\n\n# file.json (file with json data)\necho '{\"int\": 1, \"array\": [1,2,3], \"dictionary\": {\"key\":\"value\"}}' &gt; file.json\nNow, view and check the contents of each file using cat, less, or more.\n\n\n\nThis exercise demonstrates some of the basic steps of any data analysis: reading data, processing or interacting with data, writing data. These steps can be iterated any number of times and combined in many ways.\nNote the conceptual similarity between these data analysis pipelines and the Linux pipes demonstrated in the Linux exercises (taking one command output and pass it to another, e.g. env | grep USER`).\nThe following code will walk you through the following steps: - how to open a file in python - how to read data from a file - become familiar with different data structures: string, array, dictionary, etc. - how to manipulate the data (e.g. perform some analysis) - how to mix different data structures together - how to write data to a file\nLet’s proceed with basic analysis using python: 1. Type python (in the same directory where the files you created above are located). This will open an interactive python session. 1. Type the commands below and observe the output:\n# open file\nopen('file.txt')\n\n# open file and assign it to a file descriptor\nfds = open('file.txt')\n\n# read data from our file descriptor\ndata = fds.read()\nprint(\"text data:\", data)\n\n# close the file descriptor\nfds.close()\n\n# load data from a data file\ndata = open('file.data', 'r').read()\nprint(\"structural data:\", data)\n\n# load json data from a json data file\nimport json\ndata = json.load(open('file.json', 'r'))\nprint(\"json data:\", data)\n\n# let's perform data analysis on our data and enhance it further\nif 'array' in data:\n    arr = data['array']\n    for item in arr:\n        print(\"array item\", item)\n    data['sum'] = sum(arr)\n\n# let's create our own data and write it back to new file\nwith open('analysis.json', 'w') as ostream:\n    ostream.write(json.dumps(data))\n\nType Ctrl-D to exit the interactive python session\n\n\n\n\nA virtual environment is a self-contained directory that contains a Python installation for a particular project, along with all its packages. It isolates project dependencies, ensuring that different projects don’t interfere with each other’s libraries and versions.\nWhy use it? - Avoid dependency conflicts between projects. - Safely test upgrades without affecting system Python. - Clean, reproducible development environments.\n\n\n\n\nInstall virtualenv (optional) Python 3.x comes with venv built-in command which we can use to create and setup virtual environment\n\npython3 -m venv myenv\n\nmyenv is the name of the folder where the environment will be created.\n\n\nActivate the Virtual Environment\n\n\nOn Linux/macOS:\nsource myenv/bin/activate\nOn Windows (cmd.exe):\nmyenv\\Scripts\\activate\nOn Windows (PowerShell):\n.\\myenv\\Scripts\\Activate.ps1\n\nOnce activated, your shell prompt will change to show the environment name, e.g., (myenv).\n\n\n\n\nWhile the virtual environment is activated, you can install packages normally using pip:\npip install requests\n\nExample: Installing a specific version:\npip install requests==2.28.1\n\nAll installed packages will now live inside the myenv directory.\n\n\n\n\nWhen you’re done working inside the virtual environment, simply run:\ndeactivate\nThis will return you to the original system Python environment.\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nCreate venv\npython3 -m venv myenv\n\n\nActivate venv\nsource myenv/bin/activate (Linux/macOS)\n\n\nInstall pkg\npip install &lt;package&gt;\n\n\nDeactivate\ndeactivate"
  },
  {
    "objectID": "newsite/theme4/XS101/python-exercises.html#exercise-1-log-in-and-navigate-to-chess-user-directory",
    "href": "newsite/theme4/XS101/python-exercises.html#exercise-1-log-in-and-navigate-to-chess-user-directory",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "Following the Linux exercises for CHESS users: 1. Open a terminal on lnx201 1. Change directory to your CHESS user directory: cd /nfs/chess/user/&lt;your CLASSE username&gt;"
  },
  {
    "objectID": "newsite/theme4/XS101/python-exercises.html#exercise-2-create-input-files",
    "href": "newsite/theme4/XS101/python-exercises.html#exercise-2-create-input-files",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "In your CHESS user directory, type following command to copy three files (file.txt, file.data, and file.json) to your current directory (.):\ncp /nfs/chess/user/x-cite/data/python/* .\n[Optional] Instead of copying the above files, type the following commands to create your own from scratch:\n# file.txt (file with a text)\necho 'Hello world!' &gt; file.txt\n\n# file.data (file with numeric array)\necho '[1,2,3,4,5]' &gt; file.data\n\n# file.json (file with json data)\necho '{\"int\": 1, \"array\": [1,2,3], \"dictionary\": {\"key\":\"value\"}}' &gt; file.json\nNow, view and check the contents of each file using cat, less, or more."
  },
  {
    "objectID": "newsite/theme4/XS101/python-exercises.html#exercise-3-interact-with-files",
    "href": "newsite/theme4/XS101/python-exercises.html#exercise-3-interact-with-files",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "This exercise demonstrates some of the basic steps of any data analysis: reading data, processing or interacting with data, writing data. These steps can be iterated any number of times and combined in many ways.\nNote the conceptual similarity between these data analysis pipelines and the Linux pipes demonstrated in the Linux exercises (taking one command output and pass it to another, e.g. env | grep USER`).\nThe following code will walk you through the following steps: - how to open a file in python - how to read data from a file - become familiar with different data structures: string, array, dictionary, etc. - how to manipulate the data (e.g. perform some analysis) - how to mix different data structures together - how to write data to a file\nLet’s proceed with basic analysis using python: 1. Type python (in the same directory where the files you created above are located). This will open an interactive python session. 1. Type the commands below and observe the output:\n# open file\nopen('file.txt')\n\n# open file and assign it to a file descriptor\nfds = open('file.txt')\n\n# read data from our file descriptor\ndata = fds.read()\nprint(\"text data:\", data)\n\n# close the file descriptor\nfds.close()\n\n# load data from a data file\ndata = open('file.data', 'r').read()\nprint(\"structural data:\", data)\n\n# load json data from a json data file\nimport json\ndata = json.load(open('file.json', 'r'))\nprint(\"json data:\", data)\n\n# let's perform data analysis on our data and enhance it further\nif 'array' in data:\n    arr = data['array']\n    for item in arr:\n        print(\"array item\", item)\n    data['sum'] = sum(arr)\n\n# let's create our own data and write it back to new file\nwith open('analysis.json', 'w') as ostream:\n    ostream.write(json.dumps(data))\n\nType Ctrl-D to exit the interactive python session"
  },
  {
    "objectID": "newsite/theme4/XS101/python-exercises.html#exercise-4-set-up-python-virtual-environment",
    "href": "newsite/theme4/XS101/python-exercises.html#exercise-4-set-up-python-virtual-environment",
    "title": "Python Exercises for CHESS Users",
    "section": "",
    "text": "A virtual environment is a self-contained directory that contains a Python installation for a particular project, along with all its packages. It isolates project dependencies, ensuring that different projects don’t interfere with each other’s libraries and versions.\nWhy use it? - Avoid dependency conflicts between projects. - Safely test upgrades without affecting system Python. - Clean, reproducible development environments.\n\n\n\n\nInstall virtualenv (optional) Python 3.x comes with venv built-in command which we can use to create and setup virtual environment\n\npython3 -m venv myenv\n\nmyenv is the name of the folder where the environment will be created.\n\n\nActivate the Virtual Environment\n\n\nOn Linux/macOS:\nsource myenv/bin/activate\nOn Windows (cmd.exe):\nmyenv\\Scripts\\activate\nOn Windows (PowerShell):\n.\\myenv\\Scripts\\Activate.ps1\n\nOnce activated, your shell prompt will change to show the environment name, e.g., (myenv).\n\n\n\n\nWhile the virtual environment is activated, you can install packages normally using pip:\npip install requests\n\nExample: Installing a specific version:\npip install requests==2.28.1\n\nAll installed packages will now live inside the myenv directory.\n\n\n\n\nWhen you’re done working inside the virtual environment, simply run:\ndeactivate\nThis will return you to the original system Python environment.\n\n\n\n\n\n\n\nAction\nCommand\n\n\n\n\nCreate venv\npython3 -m venv myenv\n\n\nActivate venv\nsource myenv/bin/activate (Linux/macOS)\n\n\nInstall pkg\npip install &lt;package&gt;\n\n\nDeactivate\ndeactivate"
  },
  {
    "objectID": "newsite/theme4/XS101/data-analysis.html",
    "href": "newsite/theme4/XS101/data-analysis.html",
    "title": "Hands-On Exercises",
    "section": "",
    "text": "Hands-On Exercises\nThe following hands-on exercises demonstrate various methods of analyzing and viewing x-ray detector images. All exercises (except the last one) require an activated CLASSE account and access to the CLASSE JupyterHub.\n\nBasic skills: Linux at CHESS, Python programming at CHESS\n\nSkills: Linux command line, navigating CHESS filesystms, Python\n\nData analysis (low complexity): Azimuthal integration of 2D diffraction patterns\n\nSkills: CHAP, Python, Linux command line, navigating CHESS filesystems, Jupyter notebooks, matplotlib\n\nData analysis (medium complexity): Tomographic reconstruction\n\nSkills: CHAP, Python, Linux command line, navigating CHESS filesystems, NoMachine, NeXpy, Galaxy\n\nViewing metadata (demo): Web tutorial\n\nSkills: navigating and searching for CHESS datasets\n\nScientific workflows (high complexity): Pegasus workflow management system\n\nSkills: setting up and running Pegasus workflows at CHESS"
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#chess-experimental-stations",
    "href": "newsite/theme4/XS100/data-collection.html#chess-experimental-stations",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "CHESS Experimental Stations",
    "text": "CHESS Experimental Stations\nData collection at all the CHESS experimental stations is supported by a complex controls, software, hardware, and cyberinfrastructure ecosystem. Having a basic understanding of this system and some relevant computing and software literacy will help prepare you for your beamtime. Historically, we have performed these trainings on the day of arrival, but preparation ahead of the beamtime will allow you to be more engaged with decisions on your experiment and focus on producing the highest quality experiment with your allocated beamtime.\nThe Cornell High Energy Synchrotron Source (CHESS) is currently home to 7 experimental stations spanning 3 sub-facilities. The diverse science, techniques, and missions of each beamline program leads to heterogeneous landscape of data collection experiences and computing literacy needed for each user. This training will cover the commonalities of CHESS data collection and resources to leverage, as well as where to expect to expect differences across beamlines and what questions to ask."
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#anatomy-of-an-experiment",
    "href": "newsite/theme4/XS100/data-collection.html#anatomy-of-an-experiment",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Anatomy of an experiment",
    "text": "Anatomy of an experiment\n\nMost experiments start well in advance of the awarded beamtime - and so does the computing and cyber-infrastructure needs.\nWhile CHESS provides state-of-the-art hardware, software, computing resources and trainings, users are responsible for the integrity of their experiment through thoughtful planning, experimental execution, and data handling and analysis. This includes maintaining best practices in experimental logs, metadata tracking, and recording of researcher decisions. Due to the nature of synchrotron experiments, the data integrity and intepretabilty - even within one research group - will be dependent on the practices adpoted by the research group."
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#beamtime-notes-and-experimental-logbook",
    "href": "newsite/theme4/XS100/data-collection.html#beamtime-notes-and-experimental-logbook",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Beamtime Notes and Experimental Logbook:",
    "text": "Beamtime Notes and Experimental Logbook:\nIt is always the responsibility of the experimenter to take detailed beamtime notes and a log of the data. Although work is ongoing to integrate metadata and capture requisite information in the data itself, automate workflows, visualizations, etc., it is imperative that experimenter notes are taken and ideally a copy is kept with the data on the CHESS system.\nJoint Experimenter Notes\nOften you will have a team of researchers taking data together - it is typically best practice to keep a collaborative log. In addition to your team - you should share these notes with your beamline scientist - they can often provide useful details you may miss if they observe some important or irregular behavior about the instrument itself that may or may not be obvious from the metadata streams.\nRemember - your beamline scientist is not responsible for memorizing the history of your data collected. They see so many experiments, if you ask 2 years later if they rememeber what you did - good luck.\nFor recording notes during the beamtime, we recommend using plain text or Markdown language (formatted text file) because it is easy to read in many systems, rather than a Microsoft Word Document which has a proprietary format. Images can be rendered in markdown formats.\nCo-Locate Your Beamtime Notes with Data\nSave your beamtime notes or a copy of your beamtime notes or (link to your google doc / equivalent) on the CHESS filesystem with your data (your beamline scientist will tell you where the best place is).\nLeverage MetaData Services\nWhenever possible, we encourage users to leverage the metadata services, beamline-specific software strategies (e.g. adding metadata to image frame headers), piping unique signals through software or hardware signals - saving these metadata with the data at the time of data collection. For unique aspects of your experiment, it is important to identify useful metadata to have associated with the x-ray data ahead of the beamtime. If signals are required to be monitored, this may need to be arranged for two weeks in advanced (see Bring Your Own Device (BYOD).\nCommenting Software\nComment any code produced at the beamtime. If this code was used to make decisions about the experiment, it should be saved and referred to in your experimental log."
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#station-computer-beamline-control-central",
    "href": "newsite/theme4/XS100/data-collection.html#station-computer-beamline-control-central",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Station Computer : Beamline Control Central",
    "text": "Station Computer : Beamline Control Central\nEvery experimental station has a station computer that acts as controls central. The station computer typically runs a number of processes and is responsible for orchestrating data collection, motor motions, synchronized triggers, metadata logging, and more.\n\nStation Computer ground rules:\n\nMost users directly interact with the station computer. Users associated with the beamtime will have permission to log directly into the station computer remotely through No Machine (LINK to CLASSE). Your beamline scientist will be training you in how to run your experiment - pay close attention and take ownership of your role during your beamtime. Your beamline scientist may only be training your group at a specific time(s) during the beamtime - make sure all users can be present during this training and/or take notes and be prepared to train your fellow users on the basic operations. When in doubt, always communicate with your staff scientist. Some processes staff scientists will insist that the user be trained by the scientist and not a fellow user.\nThe station computer is running many processes that enable your experiment to run. You will be asked to interact with specific processes during your beamtime. It is imperative you ONLY interact with the processes that your beamline scientist has given you permission to run. Some UI’s and data reduction processes will be running on other computers through terminals on the station computer - accidentally running these processes on the station computer itself may disrupt data collection, overwhelm the station computer, or even cause it to freeze or shut down.\nThe station computer has many special permissions, for instance it is able to write to the the CHESS DAQ (raw directory). When saving files such as beamtime notes, it is important to save these in the directories prescribed by your beamline scientist. (link to later section on CHESS file system and directories)\n\n\nThis is an example of a station computer screen shot with many processes. There are 4 desktops on the station computer, each with windows spanning 4 screens. This image is of the first desktop and shows a main controls terminal (SPEC), controls screens (MEDM screens), a data reduction GUI (HEXRD). There will typically be even more processes running than this.\nEvery beamline will have a unique version of this computer - some techniques even may be executed exclusively through a GUI.\n###1 Controls Hardware, Software, and Signal Monitoring\nThis section will discuss the hardware connections, motor configurations, and overall connectivity of signals being tracked in the beamline."
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#controls-software",
    "href": "newsite/theme4/XS100/data-collection.html#controls-software",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Controls Software",
    "text": "Controls Software\nThere are many controls languages and strategies across the lab. The two most common cases are SPEC and EPICS which will briefly be introduced here. Python-based controls are also very common.\nSPEC\nSPEC is a language, loosely based on C, used for instrument control and data acquisition at many synchrotrons.\nWhen using spec, you will interact with a SPEC terminal and run a combination of “standard” SPEC commands and/or a series of compiled programs for your technique\nImportantly, only ever use SPEC or edit Macros with the explicit permission of your staff scientist. This may vary from beamline to beamline SPEC commands continued.\nBelow is a video of a SPEC command window with built-in and custom macros.\n\n\n\nEPICS\nEPICS is a set of software tools and applications which provide a software infrastructure for use in building distributed control systems to operate devices\nMany of the devices at CHESS leverage EPICS drivers for operation EPICS PVs (process variables) are commonly used for signal monitoring and metadata/data logging.\nMany important metadata signals can also be tracked using “EPICS PVs.” While many of these PVs (process variables) are used throughout data collection, they can also be an important part of data monitoring. Your beamline scientist may have you observe the monitoring page depending on your experiments sensitivity to certain signals to monitor specific station signals (link to signals.chess.cornell.edu).\nAn Epics MEDM (Motif Editor and Display Manager) Screen for a Detector is shown in the annotated station view.\nPYMCA\nA common GUI used at the beamline is the PyMCA GUI. In addition to it’s original use for XRF, this GUI can be used to load in spec.log data and plot the counters at your beamline.\nPython, MATLAB, etc.\nYou may have specific other software: Python scripts (link to python tutorial), other UI’s for instrumentation, that you should receive training from your beamline scientist. See Python module if that is important."
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#networks-and-filesystems",
    "href": "newsite/theme4/XS100/data-collection.html#networks-and-filesystems",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Networks and Filesystems",
    "text": "Networks and Filesystems\n\nDuring data collection, raw data is written directly to the CHESS-DAQ filesystems. The CHESS-DAQ consists of approximately 2 petabytes of dedicated online storage arrays connected to the CHESS experimental stations through a high-speed 10Gb data collection network.\nTo protect the communication signals between the station computer, experimental station equipment, and other local systems, each experimental station has an isolated network with direct connections.\nDetectors often have direct fiber optic / high speed data lines to inline computing resources and/or the CHESS-DAQ.\n\nThe CHESS filesystem has different locations for storing raw data, reduced data, etc. These different locations have different backup schedules and total storage amounts. Typically best practice is as follows: 1. Raw Data that cannot be reproduced is located in RAW/DAQ 2. Reduced Data that can be reproduced from Raw Data/Other Protected Data is in REDUCED DATA 3. Metadata that is small and not reproducible should be saved in METADATA (backed up nightly) 4. Data that is being produced and does not need to be backed up and could be processed again should be done in SCRATCH. This is a good location for testing code before performing Data Reduction. 5. For Data NOT associated with a particular beamtime, USER is an appropriate place for these projects.\n\nProtected Data: Intellectual Property (IP) and Export Control\nSome data needs to be protected, e.g. data covered under Intellectual Property or Export Control agreements\nAll such data must be declared and all agreements signed before ANY data is on CHESS/Cornell computing systems (including preparatory material that falls under IP or Export Control categories).\nData Collection, Storage, and Analysis can be customized to comply with data agreements, including: - Modifying isolated networks - Mounting encrypted drives - Configuring encrypted computers - Modifying permissions on filesystem locations - Securing the experimental station with an entry password - Disconnecting streaming video to the experimental station"
  },
  {
    "objectID": "newsite/theme4/XS100/data-collection.html#data-handling-and-analysis",
    "href": "newsite/theme4/XS100/data-collection.html#data-handling-and-analysis",
    "title": "Data Collection, Preparing Input Parameters, SPEC and CLI",
    "section": "Data Handling and Analysis",
    "text": "Data Handling and Analysis\n\nIf you wish to move any data from the CHESS filesystem to another location, the preferred way of doing so is through Globus. Please see here (https://wiki.classe.cornell.edu/Computing/GlobusDataTransfer) for directions on ways to transfer data from the CHESS filesystem.\nYour beamline may be producing very large quantities of data. Due to it’s size, you may not be able to take your data home or transfer it home via globus. your data in raw may only stay in hot storage for a short amount of time (6 months). Your experimental station will have best practices for how to compress or reduce this data so that it is small enough to take home or live in a different part of our filesystem.\n\nThe data is still saved, but to transfer or perform analysis on the files you will need to arrange to have it rolled back into “hot storage” - AKA take out an IT ticket: https://wiki.classe.cornell.edu/Computing/ServiceRequestTips\n\n\nBring Your Own Device (BYOD)\nUsers may need to bring their own devices to be beamline - either physically in the lab or remotely connected to the CHESS networks\nExamples include: - Controls computer for equipment they have integrated for an experiment - Analysis computer for on-the-fly analysis\nAll devices must be approved at least two weeks in advance. It may not be possible to consider integration on a shorter time period.\nBecause the CHESS-DAQ filesystems are a critical resource for data collection, write access is only granted to registered devices on the CHESS-DAQ network. If you wish to bring your own device to write data to the CHESS-DAQ, please discuss your needs with your staff scientist at least one month before your beamtime. Before your device can be registered on the CHESS-DAQ, it must undergo a cybersecurity evaluation by CLASSE-IT.\nRead access to the CHESS-DAQ filesystem may be obtained by registering your device for the LNS Protected network using this request form.\n\n\nMetaData Handling\nIdeally, all the data necessary to fully reproduce your results are recorded and disseminated in a manner that others can interpret after your experiment. Ideally the provenance remains unbroken from experiment planning.\nMetadata and parallel data streams are generated at every stage of your experiment. CHESS is continuing to develop and implement services to help with this creation. From programs like Galaxy, to our Metadata service -\nThe metadata service (https://wiki.classe.cornell.edu/bin/viewauth/CHESS/Private/CHESSMetadataService) provides tools to record and automatically ingest machine-readable metadata in a systematic way. It includes variables that historically were not recorded via a second data stream (e.g. the material processing parameters).\n\n\n\nOn-the-fly Data Processing & Visualization\nNeed to be looking at your data as it is coming off for data fidelity. At some beamlines, you will need to interact manually with a GUI to render your images and make sure the data quality is what you are expecting. At some beamlines, the new NSDF (National Data Science Fabric) Dashboard has been deployed for some datatypes which allows interactive visualization of data as it is coming off the beamline. An example can be found here: https://services.nationalsciencedatafabric.org/chess/.\nMany beamlines have an initial “data reduction” procedure that reduces the size of the data through compression or on-the-fly analysis to have smaller file sizes that are more manageable to analyze at your home institution.\nSome of these processes are software procedures performed on the data after it is written to the raw directory, while other beamlines utilize inline data processing/compression prior to writing their files to the raw directory. Overtime we will build out station specific training on these, for now it is the responsibility of the user to work with their beamline scientist and read any requisite materials they provide.\n\n\nData Reduction and Analysis\n\nCompute Farm\nThe CLASSE Compute Farm is a central resource consisting of approximately 60 enterprise-class Linux nodes (with around 400 cores), with a front-end queueing system (Son of Grid Engine, or SGE) that distributes jobs across the Compute Farm nodes. SGE supports interactive, batch, parallel, and GPU jobs, and it ensures equitable access to the Compute Farm for all users.\nData on the CHESS-DAQ filesystems can be directly accessed using the Compute Farm, and instructions for job submission are available here.\nCHESS Analysis Pipeline (CHAP)\nThe CHESS Analysis Pipeline (CHAP) is an object-oriented framework for organizing data analysis code into reusable modules. The most basic pipeline consists of the following modules: - Reader: takes an input file or data source and converts it into a standard data structure - Processor: takes a data structure from a Reader, executes a data processing algorithm, and writes an output data structure - Writer: takes a data structure from a Processor and converts it to a specific file format\n\nAn example of a concrete CHAP implementation is shown below. Here, the Processor accepts inputs from multiple Readers that provide both raw data and metadata.\n\nCHAP pipelines can be executed from a Linux command line or from the Galaxy science gateway. A third method called CHAPBook is currently under development, which presents a notebook-like coding interface for non-expert users.\nTechnique/Beamline Specific Software\nCHESS has many common X-ray software packages available on the system. Speak with your beamline scientist for the preferred software package for your experiment."
  },
  {
    "objectID": "newsite/index.html",
    "href": "newsite/index.html",
    "title": "X-CITE training materials",
    "section": "",
    "text": "X-CITE (CyberInfrastructure Training and Education for Synchrotron X-Ray Science) develops training materials for the community of scientists and researchers using the CHESS synchrotron X-ray facility and similar light sources.\nOur training modules will get you up to speed in working with the CyberInfrastructure (“CI”) at CHESS - the High Performance Computing (HPC) and Networking resources. Understanding the CI and how to use it is practically essential for making the most of your beamline time and the analysis that follows after the fact.\nBelow you’ll find links to the training modules we have developed. These are grouped by general topic into themes; the first is named “Essential Elements” and is absolutely critical. Make certain you understand the material in that section before you move on to the others, but once you cross this hurdle you can move through the other collections in any order. Some of the Essentials topics may be old hat to you (Python programming, for instance) while others are CHESS-specific - Data Collection, for instance."
  },
  {
    "objectID": "newsite/index.html#essential-elements",
    "href": "newsite/index.html#essential-elements",
    "title": "X-CITE training materials",
    "section": "Essential Elements",
    "text": "Essential Elements\n\nXS 100: Data collection, preparing input parameters, SPEC and CLI\nSF 100: Intro to Linux, the command line, and programming in Python\nXS 101: Basic / on-the-fly data analysis, viewing detector images\nPE 100: Python Programming and Jupyter notebooks"
  },
  {
    "objectID": "newsite/index.html#programming-fundamentals",
    "href": "newsite/index.html#programming-fundamentals",
    "title": "X-CITE training materials",
    "section": "Programming Fundamentals",
    "text": "Programming Fundamentals\n\nPE 101: Using Python packages & libraries, Conda\nPE 102: Numerical data analysis with Python\nPE 103: Software version control, testing, and debugging"
  },
  {
    "objectID": "newsite/index.html#systems-fundamentals",
    "href": "newsite/index.html#systems-fundamentals",
    "title": "X-CITE training materials",
    "section": "Systems Fundamentals",
    "text": "Systems Fundamentals\n\nSF 101: Containers and virtualization\nSF 200: Parallel computing concepts\n\nSF 201: Batch systems and compute farms with CHESS example"
  },
  {
    "objectID": "newsite/index.html#distributed-computing-and-the-ci-ecosystem",
    "href": "newsite/index.html#distributed-computing-and-the-ci-ecosystem",
    "title": "X-CITE training materials",
    "section": "Distributed Computing and the CI Ecosystem",
    "text": "Distributed Computing and the CI Ecosystem\n\nDC 100: Distributed computing concepts\nDC 101: Scientific workflow management\nDC 102: Using science gateways with Open OnDemand example\nDC 200: Computing with CI ecosystem - ACCESS, PATh, Campus"
  },
  {
    "objectID": "newsite/index.html#x-ray-science-se-software",
    "href": "newsite/index.html#x-ray-science-se-software",
    "title": "X-CITE training materials",
    "section": "X-Ray Science S&E Software",
    "text": "X-Ray Science S&E Software\n\nXS 102: Large-scale data analysis: from images to science parameters to interpretation\nXS 200: Metadata for data fidelity and systematic checks"
  },
  {
    "objectID": "newsite/index.html#data-curation-and-fair",
    "href": "newsite/index.html#data-curation-and-fair",
    "title": "X-CITE training materials",
    "section": "Data Curation and FAIR",
    "text": "Data Curation and FAIR\n\nCF 100: Intro to domain metadata standards, formats and repositories\nCF 101: Best practices for developing DMP\nCF 102: Metadata annotation and DOI\nCF 200: Curating data, code, workflows, and publishing"
  },
  {
    "objectID": "newsite/theme5/CF102/metadata-annotation-and-doi.html",
    "href": "newsite/theme5/CF102/metadata-annotation-and-doi.html",
    "title": "Metadata annotation and DOI",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-metadata",
    "href": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-metadata",
    "title": "Metadata annotation and DOI",
    "section": "What is metadata?",
    "text": "What is metadata?\n\nMirriam-Webster dictionary defines data as: “factual information (such as measurements or statistics) used as a basis for reasoning, discussion, or calculation”, or: “information in digital form that can be transmitted or processed.”\nFor our purposes, let us use the latter definition: data is information in digital form that can be stored, transmitted, or processed. Our experiments may consume data, produce data, transmit data between instruments and computers, and we might want to analyze data to make sense of it. We store data somewhere, for eventual use and/or sharing.\n\nWhat is metadata then?\nMetadata is data that describes data, and what that description says depends on context.\nFor example, EXIF (which is an abbreviation of “Exchangeable image file format”) is a form of image metadata. When you take a picture with cellphone, you get an image file. In addition to “pure” compressed on uncompressed image data, this image file also contains some extra data describes the image: details about the camera, lens, aperture, shutter speed, date and time at the time when the picture was taken, location if available, such things.\nIn addition to EXIF, there is the metadata that the operating system itself maintains about an image file: its ownership, date and time when the file was created and modified (as far as the operating system is concerned), and so on.\nSo when we talk about metadata, it is important to be clear about the context in which we talk about metadata.\nFor CHESS experimenters, metadata would be information about observational or experimental data that provides a “fuller picture” about the observation or experiment."
  },
  {
    "objectID": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-annotation",
    "href": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-annotation",
    "title": "Metadata annotation and DOI",
    "section": "What is annotation?",
    "text": "What is annotation?"
  },
  {
    "objectID": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-doi",
    "href": "newsite/theme5/CF102/metadata-annotation-and-doi.html#what-is-doi",
    "title": "Metadata annotation and DOI",
    "section": "What is DOI?",
    "text": "What is DOI?\nA DOI (Digital Object Identifier) is a unique and stable string assigned articles, books, and other works. DOIs make it easier to find and retrieve works. DOIs are designed to be used by humans as well as machines. DOIs are commonly used to identify publications and data sets.\nA DOI is meant to help us to resolve its target. The location of an online document or data set may change over time, because domain names and links change. However, since a DOI remains stable over time, with a DOI people should be able to find the current location of the target. The publisher of the document or data set is responsible for keeping the record up-to-date.\nA DOI takes the form of a character string divided into two parts, a prefix and a suffix, separated by a slash, in prefix/suffix form.\nHere is an example of a DOI: 10.1177/0306312719863494. Here the prefix 10.1177 identifies the registrant of the DOI, and the suffix 0306312719863494 identifies the article.\n\n\nHow can I resolve a DOI?\nTo resolve the article referred by 10.1177/0306312719863494, you would use a resolver. The search box at doi.org is an interface to such a resolver.\nA DOI is also resolvable as a URL using a proxy server. You could prefix the DOI with https://doi.org/ to get the URL https://doi.org/10.1177/0306312719863494, which will redirect you to the actual article, currently available at https://journals.sagepub.com/doi/10.1177/0306312719863494:\nThe https://doi.org resolver is operated by the DOI Foundation, a non-profit that governs the DOI system. The DOI system is standardized by International Organization for Standardization (ISO).\n\n\nHow do I use DOI in citations?\nHere’s how you would cite the above-mentioned article:\n\nMayernik, M. S. (2019). Metadata accounts: Achieving data and evidence in scientific research. Social Studies of Science, 49(5), 732-757. https://doi.org/10.1177/0306312719863494\n\n\n\n\nHow do I get myself a DOI?\n\n\nIn order to get a DOI for your paper or data set, you will need to work with a registration agency, such as crossref or datacite."
  },
  {
    "objectID": "newsite/theme5/CF102/metadata-annotation-and-doi.html#references",
    "href": "newsite/theme5/CF102/metadata-annotation-and-doi.html#references",
    "title": "Metadata annotation and DOI",
    "section": "references",
    "text": "references\n\nCarpentries Incubator: Data and Metadata\n\nCarpentries Incubator Training Course Material: Fundamentals of Scientific Metadata\n\nMetadata accounts: Achieving data and evidence in scientific research. Matthew S Mayernik, National Center for Atmospheric Research.\nObservational Health Data Sciences and Informatics forums: How do we define “Metadata” and “Annotation?”\nWikipedia page on Digital object identifier\nDOI Foundation: What is a DOI?\nScribbr: What is a DOI? Finding and Using Digital Object Identifiers\nLong Term Ecological Research Network: Enriching Ecological Data Using Annotated Metadata\nCool DOIs"
  },
  {
    "objectID": "newsite/theme5/CF100/domain-metadata-standards.html",
    "href": "newsite/theme5/CF100/domain-metadata-standards.html",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Wouldn’t it be great if you could quickly find data generated from other people’s experiments, combine it with your own data, and produce new, high-value output? Wouldn’t it be even better if other people could discover your data, utilize it, and cite your data in their papers? That is precisely the goal of FAIR Data - making results Findable, Accessible, Interoperable, and Reusable.\nIn this section of the X-CITE training materials, we’ll look at the motivations behind FAIR, take a more detailed look at each of the four components, and consider some of the implications of these large-scale sharing principles.\n\n\nRunning experiments and collecting data is expensive [citation needed]. Funding agencies don’t want to pay for running the same experimental design over and over beyond, perhaps, a replication study. Publishers are looking for new ways to add value to their curation of articles. And most importantly (to us, at least) there are the researchers who are producing these voluminous datasets.\nIndividual researchers have their own needs for FAIR. Speed is a big one - having Findable and Accessible data already available can lead to faster hypothosis formation, allow earlier grant proposals, and establish priority in publication. A second concern is publication. Historically it wasn’t easy to publish raw data because it’s expensive to keep and grants wouldn’t pay for it. There are repositories now where your experimental data can be stored and accessed. Some of them are very domain specific (such as Cornell’s FOXDEN, which we look at later) and others are designed more for “general purpose” use. Finally, adhering to the FAIR Data principles makes it easy (and expected!) for other researchers using your data to cite your work with unambiguous credit and with a permanent network location where the material can be found. FAIR is not restricted to data in the historical sense, but also encompasses source code and workflows.\nPublishers stand to benefit from FAIR, especially if they take full advantage of the ideas. Findable and Accessible (not to mention Interoperable and Reusable) data leads to it being used more and by a wider distribution of users. Researchers in one field probably don’t read journals in a different one. But what if that outlier of a researcher was able to discover data relevant to their work? Now the publisher may have a broader market to sell into. Discoverable data doesn’t come from outer space, with apologies to the astronomers, but is a product of research and that research is accompanied by publications. Publishers will barely care if the underlying data is cited, but they care deeply that the associated journal articles are. FAIR principles, through the “side effect” of increasing publication citations, boost the Impact Factor for journals and that benefits both the authors and the publisher.\n\n\n\nFindable, Accessible, Interoperable, and Reusable.\nIn this section we will look at the aspects that make up each of Findability, Accessibility, Interoperability, and Reusability. As you read this, keep in mind that the principles are descriptive rather than prescriptive. There is no attempt to describe how to implement an archive, for instance, but rather to discuss what the requirement is and leave the implementation details up to the implementers.\n\n\nThe first requirement for a data set to be useful is for it to be findable. If no one can find it, it may as well not exist. There has to be some kind of automatic way to describe what you are looking for and get a list of possible results in return. If this sounds like a Search Engine, then you’re on the right track. Search engines are pretty good at what they’re designed for, which is hunting through immense collections of text and determining which documents are statistically likely to be useful. The problem is the “collections of text” part: general-purpose search engines have little or no conception of structured data. Imagine trying to search for spreadsheets that contain bond angle data and find the ones that have particular angles, in a particular order, under specific circumstances. Clearly, we’re going to need something more versatile than just a text search engine (sorry, Google).\nThere are four elements that make up the “Findable” principle:\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identfier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nLooking at these elements, we see that there must be rich metadata available to describe a data set well enough to facilitate people finding the (meta)data, and both the data itself as well as the metadata need to be published in a way that facilitates indexing both. The data and metadata must each have a Globally Unique Identifier - think of it as being like a URL. If you have already seen “doi:” used to refer to journal articles, then it will come as no surprise to see this extended to (meta)data.\n\n\n\nMoving on to “Accessible”, let’s take a look at what goes into that:\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\nA2. metadata are accessible, even when the data are no longer available\n\nThere are two basic aims here. The first is to use a well-known, fully-documented, unencumbered protocol that (optionally) allow for authentication and authorization tasks (“logging in”) to be completed. The last aim is unexpected at first blush but makes sense when you think about it: metadata live forever, even after the data they describe has finally been deleted. Permanently keeping the metadata as a record of what work has gone before makes it possible to get an idea of past research directions. Interestingly, “mining” collections of metadata can produce new knowledge of its own.\n\n\n\nInteroperability simply means that the data are sufficiently described by their metadata so that researchers can create their own tools, or use the tools of others, to work with the data without having to use the same software as the original researcher. Very importantly, if at all possible the data should be preserved in a way that doesn’t require the use of proprietary software to read it. This is not simply because of cost, though that can be a major barrier, but is also a matter of historical preservation - it may be impossible to locate or run old enough software. Imagine having a really useful data set except it’s only readable by a program that only runs on a 35 year old version of MacOS. Yes, that really happens. If we design our (meta)data for interoperability then we can take that 35+ year old dataset and work with it using modern tools.\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nThe first of these elements is a challenge, honestly. The goal is to have a very rich way of representing metadata that preserves the semantics of the metadata. It’s not enough to simply say “the third column is conductance”. What is needed is a way to say “the third column is conductance, it’s an electronic measure, it’s related to resistance, and here is how”. It’s a lot of work, but luckily for us most of it has been done and we can reuse it - Resource Definition Framework (RDF) is made for this job. We’ll talk more about this in a later section.\n\n\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards\nThe crux of this principle is ensuring high-quality curation. The metadata should be thoroughly descriptive and will indicate, among other things, the rights the authors has granted to future users and a “chain of custody” that describes how the (meta)data came to be. Finally, the (meta)data should of course be of high quality to begin with, in line with the standards of the discipline. If you are dealing with human subjects, be aware that data privacy requirements (HIPPA, IRB) may restrict or prohibit the publication of data and even metadata. Enough metadata can sometimes be used to reconstruct the data itself. As an example, sometimes age, zip code, and gender is enough to identify someone - some zip codes have very people in them. 28520 has about 250 people.\n\n\n\n\nAs mentioned above, metadata is simply information that describes your data. Hidden behind that word “simply” is the slight complication that metadata can be anything from really simple to really complicated. Where it falls on that continuum is situational. Small amounts of simple data will probably have simple metadata.\n\n\nWithout a standard means of representation and agreed-upon meanings, the metadata we assemble might be more of a hinderance than a help. Selecting our data’s properties to record is domain-specific in many cases. A very general set of attributes is available at schema.org. Other ontologies exist, of course, and selection among them tends to narrow as you go.\n\n\nRDF is “Resource Description Format” and is a broadly used concept even outside of FAIR. Fundamentally, RDF’s “intention” is to describe the world in terms of triples: subject, predicate, and object. From these building blocks we can construct graph structures (in the “discrete math” sense of the term). As they grow, they can represent linkages between related items, for instance, and that is when the real power of RDF can start to be exploited. With small collections of metadata there is little choice but to handle search terms, for instance. Once the collection expands, new kinds of queries are possible. For instance, different researchers might submit data to an archive. We know the possible range of metadata descriptors and we know what each one means. At this point, we can traverse the RDF graph. It’s easy to go from the starting place to enter the graph and then follow along, hop by hop, expanding the search possibilities by adding related terms.\n\n\n\nFor any data to be useable by a computer, it has to be represented in a way that it can be understood. Metadata is no exception. RDF is both a conceptual layout for in-memory processing and also a defined way of writing out the structure. There are just two problems. One is that the format is a lot to digest when you first start working with it. The other is that, depending on the language you’re using, you might end up having to write your own parser for this. Luck is smiling on us, though, in the form of alternative ways to represent that graph structure. A very common representation, and one that is becoming increasingly popular, is JSON (Javascript Object Notation). It has its roots in Javascript, but it has spread far and wide in dozens of languages. Support for it is nearly ubiquitous now. Take a look at it.\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\nJSON is good for representing lots of structured data, but there needs to be something that can go beyond that and hold references to the schemas that apply to certain fields. For this, there is JSON-LD. The LD stands for “Linked Data”. JSON-LD stores additional data in the form of “context” fields and these fields can contain URLs to outside sites storing official, curated ontologies:\n{\n  \"@context\": \"https://json-ld.org/contexts/person.jsonld\",\n  \"@id\": \"http://dbpedia.org/resource/John_Lennon\",\n  \"name\": \"John Lennon\",\n  \"born\": \"1940-10-09\",\n  \"spouse\": \"http://dbpedia.org/resource/Cynthia_Lennon\"\n}\n\n\n\n\n\nFOXDEN (FAIR Open-Science Extensible Data Exchange Network) is a preliminary implementation of a data management system that can be used to meet the FAIR requirements. The system gets inspiration from Linux, in that it provides a collection of tools that work together in a modular fashion. It is possible to use all or some of the components.\nAccess to the FOXDEN modules is via either a web page for each module or by using a command-line tool. Besides being a perfectly reasonable way to use the tools, the command line tool is also well suited to use in scripts.\n\n\nFOXDEN’s modular architecture makes it easy to select which components of it you’d like to use and even makes it possible to substitute your own software if you have specialized needs. The FOXDEN documentation has a walkthrough of basic use. This document will instead give some brief background on each component. You’re encouraged to work through the “Quick Start Guide”.\n\n\nAs you would (likely) expect, the Frontend service generates the web pages through which users can easily interact with the system. Initially, the user is presented with a login page. Once past that, access to the other modules is a click or two away. Of particular interest is the “docs” button toward the upper-right corner. Having the documentation close at hand will prove… handy.\n\n\n\nThe command line tool (“foxden”) is both an alternative way users can access the system as well as a means to interface shell scripts to the system for automating common tasks. The “foxden” command by itself with no arguments will display a list of the available commands and also gives a link to the documentation and a reminder of how to get more detailed help.\n\n\n\nEven in a purely open research environment, it’s still necessary to keep track of who is making changes. This is both for proper attribution as well as non-repudiation (perhaps less of a factor in X-Ray Science than in other disciplines, but the system is built to be versatile). Web users will see a familiar-looking login screen. CLI users will need to authenticate via Kerberos - don’t worry, it’s well described in the introductory documentation. FOXDEN mercifully provides a way to use Kerberos that is simpler than the old-school way.\n\n\n\nThe Discovery service provides a way to query the underlying “management database” that tracks movement of files and the metadata associated with them. The query language is the same one MongoDB used (Mongo QL).\n\n\n\nThe MetaData service is one of the critical components. This module can not only query metadata, for instance finding matching schemas, but also create new schemas and manipulate existing ones.\n\n\n\nThe Provenance service provides a lot of functionality. The tracking of “provenance” is not just something art historians do. The term refers to the tracking of every movement of the data we’re managing, what tools were used to transform it and under what circumstances, and where the data came from. This last element could be, say, “from this instrument on this beamline” or it could be “Dr. J. Doe’s Sept 13th dataset, reduced by this lump of MATLAB code”.\n\n\n\nThe Data Management service abstracts data movement in and out of the underlying Object Store (AWS S3 or compatible). Functions are provided to both manage the Object Store as well as to move data in, move it out, or delete it.\n\n\n\nThe Publication service has two major sections. The first handles the creation and assignment of DOIs (Document Object Identifiers - you’ve seen these in “References” sections) and the association of that identifier with metadata and data. The second section provides a means to interact with Zenodo in a manner consistent with the rest of FOXDEN.\n\n\n\nSpecScan is pretty specific: it is used to create and manipulate records for spec scans. It does what it says on the tin.\n\n\n\nThe MLHub service allows the user to run various Machine Learning (ML) algorithms directly in the FOXDEN environment. TensorFlow is directly supported. Doing this directly inside of FOXDEN seems odd at first, but it follows the paradigm of “moving the compute to the data”, preventing time-consuming retrievals.\n\n\n\nThe CHAP service simplifies running the CHESS-developed CHAP algorithms on data stored in FOXDEN.\n\n\n\nDesigned for novice programmers, the CHAP Notebook service simplifies data analysis by giving users a Jupyter-like interface for writing code modules that are inserted into pre-defined workflows. These modules are also deposited in a code repository for future dissemination."
  },
  {
    "objectID": "newsite/theme5/CF100/domain-metadata-standards.html#why-fair",
    "href": "newsite/theme5/CF100/domain-metadata-standards.html#why-fair",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Running experiments and collecting data is expensive [citation needed]. Funding agencies don’t want to pay for running the same experimental design over and over beyond, perhaps, a replication study. Publishers are looking for new ways to add value to their curation of articles. And most importantly (to us, at least) there are the researchers who are producing these voluminous datasets.\nIndividual researchers have their own needs for FAIR. Speed is a big one - having Findable and Accessible data already available can lead to faster hypothosis formation, allow earlier grant proposals, and establish priority in publication. A second concern is publication. Historically it wasn’t easy to publish raw data because it’s expensive to keep and grants wouldn’t pay for it. There are repositories now where your experimental data can be stored and accessed. Some of them are very domain specific (such as Cornell’s FOXDEN, which we look at later) and others are designed more for “general purpose” use. Finally, adhering to the FAIR Data principles makes it easy (and expected!) for other researchers using your data to cite your work with unambiguous credit and with a permanent network location where the material can be found. FAIR is not restricted to data in the historical sense, but also encompasses source code and workflows.\nPublishers stand to benefit from FAIR, especially if they take full advantage of the ideas. Findable and Accessible (not to mention Interoperable and Reusable) data leads to it being used more and by a wider distribution of users. Researchers in one field probably don’t read journals in a different one. But what if that outlier of a researcher was able to discover data relevant to their work? Now the publisher may have a broader market to sell into. Discoverable data doesn’t come from outer space, with apologies to the astronomers, but is a product of research and that research is accompanied by publications. Publishers will barely care if the underlying data is cited, but they care deeply that the associated journal articles are. FAIR principles, through the “side effect” of increasing publication citations, boost the Impact Factor for journals and that benefits both the authors and the publisher."
  },
  {
    "objectID": "newsite/theme5/CF100/domain-metadata-standards.html#the-principles-of-fair-ness",
    "href": "newsite/theme5/CF100/domain-metadata-standards.html#the-principles-of-fair-ness",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "Findable, Accessible, Interoperable, and Reusable.\nIn this section we will look at the aspects that make up each of Findability, Accessibility, Interoperability, and Reusability. As you read this, keep in mind that the principles are descriptive rather than prescriptive. There is no attempt to describe how to implement an archive, for instance, but rather to discuss what the requirement is and leave the implementation details up to the implementers.\n\n\nThe first requirement for a data set to be useful is for it to be findable. If no one can find it, it may as well not exist. There has to be some kind of automatic way to describe what you are looking for and get a list of possible results in return. If this sounds like a Search Engine, then you’re on the right track. Search engines are pretty good at what they’re designed for, which is hunting through immense collections of text and determining which documents are statistically likely to be useful. The problem is the “collections of text” part: general-purpose search engines have little or no conception of structured data. Imagine trying to search for spreadsheets that contain bond angle data and find the ones that have particular angles, in a particular order, under specific circumstances. Clearly, we’re going to need something more versatile than just a text search engine (sorry, Google).\nThere are four elements that make up the “Findable” principle:\n\nF1. (meta)data are assigned a globally unique and persistent identifier\nF2. data are described with rich metadata (defined by R1 below)\nF3. metadata clearly and explicitly include the identfier of the data it describes\nF4. (meta)data are registered or indexed in a searchable resource\n\nLooking at these elements, we see that there must be rich metadata available to describe a data set well enough to facilitate people finding the (meta)data, and both the data itself as well as the metadata need to be published in a way that facilitates indexing both. The data and metadata must each have a Globally Unique Identifier - think of it as being like a URL. If you have already seen “doi:” used to refer to journal articles, then it will come as no surprise to see this extended to (meta)data.\n\n\n\nMoving on to “Accessible”, let’s take a look at what goes into that:\n\nA1. (meta)data are retrievable by their identifier using a standardized communications protocol\nA1.1 the protocol is open, free, and universally implementable\nA1.2 the protocol allows for an authentication and authorization procedure, where necessary\nA2. metadata are accessible, even when the data are no longer available\n\nThere are two basic aims here. The first is to use a well-known, fully-documented, unencumbered protocol that (optionally) allow for authentication and authorization tasks (“logging in”) to be completed. The last aim is unexpected at first blush but makes sense when you think about it: metadata live forever, even after the data they describe has finally been deleted. Permanently keeping the metadata as a record of what work has gone before makes it possible to get an idea of past research directions. Interestingly, “mining” collections of metadata can produce new knowledge of its own.\n\n\n\nInteroperability simply means that the data are sufficiently described by their metadata so that researchers can create their own tools, or use the tools of others, to work with the data without having to use the same software as the original researcher. Very importantly, if at all possible the data should be preserved in a way that doesn’t require the use of proprietary software to read it. This is not simply because of cost, though that can be a major barrier, but is also a matter of historical preservation - it may be impossible to locate or run old enough software. Imagine having a really useful data set except it’s only readable by a program that only runs on a 35 year old version of MacOS. Yes, that really happens. If we design our (meta)data for interoperability then we can take that 35+ year old dataset and work with it using modern tools.\n\nI1. (meta)data use a formal, accessible, shared, and broadly applicable language for knowledge representation.\nI2. (meta)data use vocabularies that follow FAIR principles\nI3. (meta)data include qualified references to other (meta)data\n\nThe first of these elements is a challenge, honestly. The goal is to have a very rich way of representing metadata that preserves the semantics of the metadata. It’s not enough to simply say “the third column is conductance”. What is needed is a way to say “the third column is conductance, it’s an electronic measure, it’s related to resistance, and here is how”. It’s a lot of work, but luckily for us most of it has been done and we can reuse it - Resource Definition Framework (RDF) is made for this job. We’ll talk more about this in a later section.\n\n\n\nR1. meta(data) are richly described with a plurality of accurate and relevant attributes R1.1. (meta)data are released with a clear and accessible data usage license R1.2. (meta)data are associated with detailed provenance R1.3. (meta)data meet domain-relevant community standards\nThe crux of this principle is ensuring high-quality curation. The metadata should be thoroughly descriptive and will indicate, among other things, the rights the authors has granted to future users and a “chain of custody” that describes how the (meta)data came to be. Finally, the (meta)data should of course be of high quality to begin with, in line with the standards of the discipline. If you are dealing with human subjects, be aware that data privacy requirements (HIPPA, IRB) may restrict or prohibit the publication of data and even metadata. Enough metadata can sometimes be used to reconstruct the data itself. As an example, sometimes age, zip code, and gender is enough to identify someone - some zip codes have very people in them. 28520 has about 250 people."
  },
  {
    "objectID": "newsite/theme5/CF100/domain-metadata-standards.html#metadata",
    "href": "newsite/theme5/CF100/domain-metadata-standards.html#metadata",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "As mentioned above, metadata is simply information that describes your data. Hidden behind that word “simply” is the slight complication that metadata can be anything from really simple to really complicated. Where it falls on that continuum is situational. Small amounts of simple data will probably have simple metadata.\n\n\nWithout a standard means of representation and agreed-upon meanings, the metadata we assemble might be more of a hinderance than a help. Selecting our data’s properties to record is domain-specific in many cases. A very general set of attributes is available at schema.org. Other ontologies exist, of course, and selection among them tends to narrow as you go.\n\n\nRDF is “Resource Description Format” and is a broadly used concept even outside of FAIR. Fundamentally, RDF’s “intention” is to describe the world in terms of triples: subject, predicate, and object. From these building blocks we can construct graph structures (in the “discrete math” sense of the term). As they grow, they can represent linkages between related items, for instance, and that is when the real power of RDF can start to be exploited. With small collections of metadata there is little choice but to handle search terms, for instance. Once the collection expands, new kinds of queries are possible. For instance, different researchers might submit data to an archive. We know the possible range of metadata descriptors and we know what each one means. At this point, we can traverse the RDF graph. It’s easy to go from the starting place to enter the graph and then follow along, hop by hop, expanding the search possibilities by adding related terms.\n\n\n\nFor any data to be useable by a computer, it has to be represented in a way that it can be understood. Metadata is no exception. RDF is both a conceptual layout for in-memory processing and also a defined way of writing out the structure. There are just two problems. One is that the format is a lot to digest when you first start working with it. The other is that, depending on the language you’re using, you might end up having to write your own parser for this. Luck is smiling on us, though, in the form of alternative ways to represent that graph structure. A very common representation, and one that is becoming increasingly popular, is JSON (Javascript Object Notation). It has its roots in Javascript, but it has spread far and wide in dozens of languages. Support for it is nearly ubiquitous now. Take a look at it.\n{\n  \"first_name\": \"John\",\n  \"last_name\": \"Smith\",\n  \"is_alive\": true,\n  \"age\": 27,\n  \"address\": {\n    \"street_address\": \"21 2nd Street\",\n    \"city\": \"New York\",\n    \"state\": \"NY\",\n    \"postal_code\": \"10021-3100\"\n  },\n  \"phone_numbers\": [\n    {\n      \"type\": \"home\",\n      \"number\": \"212 555-1234\"\n    },\n    {\n      \"type\": \"office\",\n      \"number\": \"646 555-4567\"\n    }\n  ],\n  \"children\": [\n    \"Catherine\",\n    \"Thomas\",\n    \"Trevor\"\n  ],\n  \"spouse\": null\n}\nJSON is good for representing lots of structured data, but there needs to be something that can go beyond that and hold references to the schemas that apply to certain fields. For this, there is JSON-LD. The LD stands for “Linked Data”. JSON-LD stores additional data in the form of “context” fields and these fields can contain URLs to outside sites storing official, curated ontologies:\n{\n  \"@context\": \"https://json-ld.org/contexts/person.jsonld\",\n  \"@id\": \"http://dbpedia.org/resource/John_Lennon\",\n  \"name\": \"John Lennon\",\n  \"born\": \"1940-10-09\",\n  \"spouse\": \"http://dbpedia.org/resource/Cynthia_Lennon\"\n}"
  },
  {
    "objectID": "newsite/theme5/CF100/domain-metadata-standards.html#foxden---a-pilot-prototype-example",
    "href": "newsite/theme5/CF100/domain-metadata-standards.html#foxden---a-pilot-prototype-example",
    "title": "FAIR Data - Better Science Through Data Sharing",
    "section": "",
    "text": "FOXDEN (FAIR Open-Science Extensible Data Exchange Network) is a preliminary implementation of a data management system that can be used to meet the FAIR requirements. The system gets inspiration from Linux, in that it provides a collection of tools that work together in a modular fashion. It is possible to use all or some of the components.\nAccess to the FOXDEN modules is via either a web page for each module or by using a command-line tool. Besides being a perfectly reasonable way to use the tools, the command line tool is also well suited to use in scripts.\n\n\nFOXDEN’s modular architecture makes it easy to select which components of it you’d like to use and even makes it possible to substitute your own software if you have specialized needs. The FOXDEN documentation has a walkthrough of basic use. This document will instead give some brief background on each component. You’re encouraged to work through the “Quick Start Guide”.\n\n\nAs you would (likely) expect, the Frontend service generates the web pages through which users can easily interact with the system. Initially, the user is presented with a login page. Once past that, access to the other modules is a click or two away. Of particular interest is the “docs” button toward the upper-right corner. Having the documentation close at hand will prove… handy.\n\n\n\nThe command line tool (“foxden”) is both an alternative way users can access the system as well as a means to interface shell scripts to the system for automating common tasks. The “foxden” command by itself with no arguments will display a list of the available commands and also gives a link to the documentation and a reminder of how to get more detailed help.\n\n\n\nEven in a purely open research environment, it’s still necessary to keep track of who is making changes. This is both for proper attribution as well as non-repudiation (perhaps less of a factor in X-Ray Science than in other disciplines, but the system is built to be versatile). Web users will see a familiar-looking login screen. CLI users will need to authenticate via Kerberos - don’t worry, it’s well described in the introductory documentation. FOXDEN mercifully provides a way to use Kerberos that is simpler than the old-school way.\n\n\n\nThe Discovery service provides a way to query the underlying “management database” that tracks movement of files and the metadata associated with them. The query language is the same one MongoDB used (Mongo QL).\n\n\n\nThe MetaData service is one of the critical components. This module can not only query metadata, for instance finding matching schemas, but also create new schemas and manipulate existing ones.\n\n\n\nThe Provenance service provides a lot of functionality. The tracking of “provenance” is not just something art historians do. The term refers to the tracking of every movement of the data we’re managing, what tools were used to transform it and under what circumstances, and where the data came from. This last element could be, say, “from this instrument on this beamline” or it could be “Dr. J. Doe’s Sept 13th dataset, reduced by this lump of MATLAB code”.\n\n\n\nThe Data Management service abstracts data movement in and out of the underlying Object Store (AWS S3 or compatible). Functions are provided to both manage the Object Store as well as to move data in, move it out, or delete it.\n\n\n\nThe Publication service has two major sections. The first handles the creation and assignment of DOIs (Document Object Identifiers - you’ve seen these in “References” sections) and the association of that identifier with metadata and data. The second section provides a means to interact with Zenodo in a manner consistent with the rest of FOXDEN.\n\n\n\nSpecScan is pretty specific: it is used to create and manipulate records for spec scans. It does what it says on the tin.\n\n\n\nThe MLHub service allows the user to run various Machine Learning (ML) algorithms directly in the FOXDEN environment. TensorFlow is directly supported. Doing this directly inside of FOXDEN seems odd at first, but it follows the paradigm of “moving the compute to the data”, preventing time-consuming retrievals.\n\n\n\nThe CHAP service simplifies running the CHESS-developed CHAP algorithms on data stored in FOXDEN.\n\n\n\nDesigned for novice programmers, the CHAP Notebook service simplifies data analysis by giving users a Jupyter-like interface for writing code modules that are inserted into pre-defined workflows. These modules are also deposited in a code repository for future dissemination."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html",
    "href": "newsite/theme2/SF100/index.html",
    "title": "Linux, Command Line, and Scripting",
    "section": "",
    "text": "The following notes assume that you are all set up to use your accounts on the CLASSE Linux systems.\nDepending on your level of familiarity with the system, you might know enough commands to find your way around. Since it probably is not a good idea to make such assumptions right off the bat, let us see what you might need to know to in order to become a proficient user of the systems."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#linux",
    "href": "newsite/theme2/SF100/index.html#linux",
    "title": "Linux, Command Line, and Scripting",
    "section": "Linux",
    "text": "Linux\nLinux is a free and open source operating system known for stability, security, and versatility. Linux runs on a variety of machines from small embedded systems to powerful servers. A great deal of software runs on Linux.\nYou probably know all this already, so let us skip ahead."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#the-command-line",
    "href": "newsite/theme2/SF100/index.html#the-command-line",
    "title": "Linux, Command Line, and Scripting",
    "section": "The command line",
    "text": "The command line\nTo perform certain kinds of tasks, using the command line is often quicker and more efficient. You can “chain” or compose separate programs together, each of them specializing in doing different things. You can save longer tasks in the form of scripts for later use, and share them with your colleagues.\nHere’s a quick example. You will find documentation for the software installed on lnx201 in the directory /usr/share/doc. Many of those are named README, or README.md, or README.rst, or readme.txt, or some such variation. How many such files are there in /usr/share/doc?\nWe can find that out by using find (a program for searching for files under a directory tree) and wc (a word count program):\n[ssasidharan@lnx201 ~]$ find /usr/share/doc/ -iname \"readme*\" | wc -l\n1589\nMany of the files in /usr/share/doc mention the word “license” or “LICENSE” or some variation thereof. How many such lines are there? In order to find that out, we can use grep (a program that matches patterns), and wc together:\n[ssasidharan@lnx201 ~]$ grep -ir license /usr/share/doc/ | wc -l\n84089\nLearning to use the command line well will leave more power on your hands."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#using-the-shell-prompt",
    "href": "newsite/theme2/SF100/index.html#using-the-shell-prompt",
    "title": "Linux, Command Line, and Scripting",
    "section": "Using the shell prompt",
    "text": "Using the shell prompt\nThe [ssasidharan@lnx201 ~]$ thing with a blinking cursor at the end is called a shell prompt. You type commands at the shell prompt, hit enter, and then something happens in response to that.\nThe examples in these notes are my shell prompt: it contains my username on lnx201, followed by @ character, followed by the name of the computer (or “hostname”), followed by the current directory. Your prompt will be different, because it will contain your username.\nAfter entering the first few characters of a command, you can use the tabtab key for auto-completing commands.\n[ssasidharan@lnx201 /]$ ssh&lt;tab&gt;\nssh          ssh-agent    sshd         sshfs        ssh-keyscan  \nssh-add      ssh-copy-id  sshd-keygen  ssh-keygen   sshpass   \n[ssasidharan@lnx201 /]$ condor_&lt;tab&gt;\nDisplay all 119 possibilities? (y or n)\nBash offers some helpful methods for editing and navigating the history of commands you have previously executed.\n\nYou can use up/down arrow keys to navigate history.\nhistory command will print a list of recently used commands.\nYou can use Ctrl-RCtrl-R to search command history.\nCtrl-ACtrl-A will make the cursor to the beginning of the line.\nCtrl-ECtrl-E will go to the end of the line.\nCtrl-KCtrl-K will “kill” (cut) text from current position to end of line to a buffer called the “kill-ring”.\nCtrl-YCtrl-Y will “yank” (paste) most recently killed text from the kill ring to current cursor position.\nAlt-YAlt-Y will cycle through the kill-ring.\n\nTo exit the shell, you can use exit command or Ctrl-DCtrl-D.\nIf you are using ssh to connect to lnx201, exiting the shell will end your ssh session. If you had opened a terminal window, exiting the shell will close the window."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#how-does-bash-set-up-the-environment",
    "href": "newsite/theme2/SF100/index.html#how-does-bash-set-up-the-environment",
    "title": "Linux, Command Line, and Scripting",
    "section": "How does bash set up the environment?",
    "text": "How does bash set up the environment?\nThere are two kinds of shell sessions: login and non-login. login session starts when you enter a username and password, such as when using ssh. A non-login session starts when you open a terminal window from a desktop.\nDepending on how the session was started, a few shell scripts are read and executed when starting a shell.\nFor login shells these will be:\n\n/etc/profile is a global script that applies to all users.\n\n~/.bash_profile is a script in your home directory, and it is applied when you start a shell.\nIf ~/.bash_profile was not found, bash will attempt to read ~/.bash_login and ~/.profile in order.\n\nFor non-login shells:\n\n/etc/bashrc is the script that applies to everyone.\n~/.bashrc is the script that applies to you.\n\nNon-login shells also inherit the environment from their parent process, which is usually a login shell.\nSystems vary on how they are set up. You should look around lnx201 to find out how this is done there. These files are some examples of shell scripts, which is a topic we’ll visit later in these notes."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#changing-environment-variables",
    "href": "newsite/theme2/SF100/index.html#changing-environment-variables",
    "title": "Linux, Command Line, and Scripting",
    "section": "Changing environment variables",
    "text": "Changing environment variables\nYou can also use export command to overwrite existing environment variables, or add new ones. For example:\n[ssasidharan@lnx201 /]$ export HISTSIZE=2000\n[ssasidharan@lnx201 /]$ echo $HISTSIZE\n2000\nNote that this change in HISTSIZE applies only to the current shell. It will be forgotten when you exit the shell.\nIn order to make the change permanent, you will need to add the line export HISTSIZE=2000 to your ~/.bash_profile file."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#absolute-and-relative-paths",
    "href": "newsite/theme2/SF100/index.html#absolute-and-relative-paths",
    "title": "Linux, Command Line, and Scripting",
    "section": "Absolute and relative paths",
    "text": "Absolute and relative paths\nPaths can be specified in two ways: absolute or relative. An absolute pathname begins with the root directory, /, and contains every directory name, branch by branch.\nAbsolute path to the Desktop directory in my home directory on lnx201 is /home/ssasidharan/Desktop/.\nIn comparison, a relative pathname starts from the working directory. When I’m in my home directory, I can simply use the relative pathname, Desktop.\nEvery directory contains two special directory names, . and .., in which . refers to the current directory, and .. refers to the parent directory of the current directory.\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\n[ssasidharan@lnx201 ~]$ cd .\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\n[ssasidharan@lnx201 ~]$ cd ..\n[ssasidharan@lnx201 home]$ pwd\n/home\n[ssasidharan@lnx201 home]$ cd ..\n[ssasidharan@lnx201 /]$ pwd\n/"
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#wildcards",
    "href": "newsite/theme2/SF100/index.html#wildcards",
    "title": "Linux, Command Line, and Scripting",
    "section": "Wildcards",
    "text": "Wildcards\nThe shell gives special treatment to some characters, known as wildcards. Using wildcard characters, we can quickly specify groups of files.\nThe wildcard character * stands for any set of characters. For example, you can list the names of all programs in /usr/bin that start with ab with ls /usr/bin/ab*:\n[ssasidharan@lnx201 ~]$ ls /usr/bin/ab*\n/usr/bin/ab  /usr/bin/abs2rel\nThe wildcard character ? stands for any single character. So if you want to list the filenames in /usr/bin that starts with any character, followed by abc, followed by any characters:\n[ssasidharan@lnx201 ~]$ ls /usr/bin/?abc*\n/usr/bin/kabc2mutt  /usr/bin/kabcclient"
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#the-current-working-directory",
    "href": "newsite/theme2/SF100/index.html#the-current-working-directory",
    "title": "Linux, Command Line, and Scripting",
    "section": "The current working directory",
    "text": "The current working directory\nAt any time in the shell, we are “inside” a single directory, known as the current working directory. When you list files with ls, a list of files and directories of the current working directory will be printed on the output.\nWhen you log in to lnx201, initially you will be in a directory named /home/${USER}, where ${USER} is your username on lnx201. This is what is known as your /home directory/. When you log in first, your home directory will be your current working directory.\nTo find where you are, use the command pwd.\nThe below commands are useful:\n\nmkdir test will create a directory named test\ncd test will change the working directory to test.\nls will list files and directories in the current working directory.\nrm will remove a file.\nrm &lt;name of directory&gt; will not remove a directory; you have to remove it recursively, like so: rm -r &lt;name of directory&gt;.\n\ncd - is useful: it will switch you to the directory that you were previously in:\n[ssasidharan@lnx201 /]$ cd /usr/\n[ssasidharan@lnx201 usr]$ pwd\n/usr\n[ssasidharan@lnx201 usr]$ cd share/\n[ssasidharan@lnx201 share]$ pwd\n/usr/share\n[ssasidharan@lnx201 share]$ cd -\n/usr\n[ssasidharan@lnx201 usr]$ pwd\n/usr\nRunning cd ~ (or simply cd) will drop you back in your home directory:\n[ssasidharan@lnx201 ~]$ cd /usr/share/doc/\n[ssasidharan@lnx201 doc]$ pwd\n/usr/share/doc\n[ssasidharan@lnx201 doc]$ cd ~\n[ssasidharan@lnx201 ~]$ pwd\n/home/ssasidharan\nIt is worth noting that the shell will substitute ~ for your home directory.\n. and .. are special directory names: . means the current directory, and .. means its parent directory, or the directory above it in the directory hierarchy.\ntouch command is used to change file timestamps. You can also use touch to create an empty file, like so: touch test.txt.\n\nSymbolic links\nOn my home directory on lnx201, when I do an ls -l (which is for ls with long file listing format), I would see something like this:\nlrwxrwxrwx  1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\nThe first letter of the listing is l and the entry kind of suggests that my Downloads directory is a reference to another directory, /cdat/tem/ssasidharan/Downloads. The Downloads directory in my home directory is what is called a symbolic link, which also known as a soft link or “symlink”.\nWith symbolic links, we can have shortcuts to other files or directories."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#users-and-groups",
    "href": "newsite/theme2/SF100/index.html#users-and-groups",
    "title": "Linux, Command Line, and Scripting",
    "section": "Users and groups",
    "text": "Users and groups\nLinux is a multi-user operating system. Since many people can be using the system, there needs to be mechanisms in place to ensure separation between them, while ensuring that they can access shared resources when necessary.\nThe basic mechanism is the concept of users and groups.\nThe root user is a special user that has all the permissions. They can change most things about the system. The root user can change system configuration, add and remove users and groups, etc.\nMost of the time, we do not need neither the power nor the responsibilities of the root user. So we have a non-root, regular user account in lnx201.\nYour account also belongs to certain groups. Groups are the way to grant permission to a group of accounts. You can find the groups you belong to using groups command:\n[ssasidharan@lnx201 ~]$ groups\nchess classeuser\n[ssasidharan@lnx201 ~]$\nUsers and groups have distinct numerical identifiers too. You can find them with id command:\n[ssasidharan@lnx201 ~]$ id\nuid=63499(ssasidharan) gid=262(chess) groups=262(chess),750(classeuser)\nIf you run ls -l (-l is for long listing format) command to list files and folders in your home directory, the result will be something like this:\n[ssasidharan@lnx201 ~]$ ls -l\ntotal 4\ndrwxr-xr-x 2 ssasidharan chess   28 Mar 28 09:36 bin\ndrwxr-xr-x 2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x 2 ssasidharan chess   30 Mar 26 15:22 Desktop\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Documents\nlrwxrwxrwx 1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r-- 1 ssasidharan chess 3254 Mar  7 15:55 helloworld.ipynb\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Music\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Pictures\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Public\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Templates\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Videos\nLet us see what the above columns means:\n\nThe first column lists permissions on the file/folder. (We will see what this means in the next section.)\nThe second column shows number of links to it.\nThe third one shows the user who owns it.\nThe fourth one shows the group that owns the file.\nThe fifth one is the size of the file in bytes. Note that directories are a little special here – what you see here is not the total size of all the files and folders under the directory, but the space the directory itself uses on disk.\nThe next column (the whole Mar 26 15:21 segment) shows a timestamp when the file/folder was last modified.\nFinally, the name of the file/folder. Note that Downloads -&gt;   /cdat/tem/ssasidharan/Downloads is a bit special: it means that Downloads folder is in fact a link to /cdat/tem/ssasidharan/Downloads."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#permissions-and-ownership",
    "href": "newsite/theme2/SF100/index.html#permissions-and-ownership",
    "title": "Linux, Command Line, and Scripting",
    "section": "Permissions and ownership",
    "text": "Permissions and ownership\nLet us see what a string like drwxr-xr-x from the above example means. This string, sometimes called “permission bits” or “file mode bits”, is ten characters long. Each of the characters are shorthand signifying something.\n\nThe first d stands for directory. (For files, this will be a -.)\nThe next three rwx are for user’s permissions.\nThe next three r-x are for group permissions.\nThe final three r-x are for permissions for the rest of the users.\n\nNow, what do those r and w and x mean?\n\nr means permission to read.\nw means permission to write.\nx means permission to execute, in the case of files. In the case of directories, x means that you can cd into them.\n\n\nChanging permissions with chmod\nYou can use chmod command to change permissions. If you create a shell script named test.sh, for example, it won’t be executable by default. You will have to change the file mode bits using chmod:\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ ./test.sh\n-bash: ./test.sh: Permission denied\n[ssasidharan@lnx201 ~]$ chmod +x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ ./test.sh\nYou can remove the x bit like so:\n[ssasidharan@lnx201 ~]$ chmod -x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nYou can also grant permission to just the user, or group, or others:\n[ssasidharan@lnx201 ~]$ chmod u+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod g+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod o+x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nYou can also combine u, g, o bits and r, w, x bits with + or -:\n[ssasidharan@lnx201 ~]$ chmod ugo-r test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n--wx--x--x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\nI just made the file unreadable by everyone, even me!\n[ssasidharan@lnx201 ~]$ cat test.sh\ncat: test.sh: Permission denied\nOf course you can restore the permission with chmod ugo+r test.sh\nNote that when invoking chmod, a (or all) is equivalent of ugo (user + group + others). You can also omit a or ugo if you want everyone to have the same permissions. So the below all are equivalent:\n[ssasidharan@lnx201 ~]$ chmod ugo+r test.sh\n[ssasidharan@lnx201 ~]$ chmod a+r test.sh\n[ssasidharan@lnx201 ~]$ chmod +r test.sh\n(If you want to change owner or group of a file/folder, you can do that with chown and chgrp. This probably is not immediately useful; it is enough to know that these commands exist.)"
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#noteworthy-facts-about-file-names",
    "href": "newsite/theme2/SF100/index.html#noteworthy-facts-about-file-names",
    "title": "Linux, Command Line, and Scripting",
    "section": "Noteworthy facts about file names",
    "text": "Noteworthy facts about file names\n\nFile/folder names that begin with a . (period character) are “hidden”: meaning that they will not be listed in the output of ls command by default. You can list them with ls -a. They are also called dotfiles.\nConfiguration files for the programs you use (such as .bashrc for bash configuration) are often saved in hidden files. This way they usually stay out of your way without creating a clutter.\nFile/folder names and commands are case sensitive in Linux. Thus Notes.txt and notes.txt and NOTES.TXT are all distinct files.\nAs a matter of convenience, it is better to avoid spaces and special characters in file/folder names, as it will make tasks a little more difficult. If you need to represent spaces between words, you can replace spaces with _ (the underscore character)."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#io-redirection",
    "href": "newsite/theme2/SF100/index.html#io-redirection",
    "title": "Linux, Command Line, and Scripting",
    "section": "I/O redirection",
    "text": "I/O redirection\nI/O redirection lets us to change where standard output gets printed. To redirect standard output, we use the &gt; operator.\n[ssasidharan@lnx201 ~]$ ls -l &gt; ls-output.txt\nAs a result of redirection, a new file named ls-output.txt will be created. You can view its contents using cat command.\n[ssasidharan@lnx201 ~]$ ls -l ls-output.txt\n-rw-r--r-- 1 ssasidharan chess 807 Apr  1 17:32 ls-output.txt\n[ssasidharan@lnx201 ~]$ cat ls-output.txt\ntotal 4\ndrwxr-xr-x 2 ssasidharan chess   28 Mar 28 09:36 bin\ndrwxr-xr-x 2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x 2 ssasidharan chess   30 Mar 26 15:22 Desktop\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Documents\nlrwxrwxrwx 1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r-- 1 ssasidharan chess 3254 Mar  7 15:55 helloworld.ipynb\n-rw-r--r-- 1 ssasidharan chess    0 Apr  1 17:32 ls-output.txt\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Music\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Pictures\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Public\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Templates\n-rwxr-xr-x 1 ssasidharan chess    0 Mar 28 13:39 test.sh\ndrwxr-xr-x 2 ssasidharan chess    6 Mar 26 15:21 Videos\nNote that if there already was a file named ls-output.txt, the redirection above would have overwritten its contents. You want to be careful about this.\nWhat if you want to discard stdout completely? You can redirect it to the special file /dev/null:\n[ssasidharan@lnx201 ~]$ ls -l &gt; /dev/null\nIf you want to append stdout to a file instead of overwriting it, you can use &gt;&gt; operator:\n[ssasidharan@lnx201 ~]$ ls -l &gt;&gt; ls-output.txt\nThe &lt; operator is a sort of inverse of the &gt; operator:\n[ssasidharan@lnx201 ~]$ echo \"Shall I compare thee to a summer’s day?\" &gt; sonnet18.txt\n[ssasidharan@lnx201 ~]$ cat sonnet18.txt\nShall I compare thee to a summer’s day?\n[ssasidharan@lnx201 ~]$ cat &lt; sonnet18.txt\nShall I compare thee to a summer’s day?"
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#pipes",
    "href": "newsite/theme2/SF100/index.html#pipes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Pipes",
    "text": "Pipes\nPrograms can write to standard output. Programs can also read from standard input. This means we can “chain” them together, such that one programs standard output is “piped” into another program’s standard input.\nThe operator to do this is | (vertical bar), also known as a pipe, and it is used in this manner: command1 | command2.\n[ssasidharan@lnx201 ~]$ ls -l /bin/ | less\nThe output of ls -l /bin is fairly large, so we pipe it into less, which allows you to scroll the output backward and forward, using up and down keyboard keys.\nYou can form longer pipes like this:\n[ssasidharan@lnx201 ~]$ ls /bin /usr/bin /sbin /usr/sbin | sort | uniq | wc\n   4289    4288   46820\n\nsort will sort lines of text files.\nuniq is used to filter adjacent matching lines the output of sort.\nwc is a word count program. It counts lines, words, and bytes present in its input."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#listing-processes",
    "href": "newsite/theme2/SF100/index.html#listing-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Listing processes",
    "text": "Listing processes\nYou can list running processes using ps command:\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 694411 pts/81   00:00:00 ps\n3479688 pts/81   00:00:00 bash\nBy default, ps prints processes of the current user and terminal in four columns:\n\nPID is process id.\nTTY is the terminal associated with the process.\nTIME is the elapsed CPU time for the process.\nCMD is the command that created the process.\n\nUsually there are many more processes running in the system, and sometimes they were started by other users. You can list them, with more detail, by passing some options to ps:\n[ssasidharan@lnx201 ~]$ ps -ef | head\nUID          PID    PPID  C STIME TTY          TIME CMD\nroot           1       0  0 Jan10 ?        03:14:05 /usr/lib/systemd/systemd --switched-root --system --deserialize 22\nroot           2       0  0 Jan10 ?        00:01:12 [kthreadd]\nroot           6       2  0 Jan10 ?        00:12:16 [ksoftirqd/0]\nroot           7       2  0 Jan10 ?        00:01:10 [migration/0]\nroot           8       2  0 Jan10 ?        00:00:00 [rcu_bh]\nroot           9       2  0 Jan10 ?        11:14:26 [rcu_sched]\nroot          10       2  0 Jan10 ?        00:00:00 [lru-add-drain]\nroot          11       2  0 Jan10 ?        00:05:22 [watchdog/0]\nroot          12       2  0 Jan10 ?        00:00:24 [watchdog/1]\nRun man ps for details.\nPrograms like top will list processes in friendlier, fancier format."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#background-and-foreground-processes",
    "href": "newsite/theme2/SF100/index.html#background-and-foreground-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Background and foreground processes",
    "text": "Background and foreground processes\nBy default, commands run in the foreground: they do their thing, use the terminal (to read input, print output), and finally exit. You need to wait for a foreground process to end before you start the next one, or use another terminal.\nWhen have a long-running process, you have the option of sending it to the background, using the & operator:\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 949751\nYou can use Ctrl-ZCtrl-Z to stop a foreground process and send it to the background:\n[ssasidharan@lnx201 ~]$ sleep 100\n^Z\n[1]+  Stopped                 sleep 100\nYou can list background processes using jobs command:\n[ssasidharan@lnx201 ~]$ jobs\n[1]-  Running                 sleep 100 &\n[2]+  Stopped                 sleep 100\nYou can bring a background process to foreground using fg command, and you can terminate it using Ctrl-CCtrl-C:\n[ssasidharan@lnx201 ~]$ fg 2\nsleep 100\n^C\n[ssasidharan@lnx201 ~]$\nYou can use bg command to resume a stopped background process:\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 1746205\n[ssasidharan@lnx201 ~]$ sleep 100\n^Z\n[2]+  Stopped                 sleep 100\n[ssasidharan@lnx201 ~]$ jobs\n[1]-  Running                 sleep 100 &\n[2]+  Stopped                 sleep 100\n[ssasidharan@lnx201 ~]$ bg %2\n[2]+ sleep 100 &"
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#terminating-processes",
    "href": "newsite/theme2/SF100/index.html#terminating-processes",
    "title": "Linux, Command Line, and Scripting",
    "section": "Terminating processes",
    "text": "Terminating processes\nSometimes you might want to terminate a program, perhaps because it is using too much CPU or memory. You can find out the offending program’s ID using ps or top or htop, and then you can use kill command to end the process.\nBy default, kill sends a signal called SIGTERM (more on signals later). If SIGTERM is unable to terminate the process (such as when the program is ignoring SIGTERM), you can try SIGKILL:\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 796679 pts/116  00:00:00 bash\n1185454 pts/116  00:00:00 ps\n1748299 pts/116  00:00:00 sleep\n[ssasidharan@lnx201 ~]$ kill 1748299\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 796679 pts/116  00:00:00 bash\n1203470 pts/116  00:00:00 ps\n1748299 pts/116  00:00:00 sleep\n[ssasidharan@lnx201 ~]$ kill -SIGKILL 1748299\n[2]+  Killed                  sleep 100\nYou can use killall command to kill processes by name:\n[ssasidharan@lnx201 ~]$ killall sleep\nsleep(1469283): Operation not permitted\nsleep(1509215): Operation not permitted\nsleep: no process found\nIn the above example, you are not running a sleep process, but some other users are, and you are not allowed to terminate them."
  },
  {
    "objectID": "newsite/theme2/SF100/index.html#signals",
    "href": "newsite/theme2/SF100/index.html#signals",
    "title": "Linux, Command Line, and Scripting",
    "section": "Signals",
    "text": "Signals\nAs mentioned above, kill command sends signals to running processes, and we’ve already seen SIGTERM and SIGKILL. Signals are a process control mechanism. They are used to stop, resume, or terminate processes, and more.\nWhen we use Ctrl-CCtrl-C or Ctrl-ZCtrl-Z, we are sending signals to process – SIGINT (or “keyboard interrupt”) and SIGTSTP (or “terminal stop”), respectively.\nSignals have numbers: SIGKILL is 9, so you can use kill -9 &lt;pid&gt; instead of kill -SIGKILL &lt;pid&gt;. You can also omit the SIG prefix, and use kill -KILL &lt;pid&gt;.\nHere are some common signals:\n\n       Signal     Value     Action   Comment\n       ──────────────────────────────────────────────────────────────────────\n       SIGHUP        1       Term    Hangup detected on controlling terminal\n                                     or death of controlling process\n       SIGINT        2       Term    Interrupt from keyboard\n       SIGQUIT       3       Core    Quit from keyboard\n       SIGILL        4       Core    Illegal Instruction\n       SIGABRT       6       Core    Abort signal from abort(3)\n       SIGFPE        8       Core    Floating point exception\n       SIGKILL       9       Term    Kill signal\n       SIGSEGV      11       Core    Invalid memory reference\n       SIGPIPE      13       Term    Broken pipe: write to pipe with no\n                                     readers\n       SIGALRM      14       Term    Timer signal from alarm(2)\n       SIGTERM      15       Term    Termination signal\nRun the command man 7 signal to read signal command’s manual page."
  },
  {
    "objectID": "newsite/theme2/SF100/linux-commandline-scripting.html",
    "href": "newsite/theme2/SF100/linux-commandline-scripting.html",
    "title": "Intro to Linux, the command line, and programming in Python",
    "section": "",
    "text": "Welcome to Systems Fundamentals! In this section, we’re going to learn how to use the computing resources available at CHESS. In particular, we’ll focus on the Linux cluster available to researchers. How to access these computers remotely, how to use them via their command-line interface, and how to write actual, legitimate Python programs are the three main topics. In addition, a “bonus” section at the end will look deeper into what is really happening on the servers when we do things like access files or run programs.\nThere is a lot of material to cover, and in truth we’ll still just be scratching the surface. The goal here is to give you enough information to get started and to find more information as you need it. We recommend you explore each of the following sections in order.\n\nGetting Started (logging in and out with NoMachine and ssh)\nCommand Line Basics\nWriting Standalone Python Programs\nHow Linux Really Works\n\nShall we begin? Go ahead, click on the link for “Getting Started”, and let’s connect to a server!"
  },
  {
    "objectID": "newsite/theme2/SF101/containers-and-virtualization.html",
    "href": "newsite/theme2/SF101/containers-and-virtualization.html",
    "title": "Containers and Virtualization",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-02CondaRepos.html",
    "href": "newsite/theme1/PE101/PE101-02CondaRepos.html",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "",
    "text": "As we’ve mentioned, there is an awful lot of Python code out in the world completely free for us to use. There packages as broadly useful as “SciPy” (numerical methods for the sciences) and as narrowly interesting as “bosch-thermostat-client” for setting values in Bosch Thermostats. With so many packages available, if there is something you need to do in a Jupyter notebook or in a Python program, there is a good chance someone else has done at least part of it and made it available as a package.\nPublicly-available packages have to be kept somewhere to be useful. If programmers can’t find them, then they might as well not be made public. Fortunately, the Python world has a central repository - www.pypi.org. The repository (usually shortened to just “repo”, both vowels are long) is searchable.\nOne thing to notice in the repository is that packages there have version numbers. It’s also pretty common for one package to require another - scikit-learn requires scipy, for instance. Sometimes the requirements also have version numbers. There can be cases where Package A requires Package B, and specifically Package B has to have a version number greater than or equal to 11. When you have a lot of packages to import it can get cumbersome to check all the dependencies and make sure you have a combination that satisfies all the constraints.\nIn fact, it’s more than just cumbersome. Automatic checking is the only practical solution once the problem gets much size to it, and a piece of software that does this is called a SAT solver."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-02CondaRepos.html#conda---hard-problems-made-solvable",
    "href": "newsite/theme1/PE101/PE101-02CondaRepos.html#conda---hard-problems-made-solvable",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Conda - hard problems made solvable",
    "text": "Conda - hard problems made solvable\nFortunately, there is Conda, a software tool and a repository of its own. The conda developers keep a subset of the half billion packages that are available and they ensure that their repository reflects a combination of versions that should work together. They do the hard work, we take advantage of it. They stay in business by selling their tools to commercial users but, being a research organization, we’re not required to pay.\nConda also has another useful trick: it can take advantage of Python’s virtual environments to let you load outside packages into a completely private space. This way, when you download and install the “instantnobelprize” package (I made that up), it’s only written to your own directories. Other users, and the system as a whole, are protected from whatever it might contain.\nSetting up Conda and using it with Jupyter notebooks takes a little bit of work and has to be done from the command line, but so often it’s worth it. If you haven’t used the command line yet, take a look at the training units in SF100 on the Linux command line and scripting.\nWhat follows is taken directly from the CLASSE wiki entry for JupyterHub with just a few modifications."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-02CondaRepos.html#python-environments",
    "href": "newsite/theme1/PE101/PE101-02CondaRepos.html#python-environments",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Python Environments",
    "text": "Python Environments\nA Python environment is a local, unique to a user, repository plus a copy of the Python interpreter itself. Having a private environment is how we can load specific versions of packages even when the server has a different one. It even lets us install specific versions of python without affecting anyone else.\nWhen you launch a new notebook, you are presented with a dropdown to select your desired python kernel. The default Python 3 kernel is a CLASSE-IT maintained conda environment in /nfs/opt/anaconda3/envs/python3"
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-02CondaRepos.html#adding-new-environments",
    "href": "newsite/theme1/PE101/PE101-02CondaRepos.html#adding-new-environments",
    "title": "PE101-02: Repositories, Sharing, and Conda",
    "section": "Adding New Environments",
    "text": "Adding New Environments\nIn addition, you can install your own python environments and have them added as an option when creating new notebooks.\nCreate your own python environment using your desired python installation. Please see LinuxSoftwareDevelopment for a list of centrally maintained python environments, and further down LinuxSoftwareDevelopment for tips on creating your own conda installation.\nInstall anything you like in the environment, but you MUST at least install ipykernel. For example\npip install ipykernel\nActivate the new environment. If using conda, this would look something like:\nsource /path/to/conda/install/bin/activate conda activate my-python-env\nAdd the virtual environment as a jupyter kernel using\npython -m ipykernel install --user --name=my-python-env --display-name \"My Python Env\"\nThis adds the kernel to ~/.local/share/jupyter/kernels/ and now it can be used by Jupyter. When you create a new notebook now, “My Python Env” will be one of your choices."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-01Packages.html",
    "href": "newsite/theme1/PE101/PE101-01Packages.html",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "By itself, Python provides everything you need to write programs. These programs won’t have a fancy user interface and they may not run very fast, but they’ll work. If that’s all Python offered, it might have become a popular language but it wouldn’t have taken over most of the world the way it has. No, what Python has going for it is a simple way to take commonly-used chunks of code, wrap them up neatly into sharable budles, and distribute those bundles far and wide. The mechanism for doing this in Python is called packages.\nIn this training unit, PE101-01, we’re going to look at some of the packages that come with Python. These are packages that you can count on being available anywhere you can run Python. In the next unit, PE101-02, we’ll look at how to find and use packages hosted in repositories available to anyone but not necessarily already installed where you’re running your programs.\nPython is, by itself, a rather simple language. The PE100 series of units has introduced you to almost all of the language. The language is kept small by moving the “nice to have, but not really necessary” parts into their own independent packages. Let’s start with an example:\nimport math\n\nprint(\"pi equals\", math.pi)\nprint(\"There are\", math.perm(52,2), \"possible outcomes when drawing two cards from a deck\")\nprint(\"The natural logarithm of 7.994 is\", math.log(7.994))\n\npi equals 3.141592653589793\nThere are 2652 possible outcomes when drawing two cards from a deck\nThe natural logarithm of 7.994 is 2.0786912602891316"
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-01Packages.html#packages",
    "href": "newsite/theme1/PE101/PE101-01Packages.html#packages",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Packages",
    "text": "Packages\nThere are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\nimport random as rand\n\nprint(rand.random())\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-01Packages.html#modules",
    "href": "newsite/theme1/PE101/PE101-01Packages.html#modules",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Modules",
    "text": "Modules\nSo far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\nimport os.path as op\n\nif op.exists(\"/usr/bin\"):\n    print(\"all is good.\")\nelse:\n    print(\"I don't even know how the server booted!\")\n    \n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory."
  },
  {
    "objectID": "newsite/theme1/PE101/PE101-01Packages.html#coming-up-next-packages-from-the-outside-world",
    "href": "newsite/theme1/PE101/PE101-01Packages.html#coming-up-next-packages-from-the-outside-world",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Coming up next: Packages from the outside world",
    "text": "Coming up next: Packages from the outside world\nAs we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs-testing-debugging.html",
    "href": "newsite/theme1/PE103/vcs-testing-debugging.html",
    "title": "Version Control, Testing and Debugging",
    "section": "",
    "text": "Version Control, Testing and Debugging\n\nVersion Control\nTesting\nDebugging"
  },
  {
    "objectID": "newsite/theme1/PE103/testing.html",
    "href": "newsite/theme1/PE103/testing.html",
    "title": "Testing",
    "section": "",
    "text": "Testing proves that your code works as expected in response to the inputs that it is expected to receive.\nOf course you can “manually” run your code and check that it works as expected. Since frequent manual testing will soon become tedious, you can also write code that tests your code, so that much of your testing is automated.\nWhen you test your code, you will be more confident about the correctness of your code. As your project evolves, the tests you have written will help you to test the changes more confidently. Your tests will also serve as a sort of documentation about how to use the code.\nThere are several ways to test your code:\n\nUnit tests test the smaller units of your code, namely: functions, classes, and methods.\nIntegration tests are used to prove that your code works correctly when they interface with externals systems (for example: you read weather data from NOAA).\nSystem tests are used to test that your application as a whole works as expected.\n\nThis is a big topic. In the interest of practicality, we will limit the discussion to unit tests here.\n\n\nPython standard library ships a unittest module, which provides some tools for testing your code. Let us see how this works in practice with a quick example.\nSuppose you have written a method for temperature conversion, in a module named temperature.py:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"\n    Convert temperature from Celsius to Fahrenheit.\n    \n    :param celsius (float): Temperature in Celsius\n    \n    :returns: Temperature converted to Fahrenheit\n    \"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\nYou will write tests for your code in a corresponding module named test_temperature.py:\n\n\ntest_temperature.py\n\nimport unittest\n\nfrom temperature import celsius_to_fahrenheit\n\nclass TestCelsiusToFahrenheit(unittest.TestCase):\n    def test_conversion(self):\n        # Test conversion for 0°C\n        self.assertEqual(celsius_to_fahrenheit(0), 32)\n\n        # Test conversion for 100°C\n        self.assertEqual(celsius_to_fahrenheit(100), 212)\n\n        # Test conversion for negative temperature -10°C\n        self.assertEqual(celsius_to_fahrenheit(-10), 14)\n\nif __name__ == '__main__':\n    unittest.main()\n\nAnd then you will run your tests:\n$ python3 test_temperature.py \n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nAs you see, unittest module provides:\n\nA TestCase class, which has an assertEqual() method (and several other assert methods) to check that the code has executed as expected.\n\n\nA unittest.main() function method which provides a way to run the test.\n\nThe TestCase class also provides methods to handle test fixtures to set things up before tests are run, and tear them down later, namely setUp() and tearDown().\n\n\nWhat would a failing test look like? Let us add a new test case to our TestCelsiusToFahrenheit class:\n    def test_conversion_bad_input(self):\n        # Test with bad input\n        self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\nThis will of course fail, and leave us enough hints about why it failed:\n$ python3 test_temperature.py \n.E\n======================================================================\nERROR: test_conversion_bad (__main__.TestCelsiusToFahrenheit.test_conversion_bad)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/test_temperature.py\", line 18, in test_conversion_bad\n    self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 14)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/temperature.py\", line 9, in celsius_to_fahrenheit\n    fahrenheit = (celsius * 9/5) + 32\n                  ~~~~~~~~~~~^~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nFAILED (errors=1)\nNow you can decide how to fix the code (or the test), or whether to fix anything at all. You may want to reject inputs other than numbers in your celsius_to_fahrenheit() function. Or you may decide that failing on unexpected inputs is fine, and change the test accordingly:\n    def test_conversion_bad_input_expect_exception(self):\n        # Test with bad input, and expect an exception\n        with self.assertRaises(TypeError):\n            celsius_to_fahrenheit(\"not temperature\")\n\n\n\n\nPyTest is a framework for writing and running tests. You can use PyTest along with unittest module.\nYou will need to install pytest with:\n$ pip install pytest\nPyTest provides a commandline program named pytest, which will discover the tests you have written in your project and run them. The output is a little fancier and probably more helpful than simply running the test module directly:\n$ pytest \n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\nplugins: anyio-4.3.0\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9/5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.08s ==========================\n\n\n\nWhen you share your code with your colleagues, you will want to make sure that your code works in environments other than your own too. Tox will help you to test that your project builds and installs correctly under several environments. You might want to test your code with several versions of Python, and various dependencies, for example.\nYou will write a configuration file named tox.ini:\n\n\ntox.ini\n\n[tox]\nrequires =\n    tox&gt;=4\nenv_list = py{38,39,310,311}\n\n[testenv]\ndescription = run unit tests\ndeps =\n    pytest&gt;=7\ncommands =\n    pytest {posargs:tests}\n\nYou can install tox like so:\n$ pip install tox\nAnd then run tox like so:\n$ tox\npy38: skipped because could not find python interpreter with spec(s): py38\npy38: SKIP ⚠ in 0.01 seconds\npy39: skipped because could not find python interpreter with spec(s): py39\npy39: SKIP ⚠ in 0 seconds\npy310: skipped because could not find python interpreter with spec(s): py310\npy310: SKIP ⚠ in 0 seconds\npy311: commands[0]&gt; pytest\n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\ncachedir: .tox/py311/.pytest_cache\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:19: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9 / 5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.04s ==========================\npy311: exit 1 (0.29 seconds) /home/sajith/projects/x-cite/X-CITE/theme1/PE103&gt; pytest pid=935973\n  py38: SKIP (0.01 seconds)\n  py39: SKIP (0.00 seconds)\n  py310: SKIP (0.00 seconds)\n  py311: FAIL code 1 (0.32=setup[0.04]+cmd[0.29] seconds)\n  evaluation failed :( (0.41 seconds)"
  },
  {
    "objectID": "newsite/theme1/PE103/testing.html#writing-unit-tests",
    "href": "newsite/theme1/PE103/testing.html#writing-unit-tests",
    "title": "Testing",
    "section": "",
    "text": "Python standard library ships a unittest module, which provides some tools for testing your code. Let us see how this works in practice with a quick example.\nSuppose you have written a method for temperature conversion, in a module named temperature.py:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    \"\"\"\n    Convert temperature from Celsius to Fahrenheit.\n    \n    :param celsius (float): Temperature in Celsius\n    \n    :returns: Temperature converted to Fahrenheit\n    \"\"\"\n    fahrenheit = (celsius * 9/5) + 32\n    return fahrenheit\n\nYou will write tests for your code in a corresponding module named test_temperature.py:\n\n\ntest_temperature.py\n\nimport unittest\n\nfrom temperature import celsius_to_fahrenheit\n\nclass TestCelsiusToFahrenheit(unittest.TestCase):\n    def test_conversion(self):\n        # Test conversion for 0°C\n        self.assertEqual(celsius_to_fahrenheit(0), 32)\n\n        # Test conversion for 100°C\n        self.assertEqual(celsius_to_fahrenheit(100), 212)\n\n        # Test conversion for negative temperature -10°C\n        self.assertEqual(celsius_to_fahrenheit(-10), 14)\n\nif __name__ == '__main__':\n    unittest.main()\n\nAnd then you will run your tests:\n$ python3 test_temperature.py \n.\n----------------------------------------------------------------------\nRan 1 test in 0.000s\n\nOK\nAs you see, unittest module provides:\n\nA TestCase class, which has an assertEqual() method (and several other assert methods) to check that the code has executed as expected.\n\n\nA unittest.main() function method which provides a way to run the test.\n\nThe TestCase class also provides methods to handle test fixtures to set things up before tests are run, and tear them down later, namely setUp() and tearDown().\n\n\nWhat would a failing test look like? Let us add a new test case to our TestCelsiusToFahrenheit class:\n    def test_conversion_bad_input(self):\n        # Test with bad input\n        self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\nThis will of course fail, and leave us enough hints about why it failed:\n$ python3 test_temperature.py \n.E\n======================================================================\nERROR: test_conversion_bad (__main__.TestCelsiusToFahrenheit.test_conversion_bad)\n----------------------------------------------------------------------\nTraceback (most recent call last):\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/test_temperature.py\", line 18, in test_conversion_bad\n    self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 14)\n                     ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/home/sajith/projects/x-cite/X-CITE/theme1/PE103/temperature.py\", line 9, in celsius_to_fahrenheit\n    fahrenheit = (celsius * 9/5) + 32\n                  ~~~~~~~~~~~^~\nTypeError: unsupported operand type(s) for /: 'str' and 'int'\n\n----------------------------------------------------------------------\nRan 2 tests in 0.001s\n\nFAILED (errors=1)\nNow you can decide how to fix the code (or the test), or whether to fix anything at all. You may want to reject inputs other than numbers in your celsius_to_fahrenheit() function. Or you may decide that failing on unexpected inputs is fine, and change the test accordingly:\n    def test_conversion_bad_input_expect_exception(self):\n        # Test with bad input, and expect an exception\n        with self.assertRaises(TypeError):\n            celsius_to_fahrenheit(\"not temperature\")"
  },
  {
    "objectID": "newsite/theme1/PE103/testing.html#pytest",
    "href": "newsite/theme1/PE103/testing.html#pytest",
    "title": "Testing",
    "section": "",
    "text": "PyTest is a framework for writing and running tests. You can use PyTest along with unittest module.\nYou will need to install pytest with:\n$ pip install pytest\nPyTest provides a commandline program named pytest, which will discover the tests you have written in your project and run them. The output is a little fancier and probably more helpful than simply running the test module directly:\n$ pytest \n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\nplugins: anyio-4.3.0\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:18: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9/5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.08s =========================="
  },
  {
    "objectID": "newsite/theme1/PE103/testing.html#tox",
    "href": "newsite/theme1/PE103/testing.html#tox",
    "title": "Testing",
    "section": "",
    "text": "When you share your code with your colleagues, you will want to make sure that your code works in environments other than your own too. Tox will help you to test that your project builds and installs correctly under several environments. You might want to test your code with several versions of Python, and various dependencies, for example.\nYou will write a configuration file named tox.ini:\n\n\ntox.ini\n\n[tox]\nrequires =\n    tox&gt;=4\nenv_list = py{38,39,310,311}\n\n[testenv]\ndescription = run unit tests\ndeps =\n    pytest&gt;=7\ncommands =\n    pytest {posargs:tests}\n\nYou can install tox like so:\n$ pip install tox\nAnd then run tox like so:\n$ tox\npy38: skipped because could not find python interpreter with spec(s): py38\npy38: SKIP ⚠ in 0.01 seconds\npy39: skipped because could not find python interpreter with spec(s): py39\npy39: SKIP ⚠ in 0 seconds\npy310: skipped because could not find python interpreter with spec(s): py310\npy310: SKIP ⚠ in 0 seconds\npy311: commands[0]&gt; pytest\n============================= test session starts ==============================\nplatform linux -- Python 3.11.2, pytest-8.1.1, pluggy-1.4.0\ncachedir: .tox/py311/.pytest_cache\nrootdir: /home/sajith/projects/x-cite/X-CITE/theme1/PE103\ncollected 3 items                                                              \n\ntest_temperature.py .F.                                                  [100%]\n\n=================================== FAILURES ===================================\n_________________ TestCelsiusToFahrenheit.test_conversion_bad __________________\n\nself = &lt;test_temperature.TestCelsiusToFahrenheit testMethod=test_conversion_bad&gt;\n\n    def test_conversion_bad(self):\n        # Test with bad input\n&gt;       self.assertEqual(celsius_to_fahrenheit(\"not temperature\"), 0)\n\ntest_temperature.py:19: \n_ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ _ \n\ncelsius = 'not temperature'\n\n    def celsius_to_fahrenheit(celsius):\n        \"\"\"\n        Convert temperature from Celsius to Fahrenheit.\n    \n        :param celsius (float): Temperature in Celsius\n    \n        :returns: Temperature converted to Fahrenheit\n        \"\"\"\n&gt;       fahrenheit = (celsius * 9 / 5) + 32\nE       TypeError: unsupported operand type(s) for /: 'str' and 'int'\n\ntemperature.py:9: TypeError\n=========================== short test summary info ============================\nFAILED test_temperature.py::TestCelsiusToFahrenheit::test_conversion_bad - TypeError: unsupported operand type(s) for /: 'str' and 'int'\n========================= 1 failed, 2 passed in 0.04s ==========================\npy311: exit 1 (0.29 seconds) /home/sajith/projects/x-cite/X-CITE/theme1/PE103&gt; pytest pid=935973\n  py38: SKIP (0.01 seconds)\n  py39: SKIP (0.00 seconds)\n  py310: SKIP (0.00 seconds)\n  py311: FAIL code 1 (0.32=setup[0.04]+cmd[0.29] seconds)\n  evaluation failed :( (0.41 seconds)"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-01Introduction.html",
    "href": "newsite/theme1/PE100/PE100-01Introduction.html",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "",
    "text": "Welcome to the Programming Essentials - Python Programming and Jupyter Notebooks component of the X-CITE training materials. The intent behind this unit is to show new CHESS users how to write programs in Python for experimental data analysis.\nPrerequisites? None.\nThe training materials before you are designed both for scientists who may not have any programming experience whatsoever and for those who have at least some basic programming capability but in a language other than Python.\nPython’s adoption has exploded in the last decade. Much of its success can be attributed to productivity. Many programming languages force the programmer to deal with very small details to do even the simplest things. Python’s attitude is to just take care of all the minutae so we don’t have to. On top of that, Python’s popularity has resulted in hundreds of thousands of packages of useful code for specific tasks. If there is something you need to write a program for, it’s almost definite that someone else has had the same problem. There’s a good chance at least one of those people neatly wrapped up some of their code and made it available in one of the repositories on the internet. There’s no reason for you to reinvent the proverbial wheel again - take advantage of their work (don’t forget to cite it!) and get back to doing actual science that much sooner.\nYou’re currently looking at PE100-01 Introduction. If you are new to Python and especially if you’re new to programming, you should work through each of the modules in order. More experienced programmers might benefit from skipping directly to topics that interest them. Select one of the following:\n\nPE100-02 Types, Variables, and Operators - the heart of any programming language.\nPE100-03 Decision Structures - conditional statements (“if” statements”) change the program flow.\nPE100-04 Repetition - “while” and “for” loops let us do things over and over.\nPE100-05 Functions - Python comes with a lot of functions, but we can write even more.\nPE100-06 Files - Reading input from and storing your results to disk.\nPE100-07 Exceptions - Dealing with unexpected contingencies.\nPE100-08 Lists - Another kind of variable, and the key to structuring data storage.\nPE100-09 Strings - More details on working with text.\nPE100-10 Dictionaries - Like a simple database, look up information quickly."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-10Dictionaries.html",
    "href": "newsite/theme1/PE100/PE100-10Dictionaries.html",
    "title": "PE100-10: Dictionaries",
    "section": "",
    "text": "Dictionaries are, conceptually, a special type of list. A list has an order to it. Elements are placed into a list in a particular order. Removing and inserting items changes the order in a well understood way. Because of this, yoiu can always access a list by looking into it at a specific integer index. None of this is possible with a dictionary. Instead, dictionaries have a better trick: all access is by searching.\nPython’s dictionaries store each of their elements as a pair of things, a key and a value. The key is the thing that can be searched for, and the value is the information that will be retured when that element is read.\nAn example will help:\n\nfavorite_food = { \"Alice\" : \"Apple Pie\", \\\n                 \"Bob\" : \"Ice Cream\",\\\n                 \"Charlie\" : \"Pizza\" }\nprint(favorite_food[\"Bob\"])\n                 \n\nIce Cream\n\n\nLet’s take that apart and see how it works.\n\nWe have the creation of the dictionary itself. We create a dictionary literal much like we do a list, only here we use curly brackets instead of square brackets.\nWe have key:value pairs. The keys we’re using are all strings (people’s names, in fact) but they could just as well be numbers. The values are also strings, but they can be anything we want. These keys and values are separated by colons (:).\nBetween each of the key:value pairs there is a comma.\nWe see how to look up information in a dictionary. It looks like indexing into a list to extract a particular element, except in this case we don’t give a positional index number but rather we give it a key to look up.\n\nWhen you think about it, looking for Bob’s favorite food by using favorite_food[\"Bob\"] is a pretty powerful tool. Rather than having to specify where to get something from, we can just specify what to get. This is why you’ll occasionally see dictionaries refered to as “Content Addressable Memory”. It’s nice to just get the data we want without having to step through every element of a list looking for it. It’s also faster: Python’s dictionaries use clever indexing so they can straight to what you’re looking for.\nDictionaries get their name, by the way, from real-world physical dictionaries. Suppose you have Webster’s 9th New Collegiate Dictionary in front of you. There are a lot of keys in there - each one of those words in alphabetical order is a key. There’s not really any way to look up something by page number alone - there’s no algorithm to tell me what page the definition for brisance is on. There’s no way to look up a word by knowing which word number it is. If brisance is the 8000th word in the dictionary, that knowledge does me no good. On the other hand, there is an algorithm for finding that definition by looking up the key word. I go to the “B”s, look for the “Br” part, and so forth until I find brisance. I can only look up things by key, not by position.\nOne of the great things about Python is its flexibility with data types. Lists can contain any data type. You can have lists of tuples of lists of strings if you want to. Dictionaries are similarly versatile. You can have dictionaries that contain, say, lists:\n\nfamily_info = { 'ages' : [6,8,36,38],\\\n               'names' : ['Jane', 'John', 'Alice', 'Bob'] }\n\nprint(family_info[\"names\"])\n\n['Jane', 'John', 'Alice', 'Bob']\n\n\nYou can even have dictionaries of dictionaries. They’re quite useful, in fact.\n\nfast_food = { \"McAwful\" : { \"address\" : \"1012 Western Blvd\",\\\n                            \"sanitation\" : 92} ,\\\n            \"Davids\" : { \"address\" : \"201 S Fayetteville St\",\\\n                         \"sanitation\" : 99.5 } }\n\nprint (fast_food[\"Davids\"][\"sanitation\"])\n\n99.5\n\n\nDictionaries give us the beginnings of a database. It’s not as powerful as a “real” database, but it’s good enough for a lot of things. Of course, a dictionary is like any iother variable: it only lasts as long as your program is running. You would have to combine a dictionary with some file access to have any permanent storage.\nWhat happens if we try to look up a key:value pair, and the key isn’t in the dictionary? Let’s see!\n\nprint(fast_food[\"Ruth's Chris\"])\n\n\n---------------------------------------------------------------------------\nKeyError                                  Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 print(fast_food[\"Ruth's Chris\"])\n\nKeyError: \"Ruth's Chris\"\n\n\n\nYeah, we expected that by now, didn’t we? It threw a “KeyError” exception. What should we do about this? We could always wrap the access up in a try/except structure to catch the KeyError, but this is such a common problem that Python gives us a friendlier way to do it: the in operator.\nRemember using in to see if something was in a string? This is philosophically similar. Let’s try it:\n\nif \"Ruth's Chris\" in fast_food:\n    print(\"Surprised to see such an expensive place here!\")\nelse:\n    print(\"that's a relief, actually.\")\n\nthat's a relief, actually.\n\n\nAdding to a dictionary is even easier than adding to a list. All you do is just act like the key was already there and assign it a value:\n\nprint(favorite_food)\nprint()\nfavorite_food[\"Dan\"] = 'Fish'\nprint(\"Now we have:\")\nprint(favorite_food)\n\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Pizza', 'Dan': 'Fish'}\n\nNow we have:\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Pizza', 'Dan': 'Fish'}\n\n\nWhat happens if we try to overwrite some data?\n\nfavorite_food[\"Charlie\"] = \"Soup\"\nprint(favorite_food)\n\n{'Alice': 'Apple Pie', 'Bob': 'Ice Cream', 'Charlie': 'Soup', 'Dan': 'Fish'}\n\n\nWe can change what is stored in the “value” part of the key:value pair any time we want. We can’t change the key, though. At least, we can’t change it directly. We can always delete the existing key:value pair and replace it with a new one. Let’s say Charlie really wants to be known as Chuck. He has his reasons. So let’s fix the favorite_food dictionary:\n\nmunchie = favorite_food[\"Charlie\"]\ndel favorite_food[\"Charlie\"]\nfavorite_food[\"Chuck\"]=munchie\nprint(favorite_food)\n\nWhat we did to accomplish that was 1. Look up Charlie’s favorite food and save that value. 2. use the built-in del operator to remove Charlie as a key and whatever value was associated with him. 3. Insert a new key:value pair whose key is “Chuck” and whose value is whatever we looked up before.\nSince every other Python data type that holds more than one thing can work with the built-in len() function, it stands to reason that dictionaries can, too. And as you would imagine, len() returns the number of entries in the dictionary.\n\nlen(favorite_food)\n\n4\n\n\n\nLooping and Iteration\n\nA dictionary is another type of iterable. This means we can write loops that traverse the entire dictionary, start to finish, and do something useful.\n\nfor key in favorite_food:\n    print(key)\n\nAlice\nBob\nDan\nChuck\n\n\nNotice that a traversal of a dictionary retrieves the keys. If you want to retrieve the values, just use the keys to look up the values.\nfor key in favorite_food: print(favorite_food[key])"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html",
    "href": "newsite/theme1/PE100/PE100-05Functions.html",
    "title": "PE100-05: Functions",
    "section": "",
    "text": "Functions in Python are very, very similar to functions in mathematics. Our functions take one or more input values and transform them into precisely one output. Let’s start with an example.\ndef inductiveReactance(l, f):\n    reactance = 2 * 3.14159 * f * l\n    return reactance\n\nprint (inductiveReactance(1e-3, 2e6))\n\n12566.36\nThe above code defines a function named “inductiveReactance” that accepts two input parameters. The first one, l, is the amount of inductance a coil has in henrys. The second one, f, is the frequency of interest (in hertz). We can call that function with parameters of one millihenry and two megahertz. The function computes the value 12566.36 (the unit is ohms) and returns that to the code that called it. In this case, it was a print statement that called it.\nTake a look at the first line. The very first thing is the keyword def (shortened from the word “define”). After the “def” is where you specify the name of the new function you want to create. In this case, it’s “inductiveReactance”. Next is a list of names of parameters, enclosed in parentheses. In our example, the parameters are “l” and “f”. Finally at the end of the line is a colon.\nOnce that first line is done, the next step is to write the body of the function. Just like an “if”, “while”, or “for” statement, the code block has to be indented consistently. Our example function computes the reactance of the device in question. On the last line of the function, the return statement is how the function sends its computed value back to the code that called the function in the first place. Every function should have at least one return statement. I won’t get drawn into this debate: some say a function should have precisely one return statement and utilize whatever logical means necessary to make sure that all code paths eventually lead to it. Others say it’s not a problem at all for a function to have multiple return statements (and hence multiple ways for a function to end) if it makes the logic more understandable. Personally, I try to minimize the number of return statements in my functions but I’m by no means a zealot on this one. If I need seven different places to exit the function and return a value then so be it."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#encapsulation",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#encapsulation",
    "title": "PE100-05: Functions",
    "section": "Encapsulation",
    "text": "Encapsulation\nFunctions are useful in programming for the same reason they’re useful in math - ours encapsulate a chunk of code so you don’t have to think about what is in it every time. Imagine how tedious it would be to write a program that needed to compute cosine in a lot of different places in the code. You could, I suppose, type in a Taylor series expansion for cosine in each of the places where we need to compute a cosign. That would be irritating, error prone, and confusing to anyone else who has to read it. Instead, we can write a function exactly once to compute cosine and then call that function from many places in our code. Once we have the function tested and debugged, we don’t have to think about it again. That frees up mental energy for more productive uses.\nFunctions can be classified into one of two types. Void Functions exist for encapsulation and don’t actually return a value. print() is an example of a void function. Value-Returning Functions, as the name strongly implies, return a value to the calling code. inductiveReactance() is an example of one.\nHere’s another example. This time, we’ll define a function that calls another function.\n\ndef squared(x):\n    return x ** 2\n\ndef circle_area(radius):\n    area = 3.14159 * squared(radius)\n    return area\n\n\nprint(\"Area of a circle with a radius of 2 is\", circle_area(2))\n\nArea of a circle with a radius of 2 is 12.56636\n\n\nWe defined a function to compute the area of a circle. It needed to square a number and so we decided to write a function to do that. Functions can call other functions ad infinitum. In fact, functions can even call themselves! When that happens the function is said to be recursive. Recursive functions are very useful for solving some hard problems but they’re a little beyond an introductory module like this one."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#function-and-variable-naming",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#function-and-variable-naming",
    "title": "PE100-05: Functions",
    "section": "Function (and Variable) Naming",
    "text": "Function (and Variable) Naming\nWhat kinds of names can we use for functions? The same ones we can use for variables! More specifically, * No keywords (e.g., False is invalid) * No spaces (e.g., my function is invalid) * The first character must be: * a-z, A-Z, or _ (the underscore character) * No numbers (e.g., 1st_function is invalid) * After the first character, the following are allowed: * a-z, A-Z, _, and 0-9 * No other symbols (e.g., get_room&board is invalid)\nAs a widely agreed upon best practice, names should be meaningful and be composed of lowercase characters with underscores as separators."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#function-arguments",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#function-arguments",
    "title": "PE100-05: Functions",
    "section": "Function Arguments",
    "text": "Function Arguments\nInput Parameters to functions are called arguments. They are the primary and best way to put information into a function, and definitely the way that causes the fewest problems. Arguments to a function in Python are mostly analagous to what we’re used to in math, but of course Python has some extensions.\nA function can have any number of arguments, including zero. “A function of zero arguments” might sound like a mathematician’s idea of “humor”, but it can actually make sense in programming. Sometimes you just need to encapsulate part of your code so you don’t have to worry with it again. For instance:\n\ndef say_hi():\n    print(\"==============================\")\n    print(\"==============================\")\n    print(\"Greetings, User. I'll start \")\n    print(\"loading the instrument config\")\n    print(\"files and opening connections\")\n    print(\"to them. It'll take a minute.\")\n    print(\"==============================\")\n    print(\"==============================\")\n\nsay_hi()\n\n==============================\n==============================\nGreetings, User. I'll start \nloading the instrument config\nfiles and opening connections\nto them. It'll take a minute.\n==============================\n==============================\n\n\nNow the code to print that banner is hidden away inside a function we’ll never have to look at again. Less mental clutter means fewer bugs.\nAnd for the sake of completeness, functions can also take one or more arguments:\n\ndef convert_to_miles(kilometers):\n    return kilometers * .6213712\n\ndef interesting_polynomial(a, b, c, d):\n    result = 2*(c**3) + 3.91*(c**2) + 1.1*c + d\n    return result\n\nprint(\"The race was\", convert_to_miles(10), \"miles long and my ankles were hurting the ENTIRE way.\")\n\nprint(\"The polynomial evaluates to:\", interesting_polynomial(7,4,8,1))\n\nThe race was 6.213712 miles long and my ankles were hurting the ENTIRE way.\nThe polynomial evaluates to: 1284.04\n\n\nWhen arguments are passed into a function, they become parameter variables and can be referred to inside the function just like any other variable. This handy because the variables inside a function are called local variables and they have special properties: nothing outside of the function can modify their value, they’re destroyed and re-created every time the function is called, and these local variables supercede any outside variables with the same name.\nTake a look for yourself:\n\ndef show_twice_the_wavelength(wavelength):\n    wavelength = wavelength * 2\n    print(\"Twice the wavelength is\", wavelength)\n\n# a good wavelength to listen for synchrotron radiation\n# emitted from Jupiter (the planet, not the software), in meters\nwavelength = 20\nshow_twice_the_wavelength(wavelength)\nshow_twice_the_wavelength(wavelength)\nshow_twice_the_wavelength(wavelength)\n\n    \n\nTwice the wavelength is 40\nTwice the wavelength is 40\nTwice the wavelength is 40\n\n\nDoes that seem odd to you? What happened is this: four lines from the bottom we created a variable named “wavelength” and set it to 20. We then called the function to print it out doubled. We passed the global variable “wavelength” to our function which took it as its only argument. That argument became a parameter variable that was coincidentally named “wavelength”. That “wavelength” parameter variable has nothing to do with the “wavelength” variable in the main part of the program. Our function doubles that parameter variable and prints it out. At that point, the function completes and the flow of control goes back to the main body.\nThe next time our function is called an entirely new, fresh set of variables and parameter variables is created. This is important - it means that if we call the function with the same value every time then we always get the same result. Functions are unable to save their “state”. Like a football player on a stretcher, they have no memory of what happened before.\n(OK, yes, there are ways for them to save their state. Sometimes it’s unavoidable and you just have to do it, but doing so makes more places for bugs to creep into your programs and makes it harder to understand later. Try to avoid it. We’ll talk about it later.)"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#variable-scope",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#variable-scope",
    "title": "PE100-05: Functions",
    "section": "Variable Scope",
    "text": "Variable Scope\nThe degree to which your programs can “see” a variable is called scope. There are two levels of scope in most Python programming:\n\nGlobal Scope\n\nDefined in main Python file\nOutside of ANY function\nTry to avoid these!\n\nConsidered poor design\nDangerous to use: any part of the program anywhere can change these\nBug Magnet!\n\n\nLocal Scope\n\nVariables defined within a function\nOnly visible and useable from inside their own function!\nUse these if at all possible.\n\n\nThe danger in global variables comes from two things. The first is the fact that the value can be changed anywhere in your program, either in the main program or inside of a function, and it’s devilishly hard to keep track of where that might be.\nThe second danger is more subtle. When a function saves a value into a global variable, the function is now said to have side effects. Side effects break the idea of isolation that functions are meant to give us. Imagine a mathematical function, such as tangent, if it had side effects. Calling tan(.0125) would not only result in the tangent of .0125, but it would have some other effect on some unrelated part of math. Imagine if calling tan caused your coordinate system to change every time? That would be insane.\nIt gets worse, though. What if our tangent function also read from a global variable and changed its behavior based on that. Then each time we called tan(.0125) we might get a different value.\nIn other words, we basically broke math.\nSimilarly, when we write programs, if our functions have side effects then we’ve complicated them tremendously. And more complication means more places for bugs to sneak into our code and they’ll also be harder to find.\nAs an aside, there is a style of programming that eliminates global variables and, to an extent, even local variables. It’s called functional programming, and Python has some support for that style. There is usually more than one way to do anything in Python, and experienced Pythonistas will usually try to choose the most Pythonic way. Part of being in Pythonic style means to use (at least partially) a functional style."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#constants",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#constants",
    "title": "PE100-05: Functions",
    "section": "Constants",
    "text": "Constants\nThere is an exception to the “no globals” rule: Constants. Just like in math, a constant is given a value once and never changed again. “Never changing” means “no side effects” so everything is OK. It is good practice to define your constants using ALL CAPITAL LETTERS.\n\nPLANCK = 6.626e-34\n\ndef photon_energy(freq):\n    gumption = PLANCK*freq\n    return gumption\n\nprint(photon_energy(3e14))\n\n1.9878e-19"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#abstraction",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#abstraction",
    "title": "PE100-05: Functions",
    "section": "Abstraction",
    "text": "Abstraction\nA valuable property of functions is how they isolate the code and variables inside of them from being manipulated elsewhere in your software. A consequence of that is their ability to “hide” detail from us. We’ve already talked about writing a function, debugging it, and never having to look at the code inside of it again. What is every bit as useful, if not more so, is using functions to provide abstraction.\nAbstraction is something we’ve used every day even if we haven’t thought it. Remember learning math? You started off counting things, and yes, that counts as math. If you had four bottle caps in one hand and three in the other, you could toss them all on the table, count them, and know that you have seven in total.\nThere are two problems with having to count everything. One is that the amount of stuff can get big in a hurry. Try using two hands and table to count sand grains. The other problem is that if there are any insights to be had, it’s hard to find them when you’re stuck down in the details. Fortunately, we learned arithmetic.\nArithmetic is great. We don’t have to deal with handfuls of stuff anymore. We can just use numbers and operators and get an answer without a bunch of messing around. We can start to see patterns we never would have just tossing bottle caps on the table. If we need to add 12 to something, we can instead add 10 and then add 2 more. This is so handy. Of course, it would be nice if we could just do something to analyze entire families of arithmetic problems.\nAlgebra lets us analyze entire families of arithmetic problems. We don’t have to fool with numbers if we don’t have to - we can just substitute variables in their place. We’ve hidden some of the complexity, like the petty little details of numbers, and abstracted that complexity away.\nSimilarly, a lot of problem solving is perfectly amenable to using abstraction. Let’s write a bit of code to run an experiment…\n\ndef run_experiment():\n    safe=is_it_safe_to_run()\n    if safe == True:\n        light_em_up()\n        put_facility_back_in_a_safe_state()\n        print(\"Better than Ghostbusters, huh?\")\n        \n\nThat function is a (admittedly fanciful) representation of running an experiment. It makes sense, anyone can understand it, and if there’s a bug in there then it’s going to be really obvious. The only problem: if we try to run it, it’ll crash because those other functions haven’t been defined yet. Shall we fix that?\n\ndef is_it_safe_to_run():\n    if badges_swiped_in() == 0 and shutter_closed() and not red_light_illuminated():\n        return True\n    else:\n        return False\n\ndef shutter_closed():\n    # put some code to interface with the solenoid sense switch here\n    return True;\n\ndef red_light_illuminated():\n    # more code, this time to see if the light is on...\n    return False\n\ndef badges_swiped_in():\n    # do some database lookups or something to see if we\n    # think anyone is still in the room.\n    return 0;\n\n\ndef light_em_up():\n    turn_on_red_light()\n    high_voltage(True)\n    shutter_open()\n    # a few seconds delay here, perhaps?\n\ndef put_facility_back_in_safe_state():\n    shutter_closed()\n    high_voltage(False)\n    turn_off_red_light()\n\nNotice how the program is broken up into several functions? The best part is that you don’t have to keep everything in your head. All you have to remember is the part you’re working on. Smaller pieces, fewer bugs."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-05Functions.html#modules",
    "href": "newsite/theme1/PE100/PE100-05Functions.html#modules",
    "title": "PE100-05: Functions",
    "section": "Modules",
    "text": "Modules\nOne reason Python has become so popular is the sheer amount of code that has been written in it and made available for public use. We’ve seen a few functions already that were built in to Python - int(), float(), and str(), for example - but there are many tens of thousands of modules that are freely available for use in your own software. Just picking five common ones at random:\n\nmath\nrandom\nos\nPyMySql\npsycopg\n\nThe first two contain functions for general-purpose math and for producing random numbers. The “os” module interfaces Python with the operating system the code is running on. PyMySql and psycopg provide connectivity to relational databases.\nRemember at the beginning of this lesson when we wrote a function to calculate inductive reactance? I put the value of pi in there as 3.14159, but that really isn’t anywhere near enough digits for some problems. Let’s fix that:\n\nimport math\nprint(math.pi)\n\n3.141592653589793\n\n\nThere are two things to note here. First, the keyword import is used to tell Python to go find a module with the right name and load it. The name we want it to find is the word right after the import. And secondly, just looking at the output we can see that there are a lot more digits than when we did something by hand in our Inductive Impedance example (top of this page). In general, using a module that was (a) written by someone else and (b) is widely used and has been checked by a lot of people is going to avoid a lot of bugs. For instance, I would never code my own Fast Fourier Transform. Instead, I would use the one in the “numpy” module. I know how easy it is to make a mistake and I trust their work a lot more than my own. They have tens or hundreds of thousands of users and scores of developers. I have… a copy of Numerical Recipes that’s old enough to run for President.\nSince we used the “math” module already, here’s a very incomplete list of what is in there: * sin(), cos(), tan(), acos(), asin(), atan()… - “acos” is “arc cosine”, etc. * log(), log10(), sqrt() - square root * radians(), degrees() - converts between them\nAnd lots more stuff. How do you know what’s in it? Go to the online documentation: https://docs.python.org/3/library/math.html\n\nRandom Numbers\nAnother module that is heavily used is “random”. It generates random numbers, yes, but it can also do things like take a list of things and shuffle them randomly.\n\nimport random\ninteger_number = random.randint(10,100)\nreal_number = random.random()\n\nprint(\"The random integer between 10 and 100 was:\", integer_number)\nprint(\"The random float between 0 and 1 is:\", real_number)\n\nThe random integer between 10 and 100 was: 20\nThe random float between 0 and 1 is: 0.6474502367565016\n\n\nThere are more functions available in the “random” module, including ones to select a real number from a non-uniform distribution. Take a look at https://docs.python.org/3/library/random.html\nHere’s a slightly more complicated example:\n\n# for 25 hypothetical proposals, use random() to decide\n# if it gets funded.\n\nfor proposal in range(0,25):\n    if random.random() &lt; 0.12:\n        print(\"Proposal number\", proposal,\"was funded!\")\n    else:\n        print(\"Don't feel bad... proposal number\", researcher, \"didn't get funded either.\")\n\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nProposal number 23 was funded!\nDon't feel bad... proposal number 24 didn't get funded either.\n\n\n\nLet’s try out what we’ve learned so far. Use the next code cell to write a bit of Python that simulates rolling a pair of dice and adds the two values. Print the value out.\nLet’s add to that… add a loop so that we keep doing that over and over until we get the same sum twice in a row. Some questions to ask yourself are “What kind of loop do I need?” and “How can I compare what happened between two different loop iterations?”"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-06Files.html",
    "href": "newsite/theme1/PE100/PE100-06Files.html",
    "title": "PE100-05: Files",
    "section": "",
    "text": "You can write a good deal of software that runs entirely inside Jupyter notebooks or that runs on the command line and only communicates through the screen and the keyboard. Sometimes, though, you have to do with files. It may not be practical to hardcode all your data into assignment statements in Python, or maybe you have to deal with a number of files and therefore you can’t use pipes. link to Sys Fundamentals page here\nWe’ve already seen two basic ways to do Input and Output (often referred to as “I/O”). We’ve used input() to read from the keyboard and print() to send output to the screen. Those functions work quite well, except you might have to do a lot of typing or deal with your output scrolling off the screen. In neither case is the data durable - it goes away as soon as the program is done or you close the Jupyter notebook.\nThe input() and print() functions are just the tip of the proverbial iceberg in terms of getting information in and out of running Python code. Some of our other options include: * GUI controls: text box, menu, dialog box… * Networks: HTTP, TCP/IP sockets, Infiniband… * Databases: Relational (SQL) and NoSQL * Other: cameras, microphones, speakers, LabView"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-06Files.html#files",
    "href": "newsite/theme1/PE100/PE100-06Files.html#files",
    "title": "PE100-05: Files",
    "section": "Files",
    "text": "Files\nPractically everyone is more-or-less familiar with the idea of a file, even if fairly few people know how they work. We’re going to ignore a lot of details for the moment and say this: a file is a long-lasting collection of bytes. It has a first byte, a last byte, and every one in between stays in the same order.\nThis begs the question “What is a byte?” A byte is just a small number from 0 to 255 (inclusive). We can assign meaning to those numbers, and if we’re smart about how we do it then we can represent any information a computer can process as long as we use enough of these bytes.\nWe like to think of files as being one of two types: binary files and text files. Binary files are pure data. We decide how to write bytes to a file to represent data. Then when we’re ready to read it in again, we read the bytes, process them somehow, and reconstruct the original data. It’s a great technique - it’s fast and efficient.\nWe won’t be talking about binary files in this notebook or even in this module. Fifteen years ago we wouldn’t have had a choice, we would have had to. These days, it’s unusual to have to deal with binary files, especially in Python, because there is so often a library function already available to do the work for us.\nText files, on the other hand, are probably something you’re already familar with - they are what you get when you edit a “plain text” file in “notepad” or “textedit”. In a text file, every one of the letter, number, and punctuation mark characters is assigned its own number. For instance, capital “A” is 65. “B” is 66. Not that it should ever matter, but here’s a complete list and then some!\nLet’s say you open an editor and type “CAT”. When you save that to a file, there will be a file that is three bytes long and contains the three bytes 67, 65, and 84. Actually there will usually be a fourth byte, 10, which is the character you get when you press “Enter” or “Return”.\nFor now, at least for a few minutes, we’re going to pretend the only language on earth is English. We’ll talk about other languages when we talk about networks.\nIt’s about time for an example, don’t you think?\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First Post!\")\nmy_file_object.close()\n\nThree lines of code was all it took to create a file, write to it, and tidy up after ourselves. What does each of those lines do?\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object is an object variable. Think of an object as a way to store data in a variable along with some functions that only make sense to that data. They hide a lot of complexity from us. A file object is one that keeps track of a filename, how to get to it, and how to use it. It has some functions built in to it to help us do things to the file.\nPython gives us the function “open”. It gets a file ready to be used by our code. It takes two arguments. The first is the file’s name, and the second is the mode we want to use the file in. In our example, we specified that the file’s name was “first_file.txt” and that it was in the “/tmp” directory. Then in the second argument we specified “w”, meaning we wanted to write to the file. The “w” mode will cause the file to be created if it didn’t already exist. If it did already exist, on the other hand, all the contents of it will be deleted and we’ll start writing from the beginning just as if the file was created from scratch. We’ll see more modes as we go.\nmy_file_object.write(\"First Post!\")\nThis line uses one of those functions that are tucked away inside an object. In this case, we’re calling the file object’s “write” function. It does what we expect - it takes its argument, in this case “First Post!”, and causes it to be written to disk byte by byte.\nmy_file_object.close()\nFinally, we call one more of the file object’s functions: close. When we run this, Python tells the operating system “Hey, we’re done with the file. You can get rid of any of the tedious housekeeping data that operating systems keep behind the scenes!”\nClosing files is considered “good programming hygene”. You’re allowed 1024 file objects to be open and connected to files in one program on the CLASSE cluster of computers. I’ll say from my experience: if you think you need that many, you’re probably doing something the wrong way.\nWriting files, then, is fairly easy. What about reading files? I’m glad you asked.\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n'First Post!'\n\n\nYou can probably tell mostly how that worked just by looking at. We used the open() function again, but this time with a “r” for our mode. This means “read”. Also, this time we used read() instead of write(). The read() function reads in an entire file and saves it a string variable. Finally, we call close() again to close the file and tidy up after ourselves.\nNote that if the file is, say, 500 megabytes long, the string variable is going to be very, very large - roughly half a gigabyte. Python can handle this, but it may not be terribly convenient. If the file is more than 100-200 gigabytes, the CLASSE servers are probably not going to be able to handle. I say “probably” because there are a lot of factors at play.\nJust writing one line to a file is probably not very useful. Let’s try writing two lines:\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First line written.\")\nmy_file_object.write(\"This is my second line.\")\nmy_file_object.close()\n\nWhen we run that, it will open /tmp/first_file.txt for writing and it will delete anything already in it (that’s what the “w” means, remember?). Then it will write “First line written.” and “This is my second line.”.\nLet’s read the file again and prove to ourselves that it worked…\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n'First line written.This is my second line.'\n\n\nOh no! The two lines ran together!\nAnd that is one of the first differences we’ll see between write() and print(). Print() always adds a newline character after it prints out anything. Remember when I said there would usually be a byte at the end of a line, represented by the number 10? This character is called “newline” and it, as the name implies, marks where a new line starts.\nIn all likelihood, when we do two write() statements like we did, we want to put a newline character in the file to make it into two lines. Fortunately, there are several ways to do that. Here are two of them.\nThe first way is simple and direct - call write() three times instead of two and put a newline in there “by hand”, as it were:\n\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object.write(\"First line written.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"This is my second line.\")\nmy_file_object.close()\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nthe_contents = input_file.read()\ninput_file.close()\n\nthe_contents\n\n\n\n'First line written.\\nThis is my second line.'\n\n\nThe output looks a little strange. We put an extra write() function call, but we gave it an odd looking argument - . That is a backslash (usually between the Enter and the backspace keys on a US keyboard) immediately followed by a lowercase “n”. The combination together means “newline character”. This much is fairly straightforward.\nNext we read the contents of the file. This is just like before.\nFinally, and this is where things take an unexpected turn, we evaluate the_contents and let Jupyter print that out for us. And when Jupyter does that, we see the “” there. It seems like Python didn’t convert those two characters to a newline, just sticking them in there as-is, and still left us with one long line. But is that true? Has Python foresaken us?\nRun the code in the next cell:\n\nprint(the_contents)\n\nFirst line written.\nThis is my second line.\n\n\nSalvation! print() did the right thing. This is a key difference between just typing a variable or an expression at the end of a cell and letting Python evaluate it versus putting a print() in there and having absolute control over what gets sent to the notebook and on to the screen.\nThis also illustrates something else important and useful: all of the code cells in this notebook are being run by the same Python “interpreter”. This means if we set a variable to a value in one cell, we will see the same value stored in that variable in other cells. That’s how we were able to print what was stored in the_contents in the cell above even though we had set its value to the file contents two cells above that.\nIf a file only has a line or two, it’s not a big deal dealing with that with string functions. If a file has millions of lines, then it becomes a bit of a hassle. We need a way to read a file one line at a time. Fortunately, there’s readline():\n\ninput_file = open(\"/tmp/first_file.txt\",\"r\")\nline_one = input_file.readline()\nline_two = input_file.readline()\ninput_file.close()\n\nprint(line_one)\nprint(line_two)\n\nFirst line written.\n\nThis is my second line.\n\n\nThis does almost what we expect: it reads both lines from the file, one at a time, and prints them out. The only snag is that blank space between the lines. What has happened? It turns out readline() reads the entire line, even the newline character at the end. We can see this if we evaluate the string instead of just printing it:\n\nline_one\n\n'First line written.\\n'\n\n\nThere’s that \\n again! What about the second line?\n\nline_two\n\n'This is my second line.'\n\n\nWhen readline() reads a line, it includes the newline character at the end unless it reaches the end of the file and the file didn’t end with a newline.\nIt’s rare that we would want to read a bunch of lines in a file with the newlines included. That’s just not something we do very often, and practically never in scientific software. We’ll almost always want to trim off the newline character. And for that, we have the rstrip() function. It takes a string, strips off any newlines on the right side of it, and returns that cleaned-up string. rstrip() does that for the right side of the string, lstrip() cleans up the left side (the beginning of the string) and strip() goes crazy and does both ends at the same time.\nLet’s try it:\n\nclean_first_line = line_one.rstrip('\\n')\nclean_second_line = line_two.rstrip('\\n')\n\nprint(clean_first_line)\nprint(clean_second_line)\n\nFirst line written.\nThis is my second line.\n\n\nWhat’s going on here? A couple of things. The first thing to note is that rstrip() and its close companions lstrip() and strip() take one argument, which is the character to be stripped. Practically always we’ll want to get rid of the trailing newline character.\nThe other interesting things is how we called the rstrip() function in the first place. We gave the name of the string variable, a period, and the name of the function we were calling. This is just like how we called the close() function on a file object. And in fact, strings are another kind of object in Python. We’ll see a lot more on this later.\nHistorical note: The original programming language that had objects was named “Smalltalk”. In Smalltalk, the functions that were inside of objects were called “methods”. You’ll still hear people call them that. Later, the “C++” language came along and it called methods “member functions”. When programmers talk about the functions that are contained in objects, we’ll use either term interchangably, sometimes even switching in the middle of a sentence. We now return to your Python tutorial, already in progress…\nWe read both lines in the file we created. We were able to call readline() twice and know that we had all of our lines in the file because (1) we created the file ourselves and (2) we therefore knew it had precisely two lines. It wasn’t even too bad having to type those readline() and rstrip() lines twice. But what if we had a lot more lines? We would certainly want to use a loop.\nFor example, what do we do with a five-line file?\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nmy_file_object.write(\"Line 1.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 2.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 3.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 4.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 5.\")\nmy_file_object.write('\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor i in range(5):\n    input_line = input_file.readline()\n    print(input_line.rstrip('\\n'))\n\ninput_file.close()\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nNo problem - we just use a for loop and do the readline() inside of it. It repeats the five times we asked for. In this case, after we read each line we cleaned it up a little and printed it.\nBut what if we can’t know the number of lines ahead of time? One approach is to have whatever program that creates the file write the number of lines that will be in it first. I won’t say this is a common approach in scientific software, but it isn’t exactly rare either.\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nmy_file_object.write(\"5\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 1.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 2.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 3.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 4.\")\nmy_file_object.write('\\n')\nmy_file_object.write(\"Line 5.\")\nmy_file_object.write('\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nfirst_line = input_file.readline()\nhow_many_lines = int(first_line.rstrip('\\n'))\n\nfor i in range(how_many_lines):\n    input_line = input_file.readline()\n    print(input_line.rstrip('\\n'))\n\ninput_file.close()\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe overall scheme for this is probably obvious by now. In the first half, when we’re writing the file, we write a “5” on its own line, and then write five more lines. In the second part, we 1. Read the first line. 2. rstrip() to get rid of the trailing newline 3. Use the results of that as the argument to int(), converting that string (“5”) to an actual integer (5). 4. and finally go through a for loop that many times just like before\nMost of the time we won’t have the luxury of knowing how many lines are in a file, though. We need a way to read all of the lines, line by line, without limit. For that, we can loop through the file and quit when Python returns an empty string with not even a newline character.\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nline = input_file.readline()\nwhile line != '':\n    print(line.rstrip('\\n'))\n    line = input_file.readline()\ninput_file.close()\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe while loop behaved just like we expected - strat by reading a line, and then every time the line isn’t empty, print it out and read another line. When you finally hit a line that is completely empty, exit the while loop and close the file.\nLooping through a file all the way to the end is such a common thing to do, Python has a shortcut for doing it. Remember when we talked about a for loop iterating over an ordered set? A file can be thought of as an ordered set of strings. They’re not in alphabetical order, but rather they are ordered by line number. That means we can:\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\n\nfor line in input_file:\n    print(line.rstrip('\\n'))\n\ninput_file.close()\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nAs you can imagine, reading isn’t the only file operation you can do with a loop. You can also write to a file that way. For instance,\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"w\")\nfor i in range(7):\n    output_string = str(i)\n    my_file_object.write(output_string + '\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor line in input_file:\n    print(line.rstrip('\\n'))\ninput_file.close()\n\n0\n1\n2\n3\n4\n5\n6\n\n\nFinally, we don’t have to erase the contents of a file every time we write to it. It’s perfectly normal to append to an existing file, and for that the “a” mode can be used with open().\n\nmy_file_object = open(\"/tmp/five-liner.txt\", \"a\")\nfor i in range(7,10):\n    output_string = str(i)\n    my_file_object.write(output_string + '\\n')\nmy_file_object.close()\n\ninput_file = open(\"/tmp/five-liner.txt\",\"r\")\nfor line in input_file:\n    print(line.rstrip('\\n'))\ninput_file.close()\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWhen you use the append mode, the write() calls will either add to the existing file or, if it doesn’t already exist, it will be created and then written to as though we used the “w” mode.\nSo far in this lesson we’ve acted like everything just works perfectly every time. In reality, it’s not that neat. Filenames get typed in wrong, didks get full, and lines that are supposed to be numbers might contain text instead. Any of these problems is enough to bring our Python code to a grinding halt. Our next lesson is all about how to handle these problems and many, many more like them. We’re going to learn about Exceptions!"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "",
    "text": "Niklaus Wirth was one of the founding giants of Computer Science. He wrote an introductory textbook whose title neatly summed up the act and art of programming: Algorithms + Data Structures = Programs. Data Structures are how information is stored in a computer, and algorithms are the instructions the computer applies to transform that data.\nTo run the code in a cell, first click in the cell to select it. Then you can either: 1. Go to the “Run” menu and choose “Run Selected Cells”, or 1. Just press Shift + Enter.\nLet’s do this now: click just below where it says “print (403.616”), then Go to the “Run” menu and choose “Run Selected Cells”.\nprint (403.616)\n\n403.616\nWhen it ran, it printed “403.616” on a line by itself. That was the output from the print.\nClick in the next cell (where it says “103.5”) to select that cell. Then hold down the Shift key while you press Enter.\n103.5\nIn Python, and in Jupyter notebooks, if the last (or only!) line evaluates to some value then it will be printed out. That’s how “103.5” got printed - a literal number evaluates to that number when it’s run. A “literal number” means you look at it in your code and you literally see a number.\nTake a look at a string literal. Run each of the next two cells…\nprint (\"the quick brown fox\")\nprint ('jumped over the lazy dogs')\nAt this point, we can use Python and Jupyter Lab as a scientific calculator. We have some literals of different types (int, real, and string, so far) and we can print them out with the print() function. If we don’t explicitly print anything at the end of a cell, Python will show us the last value that was computed."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#operators",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#operators",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Operators",
    "text": "Operators\nLike any programming language, Python lets you “do math” and lots of other things. Let’s take a look at some of the basic “operators”. In all of the code-containing cells through this course, try to predict what will happen first, and then run the code.\n\n2+2\n\n\n2*8\n\n\n6-4\n\n2\n\n\n\n7*6\n\n\n16/3\n\nBesides the “classic” operators, there are some handy extras:\n\n16//3\n\nWhat happened there? The // operator does integer division - it returns the whole number part of the answer, just like when we learned division in elementary school.\n\n16%3\n\nThe % operator returns the remainder. This is also called “modulo”, and the above would be pronounced “sixteen mod 3”.\n\n2**8\n\n256\n\n\n\n4**2.718281828459045\n\nThe ** operator does exponentiation. The arguments can be integers or they can be real numbers. Naturally, operators can be combined into arbitrarily long expressions.\n\n3*4*5\n\n\n6+4*5\n\nNotice what happens when we use different operators. They are applied in the “My Dear Aunt Sally” order of precendence (multiplication, division, addition, subtraction).\nOrder of operations: * Exponentiation: ** * Multiplication, Division, Remainder: * / // % * Addition and Subtraction: + -\nWithin the same level, operators are applied left-to-right. 8-5+2 is evaluated as 3+2 and yields 5. The exception is exponentiation: 2 ** 3 ** 4 is treated as 2 ** 81 and yeilds an annoyingly large number\n\nprint (2**3**4)\n\n\nprint (2**81)"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#variables",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#variables",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Variables",
    "text": "Variables\nUnless we just use Jupyter as a big, expensive scientific calculator, we need a way to store data. Variables were invented for just that purpose, and virtually every language has them. Think of them as a place to store data of some kind, and that place has a name. They behave in Python just like you’d expect.\n\nanswer = 42\nprint(answer)\n\n42\n\n\nWe just created a variable named answer and gave it the value 42. Variables are long-lived - later we’ll talk about just how long when we start writing our own functions, but until then our variables last as long as Python (or in our case, Jupyter) is running. Take a look - answer is still there.\n\nprint(answer)\n\nThe value stored in a variable can change. It can even change type:\n\nweight = 60\nweight = 70\nprint(weight)\nweight = \"not very much.\"\nprint(weight)\n\nWe can declare many variables, and we can “do things” with them just like we can when we type in numbers or strings.\n\nvolts = 120\namps = 4\nwatts = volts * amps\nwatts\n\nIn the last line, we just put watts because Jupyter automatically prints what the last line evaluates to.\nWe can use variables to change the order of operations. Let’s see the average price of two people’s meals:\n\ntotal = 22.41 + 19.45\naverage = total / 2\naverage\n\nThat’s the right answer. If we hadn’t done that, we would have gotten\n\n22.41 + 19.45 / 2\n\nwhich is utterly wrong. Beware of the order of operations… it is a frequent source of bugs in scientific programming.\n\nVariable Naming Rules\nFor the most part, you can pick whatever name makes sense for a variable, but there are some rules. When choosing a name: 1. No keywords (False won’t work.) 1. No spaces (sample thickness is invalid) 1. The first character must be one of * a-z, or A-Z, or _. (the underscore character) * As a result, no numbers (3rd_sample_holder is invalid) 1. After the first character, you can then have numbers (sample_holder_3 is perfectly valid) 1. No other symbols are allowed (exploded&destroyed_spectrometers is invalid, and probably suggests it’s time to review lab safety procedures).\nNote: Uppercase vs. Lowercase matters! Bevatron is not the same variable as bevatron"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#types",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#types",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Types",
    "text": "Types\nWe’ve hinted that variables have a “type”, and that the type can change if it needs to. The way it works is that variables keep track of what type they are (integer, real number, or string) and what their “value” is. We can even interrogate a variable as see what type it is:\n\nreading=7.2\nprint(\"reading:\")\nprint(type(reading))\nreading=\"rainbow\" # changes type of reading to string\nprint(type(reading))\n\nreading:\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\n\nmy_number = 42\nmy_string = \"was that really the right answer?\"\n\nfirst_type = type(my_number)\nsecond_type = type(my_string)\nprint(first_type, second_type)\n\nThe type of a variable matters. Let’s create a variable with an integer in it and another with a string. Then let’s do some math:\n\nfirst_thing = 6\nsecond_thing = \"7\"\n\nprint (first_thing + second_thing)\n\nHow do we handle situations like that, where second_thing held a string representing a seven, but because it was a string variable it couldn’t be used as an integer? Python provides a few functions to convert values from one type to another. The str() function takes a variable and converts it to a string. The float() and int() functions convert their arguments to floating-point and to integer numbers, respectively.\n\nprint(first_thing + int(second_thing))\nprint(first_thing + float(second_thing))\n\n13\n13.0\n\n\nBeing able to convert values from one type to another is often called type coercion. These conversions are extremely important for situation where you need to get input from a user, even more so if you need to do it repetitively."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#continuation-character",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#continuation-character",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Continuation Character",
    "text": "Continuation Character\nSometimes the expressions we need to evaluate can be very long. It would be nice if we could split up a long expression and spread it out over a few lines. As a small example, we’ll take a look at 4+2+3. Many programming languages will let us split an expression anywhere we want, such as:\n\n4+2\n+3\n\n…but that result isn’t right in Python. The last line, +3, was evaluated and printed as the result of running that cell. In Python,it turns out, if we need to continue an expression on the next line we must end the current line with a backslash \\ and press enter. It has to be a backslash, by the way, and cannot be the forward slash like we use for division.\n\n4+2\\\n+3\n\n9\n\n\nTime for an exercise! Try to predict what will be printed when you run the next cell. Then, run the next cell and see how you did. If you miss one, make sure you figure out what happened before you go. I know, we’re professionals, I shouldn’t have to say that…\n\nprint(1 + 3 + 5 * 4 / 2)\nprint(7 % 2 * 10)\nbig_num = 1 + 2 + 3 + 4 \\\n     + 5 + 6\nprint(big_num)\n\nNow write an expression to average three numbers (12, 14, and 66), divide the result by three, and square it. You can use the code cell right below here:"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#the-string-type",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#the-string-type",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "The String Type",
    "text": "The String Type\nAt the beginning of this notebook, we casually mentioned “strings” without saying what they are. They’re just “sequences of characters”. And these can be any kind of characters - the English alphabet, the Hungarian alphabet, hiragana… it doesn’t matter.\n\nprint(\"I'll see you mañana, assuming I don't get irradiated to death.\")\n\nSome, probably most, languages contain strings inside “double quotes”, \", which is shift+apostrophe on US English keyboards. Other languages (SQL and Pascal are the only two I can think of) use single quotes: '. Python lets you use either one. You do have to be consistent in each string, but it can vary from one string to the next:\n\nprint(\"double quotes work\")\nprint('single quotes also work')\nprint('but do not try to mix the two in one string!\"\n\nBecause we can use either type of quotation mark, we can exploit that to let us put quotation marks into strings:\n\nprint(\"Don't put explosive mixtures in the spectrometer, please.\")\nprint('Of course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.')\n\nDon't put explosive mixtures in the spectrometer, please.\nOf course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.\n\n\nThat lets us embed whichever kind of quotation mark we need into a string.\nBut what if we need to embed both kinds of quotes into one string? We’re in luck: we can use the backslash character again to “quote” our quotation mark. In fact, we can quote any character with it if we need to.\n\nprint(\"We told him \\\"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\\\", but we're pretty sure he ignored us.\")\n\nWe told him \"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\", but we're pretty sure he ignored us.\n\n\nThat sentence contains three things, inside the string itself: 1. Double Quotes to surround a direct quotation 2. A single quote, also called an apostrophe depending on how it’s used, to make a contraction, and 3. A totally awesome/terrifying molecule you have to google to believe.\nOK, I’ll save you the trouble. Prepare to lose most of a day’s productivity. You’re welcome.\n(Derek has written gobs of articles on fun substances. Here are some more. )\nThere is one last kind of string literal. Sometimes you need a string that is several lines long. The “triple quote” is a way to do it. You have to use three double-quotes in a row:\n\ngigantic = \"\"\"This is the first line,\nThis is the second,\nand this is the third and final line of my string.\"\"\"\nprint(gigantic)\n\nTriple quotes are also an easier way to embed mixed kinds of quotation marks into strings:\n\nmovie_opinion = \"\"\"I know people who say \"The Avengers\" isn’t a good movie, but I don’t agree.\"\"\"\nprint(movie_opinion)\n\nI know people who say \"The Avengers\" isn’t a good movie, but I don’t agree."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#coming-up-next",
    "href": "newsite/theme1/PE100/PE100-02TypesVarsAndOperators.html#coming-up-next",
    "title": "PE100-02: Types, Variables, and Operators",
    "section": "Coming Up Next",
    "text": "Coming Up Next\nWe just looked at enough of Python and Jupyter notebooks to use it as a basic calculator, but so far we can’t do any real, general-purpose programming with it. The “flow of control” sob far as been a straight line from top to bottom and we can’t change what we’re doing in response to different inputs. That’s about to change. In the next section we’ll look at the if statement and how to use it."
  },
  {
    "objectID": "newsite/theme1/PE102/PE102-02NumPy.html",
    "href": "newsite/theme1/PE102/PE102-02NumPy.html",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "In the previous section, modules for Python was introduced. In this section, we’ll take a much more detailed look at one of the most useful to scientists: NumPy. This module contains numerous routinues and support frameworks for numerical computing. The routinues in it are very carefully tested for correctness and are crafted for speed. Any time you can use something from this package, it’s a good idea to.\nPython is built for versatility and ease of programming. Unfortunately, it is not built for speed. Over the years Python has gotten faster and faster but there is still a speed penalty compared to classic compiled languages like C, C++, or Fortran.\nEnter NumPy: a package of mathematical routines written in C and Fortran and made to work with Python via a “glue” or “shim” layer. This interface is invisible to the programmer. NumPy looks and behaves just like any other Python package. Under the surface, though, lies a very fast and efficient library of algorithms."
  },
  {
    "objectID": "newsite/theme1/PE102/PE102-02NumPy.html#a-first-glimpse",
    "href": "newsite/theme1/PE102/PE102-02NumPy.html#a-first-glimpse",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "A first glimpse",
    "text": "A first glimpse\nLet’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\n\nimport numpy as np\n\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\na = np.array([[1,2,3,4], [5,6,7,8], [9,10,11,12]])\n\na\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\nz = np.zeros(3)\nz\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\nm = np.ones((3,3))\nm\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\nbig_m = np.ones((3,3,3,3))\nbig_m\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\nprint(\"z:\")\nprint(np.shape(z))\nprint()\nprint(\"m:\")\nprint(np.shape(m))\nprint()\nprint(\"big_m\")\nprint(np.shape(big_m))\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)"
  },
  {
    "objectID": "newsite/theme1/PE102/PE102-02NumPy.html#lets-do-some-actual-math-shall-we",
    "href": "newsite/theme1/PE102/PE102-02NumPy.html#lets-do-some-actual-math-shall-we",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "Let’s do some actual math, shall we?",
    "text": "Let’s do some actual math, shall we?\nThe trivial example: add a scalar (“a single number”) to every element of the matrix:\n\nprint(z)\nz_plus_three = z + 3\nprint(z_plus_three)\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\nprint (a % 2)\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\nprint(a)\nprint()\nprint(a &gt; 5)\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]"
  },
  {
    "objectID": "newsite/theme1/PE102/PE102-02NumPy.html#linear-algebra-anyone",
    "href": "newsite/theme1/PE102/PE102-02NumPy.html#linear-algebra-anyone",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "Linear algebra, anyone?",
    "text": "Linear algebra, anyone?\nLet’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\n\nimport numpy.linalg as nl\n\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\nk = np.array([[1,1,1], [1,1,0], [1,0,0]])\nprint(k)\n\nkinv = nl.inv(k)\nkinv\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\nk @ kinv\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "newsite/theme3/DC100/distributed-computing.html",
    "href": "newsite/theme3/DC100/distributed-computing.html",
    "title": "Distributed Computing Concepts",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThese notes give you a brief overview of concepts of distributed computing. If you are at this chapter, it is assumed that you are familiar with writing scientific code for your analysis and are able to run it locally on your machine (laptop/desktop).\nFor purposes of this discussion, it is important to review some terminology."
  },
  {
    "objectID": "newsite/theme3/DC100/distributed-computing.html#serial-computing",
    "href": "newsite/theme3/DC100/distributed-computing.html#serial-computing",
    "title": "Distributed Computing Concepts",
    "section": "Serial Computing",
    "text": "Serial Computing\nSerial computing can be defined as executing instructions in a sequential order as laid out in your program. The order in which the instructions are executed is important and critical for correctness.\nIn serial computing your program runs on a single CPU/processor and speed at which your programs runs is directly proportional to the CPU’s processing power which is usually measured in Clock cycles per second (Hz). A 1 Hz CPU does 1 clock cycles per second. You can consider for discussion purposes\n\n1 instruction takes 1 cycle to to get executed (not always true)\nThe clock speed defines how fast your CPU can execute instructions.\nSo a 1Gz CPU can execute 1 billion instructions per second!\n\n\n\n\nSerial Computing"
  },
  {
    "objectID": "newsite/theme3/DC100/distributed-computing.html#parallel-computing",
    "href": "newsite/theme3/DC100/distributed-computing.html#parallel-computing",
    "title": "Distributed Computing Concepts",
    "section": "Parallel Computing",
    "text": "Parallel Computing\nIn parallel computing you can leverage multiple CPU’s or cores to execute your program. It involves breaking down a problem into smaller, independent sub-tasks that can be processed simultaneously by multiple processors or cores. Remember modern day processors/CPU’s are now comprised of multiple cores which can act independently, allowing use to execute instructions in parallel.\n\n\n\nParallel Computing\n\n\nParallel computing can often be achieved by either leveraging\n\nThreads: Threads are light-weight sub-processes, that have access to your program global memory. Threads allow you to leverage the multiple cores on a processor/CPU. Threading needs be done carefully, as data is shared across threads and a thread can easliy modify data that another thread maybe using resulting in crashes or deadlocks.\nMPI: Message Passing Interface(MPI) allows independent processes to access a shared global memory via a message interface. This allows processes to control\nwhat data is shared and which is private to the process. Using MPI you can either use cores on a single CPU, or also use cores across multiple CPU’s in a high performance computing cluster, where the CPU’s are connected via a high speed network such as infiniband.\n\nEven though you can acheive serious performance gains using parallel computing, it is generally considered hard and error prone. Various reasons contribute to it such as harder debugging, memory coherence issues etc. Also as humans, we are better at thinking sequentially."
  },
  {
    "objectID": "newsite/theme3/DC100/distributed-computing.html#distributed-computing",
    "href": "newsite/theme3/DC100/distributed-computing.html#distributed-computing",
    "title": "Distributed Computing Concepts",
    "section": "Distributed Computing",
    "text": "Distributed Computing\nDistributed computing involves leveraging multiple computers that are connected via a local area network or a wide area network to solve a problem that you cannot solve on a local computer. For example, if you do a google search the search request gets broken down and distributed amongst various nodes to get you the results back.\nParallel and Distributed computing are often changed interchangeably. However, there are subtle differences between the two. For example, in distributed computing there is no shared memory to use. If anything, Distributed Computing should be viewed as the superset (that could include Parallel Computing).\n\n\n\nParallel vs Distributed Computing\n\n\n\nHigh Performance Computing\nHigh Performance Computing involves using high performance computing clusters or supercomputing clusters (when sufficiently large) that consist of tightly coupled compute nodes (often thousands) interconnected with a high performance shared filesystem and high speed interconnects.\nA typical layout of a HPC cluster is shown below.\n\n\n\nHPC Cluster Layout\n\n\nImage Credit: https://docs.ycrc.yale.edu/clusters-at-yale\nUsers largely use HPC resources to run parallel codes such as MPI applications. However, HPC clusters are also used to run long-running serial jobs, or jobs that require only one node.\n\n\nHigh Throughput Computing\nHigh Throughput Computing (HTC) involves leveraging many computing resources over long periods of time to accomplish a computational task. These computing resources are often geographically distributed and inter-connected via the internet.\nHTC paradigm focused on executing a large number of tasks over a long period of time, often using distributed resources. Instead of maximizing performance for a single job (as in High Performance Computing, HPC), HTC aims to maximize the total number of jobs completed.\nHTC workloads are often comprised of hundreds or thousands of serial jobs that are mainly independent or loosely coupled, and don’t need tight coupling the way HPC jobs require.\nHTC focuses on increasing the overall throughput of the system by running many smaller size jobs (confined to a single core or a single) in parallel over a distributed computing infrastructure."
  },
  {
    "objectID": "newsite/theme3/DC101/scientific-workflow-management.html",
    "href": "newsite/theme3/DC101/scientific-workflow-management.html",
    "title": "Scientific Workflow Management",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThis module provides an overview on how a CHESS can use notion of workflows to automate their data processing tasks and in the process shortening the turn around time for the data processing that needs to be done on data collected at a beamline.\nThe content in this module is gathered from author’s various presentations over the years and their group’s work on workflows."
  },
  {
    "objectID": "newsite/theme3/DC101/scientific-workflow-management.html#what-are-scientific-workflows",
    "href": "newsite/theme3/DC101/scientific-workflow-management.html#what-are-scientific-workflows",
    "title": "Scientific Workflow Management",
    "section": "What are Scientific Workflows",
    "text": "What are Scientific Workflows\nScientific workflows allow users to easily express multi-step computational tasks, for example retrieve data from an instrument or a database, reformat the data, and run an analysis. A scientific workflow describes the dependencies between the tasks and in most cases the workflow is described as a directed acyclic graph (DAG), where the nodes are tasks and the edges denote the task dependencies.\nA defining property for a scientific workflow is that it manages data flow. The tasks in a scientific workflow can be everything from short serial tasks to very large parallel tasks (MPI for example) surrounded by a large number of small, serial tasks used for pre- and post-processing.\n\n\n\nExample Workflow\n\n\nIn the above example figure, the nodes/vertices in indicate the job that needs to be run, while the edges indicate the dependencies between the jobs.\nThe dependencies can be both control flow or data driven i.e. - a job can be run only after the parent has finished successfully OR - a job requires data sets to run that are generated as outputs by a parent node/job. - the rectangles in the figure indicate datasets that a node/job require and generate.\nUsing scientific workflows for your data processing in general provides you the following benefits\n\nReproducibility - Scientific workflows allow you to document and reproduce your analyses, ensuring their validity.\nAutomation - Workflows automate repetitive and time-consuming tasks, thereby reducing the workload of researchers.\nScalability - Workflows allow you to scale up your data processing to handle large data sets and complex analyses, enabling you to solve large research problems in your field. It helps you run your analysis in parallel over distributed resources.\nReusability - Once your data processing pipeline is modelled as a workflow, typically it is possible to reuse the workflow in different ways. For example, you could use the workflow as part of a bigger science analysis (that maybe faclity wide), or even share the workflow with another CHESS researcher.\n\n\nHigh Level Steps on Identifying a Workflow\nHere are some tips on how you can identify a workflow for your data processing\n\nStart on a whiteboard and think about the various steps that your data processing entails. Especially when you already have existing monolithic code for data processing, it is important to think about your processing at a high level in a logical manner.\nThink about the various steps that make sense logically, and would also make sense another CHESS researcher when discussing the problem.\nThink about the granularity in context of input data. If a particular step in your identified workflow, can benefit on working on smaller chunks of data; then consider breaking down that step in smaller parallel sub steps. For example if you are working on time series dataset, and you have a dataset collected for a month, you can break down processing in chunks of\n\nper day\nper hour\nper minute\nand so forth.\n\nWhat granularity makes sense is subjective. A good rule of thumb is that the step can easily run on a wide variety of compute nodes with reasonable memory. You don’t want to model a workflow that can only run on a few extremely powerful nodes with lots of memory. In some cases this maybe unavoidable. However, it is always good to think along these lines.\nOften, each step (type of step) in your workflow would map to a different executable.\n\n\n\nWorkflow Constructs\nThere are some core different types of workflow patterns that underpin most of the complex workflows. The jobs in the examples, below map to simple commonly available linux executables and is for illustration purposes only. For your actual workflows, you use your actual scientific code.\nProcess\n\n\n\nProcess Workflow\n\n\nIn the above example, workflow consists of a single job that runs the ls command and generates a listing of the files in the / directory.\nPipeline\n\n\n\nPipeline Workflow\n\n\nIn the above example, workflow consists of two jobs linked together in a pipeline. The first job runs the curl command to fetch the Pegasus home page and store it as an HTML file. The result is passed to the wc command, which counts the number of lines in the HTML file.\nSplit\n\n\n\nSplit Workflow\n\n\nIn the above example, workflow consists of a job that downloads the Pegasus home page using the curl command, then uses the split command to divide it into 4 pieces. The result is passed to the wc command to count the number of lines in each piece.\nMerge\n\n\n\nMerge Workflow\n\n\nIn the above example, workflow consists of 3 jobns that execute the ls command on several */bin directories and passes the results to the cat command, which merges the files into a single listing.\n\n\nWorkflow Challenges\nWhen running workflows independent of what workflow system you are using, there are some common challenges that you may encounter.\n\nHow to describe your complex workflows in a simple way? This is important in relation to sharing your workflows with your colleagues. Can the workflow that you just ran locally on your resource, can be run by another researcher on the\n\nsame resource\ndifferent resource / cluster\nwhat changes if any are required?\n\nHow do you get your workflows to access distributed, heterogeneous data and resources (heterogeneous interfaces). For example, you may want to distribute your analysis across different clusters.\nHow to deal with resources/software that change over time? The resource on which you are currently running may go away (no longer operational), software dependencies on which your code requires change.\nHow to have ease of use? Ability to debug and monitor large workflows.\n\n\n\nWorkflow Creation\nThere are a variety of workflow systems available that you can use in general. Some of them allow you to create workflows using\n\nA graphical interface\nProgrammatically using an API in a main stream programming language such as Python\n\nCHESS researchers have access to the following systems for running workflows\n\nCHESS Workflow Runner\nGalaxy (to be covered in DC102)\nPegasus Workflows"
  },
  {
    "objectID": "newsite/theme3/DC101/scientific-workflow-management.html#pegasus-workflows",
    "href": "newsite/theme3/DC101/scientific-workflow-management.html#pegasus-workflows",
    "title": "Scientific Workflow Management",
    "section": "Pegasus Workflows",
    "text": "Pegasus Workflows\nPegasus WMS allows users to model their computational pipelines as workflows, that can execute in a number of different environments including\n\ndesktops,\ncampus clusters,\ndistributed computing environments such as PATh and OSG OSPool,\nsupercomputing CI such as ACCESS and clouds.\n\nPegasus bridges the scientific domain and the execution environment by automatically mapping high-level workflow descriptions onto distributed resources. It automatically locates the necessary input data and computational resources necessary for workflow execution. Pegasus enables scientists to construct workflows in abstract terms without worrying about the details of the underlying execution environment or the particulars of the low-level specifications required by the middleware (HTCondor, SLURM, or Amazon EC2).\nThe clean separation of how the user defines a Pegasus workflow (in a resource agnostic way) called the Abstract workflow and the Executable workflow that actually runs on your target resource is illustrated below.\n\n\n\nPegasus Translation of User Defined Abstract Workflow to Executable Workflow\n\n\nAs a user when you define a Pegasus workflow, you\n\nidentify both input and data sets that a job requires by their logical identifiers called LFN.\nidentify the executable that needs to be invoked for a job by it’s logical identifier called transformation.\n\nThe Abstract workflow when given to Pegasus gets transformed to an Executable workflow that can execute on your target resource. As part of this transformation of the workflow Pegasus will figure out\n\nhow to get/stage the input data required for your workflow. add data stage-in jobs\nmaps your jobs to a compute resource\nhow to stage-out data that your workflow generates\n\nAdditionally, as part of this transformation Pegasus can do a lot of optimizations on your workflow such as\n\nadd data cleanup nodes, that clean up data from cluster when it is not required\ncluster short running jobs together\ndata reuse (delete jobs whose datasets already exist)\n\n\nGetting Started with Pegasus @ CHESS\nPegasus is already installed and configured to run on the CHESS cluster. In order to use Pegasus, you need to login to the node lnx201.classe.cornell.edu .\nX-CITE vahi$ ssh  lnx201.classe.cornell.edu\n(user@lnx201.classe.cornell.edu) Password: \nSource Pegasus binaries in your PATH\nIn order to run Pegasus you need to make sure that Pegasus executables are in your PATH. You can do this by sourcing the following setup file.\n[kvahi@lnx201 ~]$ source /nfs/chess/sw/pegasus/setup.sh\n(pegasus-env) [kvahi@lnx201 ~]$ pegasus-version \n5.1.0dev\n\n\nSetup a test diamond workflow and run it\nIn this example, we will try and run a test diamond workflow through Pegasus.\n\n\n\nDiamond Abstract Workflow\n\n\nTo setup a test workflow that runs on the CHESS SGE cluster, you can use the pegasus-init executable and answer the questions asked.\nYou should do the following selections when prompted\n\nSelect an execution environment [1]: 5\nWhat’s the execution environment’s queue: chess.q\nWhat’s your project’s name []:\nSelect an example workflow [1]: 1\n\n[kvahi@lnx201 pegasus]$ pegasus-init diamond\n###########################################################\n###########   Available Execution Environments   ##########\n###########################################################\n1) Local Machine Condor Pool\n2) Local SLURM Cluster\n3) Remote SLURM Cluster\n4) Local LSF Cluster\n5) Local SGE Cluster\n6) OLCF Summit from OLCF Headnode\n7) OLCF Summit from OLCF Hosted Kubernetes Pod\n\nSelect an execution environment [1]: 5\nWhat's the execution environment's queue: chess.q\nWhat's your project's name []: \nWhat's the location of the PEGASUS_HOME dir on the compute nodes [/nfs/chess/user/kvahi/software/pegasus/default]: \n###########################################################\n###########     Available Workflow Examples      ##########\n###########################################################\n1) pegasus-isi/diamond-workflow\n2) pegasus-isi/hierarichal-sample-wf\n3) pegasus-isi/merge-workflow\n4) pegasus-isi/pipeline-workflow\n5) pegasus-isi/split-workflow\n\nSelect an example workflow [1]: 1\nFetching workflow from https://github.com/pegasus-isi/diamond-workflow.git\nGenerating workflow based on pegasus-isi/diamond-workflow\nThis workflow will target queue \"chess.q\"\nThe PEGASUS_HOME location is \"/nfs/chess/user/kvahi/software/pegasus/default\"\nCreating workflow properties...\nCreating transformation catalog...\nCreating replica catalog...\nCreating diamond workflow dag...\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000001, transformation=preprocess)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000002, transformation=findrange)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000003, transformation=findrange)\nINFO:Pegasus.api.workflow:diamond added Job(_id=ID0000004, transformation=analyze)\nINFO:Pegasus.api.workflow:inferring diamond dependencies\nINFO:Pegasus.api.workflow:workflow diamond with 4 jobs generated and written to workflow.yml\nCreating properties file...\nCreating site catalog for SitesAvailable.SGE...\nThe example workflow gets generated in the diamond directory.\n[kvahi@lnx201 pegasus]$ cd diamond/\n(pegasus-env) [kvahi@lnx201 diamond]$ ls\ndiamond-workflow  generate.sh  pegasus.properties  plan.sh  replicas.yml  sites.yml  transformations.yml  workflow.yml\nIn this directory, you will see the following yaml files\n\nworkflow.yml - the abstract workflow generated for the diamond workflow that we will run.\nreplicas.yml - a yaml formatted file that lists the locations of the inputs for the workflow.\ntransformations.yml - a yaml formatted file that lists the locations of executables used by the workflows.\nsites.yml - a yaml formated file that describe the CHESS SGE cluster to Pegasus.\n\nThe above catalog files are described in the Pegasus documentation.\nTo plan the workflow, run the ./plan.sh script.\n(pegasus-env) [kvahi@lnx201 diamond]$ ./plan.sh\n./plan.sh \n2025.05.13 14:56:10.755 EDT:    \n2025.05.13 14:56:10.761 EDT:   ----------------------------------------------------------------------- \n2025.05.13 14:56:10.766 EDT:   File for submitting this DAG to HTCondor           : diamond-0.dag.condor.sub \n2025.05.13 14:56:10.771 EDT:   Log of DAGMan debugging messages                 : diamond-0.dag.dagman.out \n2025.05.13 14:56:10.777 EDT:   Log of HTCondor library output                     : diamond-0.dag.lib.out \n2025.05.13 14:56:10.782 EDT:   Log of HTCondor library error messages             : diamond-0.dag.lib.err \n2025.05.13 14:56:10.787 EDT:   Log of the life of condor_dagman itself          : diamond-0.dag.dagman.log \n2025.05.13 14:56:10.792 EDT:    \n2025.05.13 14:56:10.798 EDT:   -no_submit given, not submitting DAG to HTCondor.  You can do this with: \n2025.05.13 14:56:10.808 EDT:   ----------------------------------------------------------------------- \n2025.05.13 14:56:23.174 EDT:   Database version: '5.1.0dev' (sqlite:////home/kvahi/.pegasus/workflow.db) \n2025.05.13 14:56:28.480 EDT:   Pegasus database was successfully created. \n2025.05.13 14:56:28.485 EDT:   Database version: '5.1.0dev' (sqlite:////nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001/diamond-0.replicas.db) \n2025.05.13 14:56:28.693 EDT:   Output replica catalog set to jdbc:sqlite:/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001/diamond-0.replicas.db \n2025.05.13 14:56:28.693 EDT:   \n\n\nI have concretized your abstract workflow. The workflow has been entered \ninto the workflow database with a state of \"planned\". The next step is \nto start or execute your workflow. The invocation required is\n\n\npegasus-run  /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n \n2025.05.13 14:56:29.864 EDT:   Time taken to execute is 28.297 seconds \nThe above command generated the executable workflow that you can run.\n\n\n\nDiamond Executabe Workflow\n\n\nTo do this copy the pegasus-run command invocation that you see in your terminal.\n(pegasus-env) [kvahi@lnx201 diamond]$ pegasus-run  /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\nSubmitting to condor diamond-0.dag.condor.sub\nSubmitting job(s).\n1 job(s) submitted to cluster 693.\n\nYour workflow has been started and is running in the base directory:\n\n/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n*** To monitor the workflow you can run ***\n\npegasus-status -l /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n\n*** To remove your workflow run ***\n\npegasus-remove /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\nTo check the status of the worklfow we can run the command pegasus-status with a -w 30 option to watch it every 30 seconds.\nAgain make sure you copy the pegasus-status invocation that you see in your terminal and add the -w 30 option after -l option.\n(pegasus-env) [kvahi@lnx201 diamond]$ pegasus-status -l -w 30 /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\n   ID        SITE      STAT  IN_STATE  JOB                      \n  693        local      Run    03:06   diamond-0 (/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001)\n  696         sge      Done    00:04   ┗━untar_diamond_0_sge    \nSummary: 2 Condor jobs total (R:1)\n\nUNREADY READY  PRE  IN_Q  POST  DONE  FAIL %DONE  STATE  DAGNAME                  \n  13      0     0    0     1     2     0    12.5 Running diamond-0.dag           \nOnce the workflow completes you will see somthing similar to the following\npegasus-env) [kvahi@lnx201 diamond]$ pegasus-status -l -w 30 /nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001\nPress Ctrl+C to exit                                    (pid=2380412)                                 Tue May-13-2025 15:07:51\n\n   ID        SITE      STAT  IN_STATE  JOB                      \n  693        local      Run    08:58   diamond-0 (/nfs/chess/user/kvahi/diamond/submit/kvahi/pegasus/diamond/run0001)\nSummary: 1 Condor job total (R:1)\n\nUNREADY READY  PRE  IN_Q  POST  DONE  FAIL %DONE  STATE  DAGNAME                  \n   0      0     0    0     0     16    0   100.0 Success diamond-0.dag            \nSummary: 1 DAG total (Success:1)\nYou can also run additional pegasus commands after your worklfow finishes. The  should be replaced by the directory you passed to the pegasus-status command.\n\npegasus-analyser  - This allows you to debug your workflow if it fails. Will tell you what jobs failed and why.\npegasus-statistics  - This allows you to generate runtime statistics about your workflow run such how long it run, how much resources were used etc.\n\n\n\nSupport\nIf you would like to use Pegasus to run your pipeline you can contact via\n\nEmail - Support requests and bug reports can be emailed to pegasus-support@isi.edu.\nSlack - We encourage you to join the Slack Workspace as it is an\non-going, open forum for all Pegasus users to share ideas, experiences, and talk out issues with the Pegasus Development team. Please ask for an invite by trying to join pegasus-users.slack.com in the Slack app, or email pegasus-support@isi.edu and request an invite."
  },
  {
    "objectID": "newsite/theme3/DC101/scientific-workflow-management.html#chess-workflow-runner",
    "href": "newsite/theme3/DC101/scientific-workflow-management.html#chess-workflow-runner",
    "title": "Scientific Workflow Management",
    "section": "CHESS Workflow Runner",
    "text": "CHESS Workflow Runner\nTo be added. Coming soon in FALL 2025."
  },
  {
    "objectID": "newsite/about.html",
    "href": "newsite/about.html",
    "title": "About X-CITE",
    "section": "",
    "text": "This site hosts material meant for CyberInfrastructure Training and Education for Synchrotron X-Ray Science (X-CITE). X-CITE is developed for the community of scientists and researchers using the CHESS synchrotron X-ray facility and similar light sources.\n\nAbout this site\nThe sources of this site are available at https://github.com/RENCI-NRIG/X-CITE.\nThe content in this website is available under Creative Commons Attribution-ShareAlike 4.0 International license. Code snippets may be used under CC0 1.0 Universal license.\nCopyrights for logos are owned by the respective organizations.\nThis site is generated using Quarto."
  },
  {
    "objectID": "newsite/404.html",
    "href": "newsite/404.html",
    "title": "Page Not Found",
    "section": "",
    "text": "The page you requested cannot be found. Perhaps it was moved or renamed.\nYou may want to try searching to find the page’s new location."
  },
  {
    "objectID": "newsite/theme3/DC200/computing-with-ci-ecosystem.html",
    "href": "newsite/theme3/DC200/computing-with-ci-ecosystem.html",
    "title": "Computing with CI Ecosystem",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThis module provides an overview of the various Cyberinfrastructure (CI) resources available to a CHESS researcher and more broadly to all the researchers based in the US.\nBefore we go into the various offerings available, it is useful to recap what we mean by CI. A widely used and accepted definition of CI by Craig A. Stewart et al is as follows:\nCyberinfrastructure consists of computing systems, data storage systems, advanced instruments and data repositories, visualization environments, and people, all linked together by software and high performance networks to improve research productivity and enable breakthroughs not otherwise possible.\nAs a CHESS researcher, you have access to the following CI resources that can help you with computing"
  },
  {
    "objectID": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#overview-of-chess-operational-workflow",
    "href": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#overview-of-chess-operational-workflow",
    "title": "Computing with CI Ecosystem",
    "section": "Overview of CHESS Operational Workflow",
    "text": "Overview of CHESS Operational Workflow\nBelow is a high level overview of an overall chess operational workflow from the time a researcher puts in a proposal to use a beamline at CHESS to data collection from the beamline; and then it’s subsequent data processing and analysis before final data curation on the outputs is done.\n\n\n\nOverview of a general CHESS Operational Workflow\n\n\nCHESS’ workflow complexity is the length of time spent performing a typical data analysis. According to a CHESS survey conducted a few years back, CHESS’ workflow complexity ranges from 1 month to 24 months among survey respondents (with the average being 7.2 months). For the most complex workflows (12 months or longer), researchers often run into challenges regarding completing their data processing within the defined time constraints.\nUse of National CI resources, can especially help CHESS researchers in the following areas\n\nProcessing data (reduction, analysis, simulation, interpretation), handling large data sets, leveraging existing software and CI.\nMetadata management, Open Science/FAIR and data curation."
  },
  {
    "objectID": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#chess-ci-resources-at-cornell",
    "href": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#chess-ci-resources-at-cornell",
    "title": "Computing with CI Ecosystem",
    "section": "CHESS CI Resources at Cornell",
    "text": "CHESS CI Resources at Cornell\nThe CLASSE cyberinfrastructure (CI) consists of an interconnected series of h high-availability server clusters (HACs), data acquisition systems, control systems, compute farms, and workstations. Most of these systems run either Scientific Linux or Windows on commodity 64-bit Intel-based hardware and are centrally managed using Puppet. The median age of key CI components is approximately 5 years, with an average refresh rate of once every 10 years. The picture below provides an overview of the CHESS CI\n\n\n\nCHESS CI immediately available for processing/analysis by users on the Compute Farm\n\n\n\nGetting access\nIn order to access the Cornell CI resources, you need a Cornell Laboratory for Accelerator-based ScienceS and Education(CLASSE) account.\nCHESS users can request a CLASSE account through the CHESS User Dashboard (BeamPASS).\nMore info in the user guide, under “Activate your CLASSE Account”.\n\n\nDAQ Cluster\nDAQ Cluster is the Data Acquisition System that runs on a dedicated server cluster. Makes available to researchers about 2PB of storage for raw data collection, analysis, and simulation. Data collected at the stations written directly to the DAQ over either NFS or Samba and is immediately available for processing/analysis by users on the Compute Farm.\nCHESS users can also download their data remotely using the CLASSE Globus Connect Server or via SFTP.\n\n\nCompute Farm\nThe HPC Cluster at CHESS/CLASSE is a central resource consisting of\n\ncentral resource of 60+ enterprise-class Linux nodes (with around 800 CPUs)\n4.5TB of memory\nuses SGE as a front-end queueing system\nsupports interactive, batch, parallel, and GPU jobs\nensures equal access to the Compute Farm for all users.\n\nThis cluster is suited for doing both HPC and HTC type jobs.\n\nLogin Node and other information\nThe CHESS cluster login node is to use ssh to login to lnx201.classe.cornell.edu which is the headnode of the cluster.\nssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu\nThere is also a shared filesystem available across all the nodes in the cluster. This can be accessed at /nfs/chess/user/ .\nA complete list of various filesystems accessible can be found at CLASSE Wiki.\n\n\n\nJob Submission\nThe CHESS cluster uses SGE as the front end scheduling system to submit jobs.\nIn general, there are two basic steps to submitting a job\n\nCreate a shell script containing the commands to be run\nSubmit this script to the Compute Farm using the qsub command.\n\nBelow is a simple shell script myscript.sh that you can submit to the SLURM cluster if you are logged onto lnx201.\n[user@lnx201 ~]$ cat myscript.sh\n#!/bin/bash\necho Running on host: `hostname`.\necho Starting on: `date`.\nsleep 10\necho Ending on: `date`.\n#$ -q all.q\n#$ -S /bin/bash\n#$ -l mem_free=8G\nIn order to submit it to the cluster you can use qsub command\n[user@lnx201 ~]$ qsub -q all.q myscript.sh\nDetailed instructions about this can be found at CLASSE Wiki.\nIf you want to submit job to another queue, you can change the -q option. In order to see all the queues on the cluster, you can run the following command.\n[user@lnx201 ~]$ qconf -sql\nall.q\nbenchmark.q\nchess.q\nchess_fast.q\n...\nchess_xleap_interactive.q\ninteractive.q\nThe above output is shortened for display purposes."
  },
  {
    "objectID": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#access",
    "href": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#access",
    "title": "Computing with CI Ecosystem",
    "section": "ACCESS",
    "text": "ACCESS\nACCESS CI is a program established and funded by the National Science Foundation (NSF) to help researchers and educators utilize the nation’s advanced computing systems and services. ACCESS is a collection of both large and experimental HPC resources.\nACCESS provides a wide range of resources and services:\n\nSystems ranging from supercomputers to smaller specialized compute clusters, each with a different focus and unique set of capabilities\nData and storage services\nExpertise to help you make effective use of resources, remove barriers, and achieve your goals\nScientific applications\nScience gateways\n\nSome of the resources available are listed below\n\n\n\nLeadership Class\n\n\n\n\n\nFrontera\nUniversity of Texas, Austin\n\n\n\n\n\n\nInnovative Production Systems\n\n\n\n\n\nAnvil\nPurdue University\n\n\nBridges 2\nCarnegie-Mellon University\n\n\nDelta/ DeltaAI\nU of Illinois Urbana-Champaign\n\n\nExpanse\nU of California, San Diego\n\n\nJetstream 2\nUniversity of Indiana + Partners\n\n\nStampede 2\nU of Texas, Austin\n\n\n\n\n\n\nPrototypes/Testbeds\n\n\n\n\n\nNeocortex\nCarnegie-Mellon University\n\n\nVoyager\nU of California, San Diego\n\n\nOokami\nStonybrook University\n\n\nNRP\nU of California, San Diego\n\n\nACES\nTexas A&M University\n\n\n\n\n\n\nCloud Technology Resources\n\n\n\n\n\nCloudbank\nU of California, San Diego\n\n\nCloudLab\nUniversity of Utah\n\n\nChameleon\nUniversity of Chicago\n\n\n\n\n\n\nLocation of various ACCESS Resources\n\n\nOf the resources above, the leadership class system and most of the Innovative Production Systems are HPC clusters, very similar to the CHESS HPC cluster at Cornell. The main difference being that they use SLURM as their frontend queueing system instead of SGE.\n\nACCESS User Registration\nIn order to use ACCESS resources, you first need to request an ACCESS user account. This account is required for you to login to the ACCESS website, manage your allocations (more on it in the next section).\nGetting an ACCESS user account is fairly straightforward. Instructions for getting it can be found here. Using an existing University account when registering with ACCESS simplifies the sign-up process and enables you to log in to ACCESS using that existing account.\n\n\nGetting an Allocation\nTo get started, you need an ACCESS project and some resource units you can spend. Your ACCESS project and resource units are what we refer to as an Allocation. An allocation is your project to use a portion of a shared resource.\nThrough ACCESS, you can get an allocation to use computing and data resources to accomplish your research or classroom objectives.\nYou can get allocation for 4 different types of projects which are listed below.\n\nEXPLORE — Great for resource evaluation, graduate student projects,\nsmall classes and training events, benchmarking, code development and porting, and similar small-scale uses.\nDISCOVER — Designed for research grants with modest resource needs, Campus Champions, large classes and training events, graduate student projects, benchmarking and code testing at scale, and gateway development.\nACCELERATE — Best for experienced users with mid-scale resource needs, consolidating multi-grant programs, collaborative projects, preparing for Maximize ACCESS requests, and gateways with growing communities.\nMAXIMIZE — The choice for large-scale research activities that need more resources than the limit for Accelerate ACCESS projects.\n\nThe EXPLORE requests are easiest to get with MAXIMIZE the hardest (in terms of supporting documentation required). The EXPLORE request will get you 400,000 ACCESS credits/SUs to start with, which is enough to explore suitability of these resources for your processing needs.\nMore details including a detailed comparison table can be found here.\n\n\nLogging to the ACCESS resource\nOnce, you have your allocation approved then in the Allocations portal you can assign your credits to a particular resource that you are interested in. Once that is done, the Resource Provider will contact you to setup your local HPC accounts.\nOnce that is done, you can * register your ssh keys to login to your account on the HPC resource.\nAdditionally, most of the ACCESS resources have Open OnDemand installed, that allow you to access and login to the clusters using a web front-end. Open OnDemand is an easy-to-use web portal that is being deployed on ACCESS resources to allow researchers to compute from anywhere without client software or command-line interface, and significantly speed up the time to science.\n\n\nSubmitting Jobs\nYou can login to the headnodes of these resources, and submit jobs to the SLURM clusters in a similar fashion that you submit jobs on the CHESS cluster.\nAdditionally, all the ACCESS resources that have an Open OnDemand install, allow you to launch Jupyter notebooks to run your analysis.\nYou can also use workflow systems such as Pegasus to run your analysis pipelines on ACCESS resources. One easy way to explore Pegasus workflows is to use ACCESS Pegasus.\n\n\nACCESS Support\nACCESS does provide support for researchers looking to leverage their resources. The support comes in various flavors as illustrated in the picture below.\nACCESS Support provides researchers with access to\n\nTools, growing knowledge base\nMatch-making with experts\nStudent engagement\nEngagement from community\n\nACCESS provides Match services (Tier 3 and Tier 4) which connects researchers with experts to help you select the right system, run on a supercomputer, and solve basic code and research problems.\n\n\n\nACCESS Support Tiers"
  },
  {
    "objectID": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#osg-and-path-facilities",
    "href": "newsite/theme3/DC200/computing-with-ci-ecosystem.html#osg-and-path-facilities",
    "title": "Computing with CI Ecosystem",
    "section": "OSG and PATh Facilities",
    "text": "OSG and PATh Facilities\nPATh and OSG make up the OSG Consortium that builds and operates a set of pools of shared computing and data capacity for distributed high-throughput computing (dHTC).\nThe OSG Consortium builds and operates a set of HTCondor pools of shared computing and data capacity for distributed high-throughput computing (dHTC). Each pool is organized and operated to serve a particular research community (e.g. a campus, multi-institutional collaboration, etc.), using technologies and services provided by the core OSG Team. One of these pools, known as the Open Science Pool is operated for all of US-associated open science.\n\n\n\nOSPools Geographic Distribution\n\n\nImage Credit: OSG Research Facilitation Team\nOne of the most attractive features of using the OSPool is that it has a No Proposal, No Allocation, No Cost principle. It povides its users with fair-share access to compute and storage capacity contributed by university campuses, and government-supported supercomputing institutions.\n\nWhat type of jobs are good fit for OSPool\nThe OSPool is made up of mostly opportunistic capacity - contributing clusters may interrupt jobs at any time. Thus, the OSPool supports workloads of numerous jobs that individually complete or checkpoint within 20 hours.\nThe OSG consortium provides guidance on what type of jobs are a good fit. It is replicated from the OSPool website below.\n\n\n\n\n\n\n\n\n\n\nIdeal Jobs!\nStill very advantageous\nMaybe not, but get in touch!\n\n\n\n\nExpected Throughput, per user\n1000s concurrent cores\n100s concurrent cores\nLet’s discuss!\n\n\nCPU\n1 per job\n&lt; 8 per job\n&gt; 8 per job\n\n\nWalltime\n&lt; 10 hrs*\n&lt; 20 hrs*\n&gt; 20 hrs\n\n\nRAM\n&lt; few GB\n&lt; 40 GB\n&gt; 40 GB\n\n\nInput\n&lt; 500 MB\n&lt; 10 GB\n&gt; 10 GB**\n\n\nOutput\n&lt; 1 GB\n&lt; 10 GB\n&gt; 10 GB**\n\n\nSoftware\npre-compiled binaries, containers\nMost other than →\nLicensed Software, non-Linux\n\n\n\n\n\nOSG User Registration\nTo get started with the OSPool, please complete the account application here. After submitting the form, you’ll receive an invitation to a new user meeting with an OSG facilitator. During the meeting, you’ll get an overview of OSG and discuss how to run your computations on the platform. Your account and project will be set up following this meeting.\n\n\nSubmitting jobs\nSubmitting jobs involves logging into one of the OSG Access points, from where you can submit a job using HTCondor or a workflow tool like Pegasus to the OSPool. Submitting the job is similar to how you submit a job on your local HPC cluster. The main difference is that you are interacting with HTCondor not SLURM or SGE.\nAnother important difference to keep in mind is that unlike HPC clusters, you cannot expect a shared filesystem between the login node (from where you submit the job) and the nodes on which your jobs execute. The nodes on which your jobs run are geographically distributed as illustrated in the figure below.\n\n\n\nOSPools Job Submission\n\n\nImage Credit: OSG Research Facilitation Team\n\n\nPATh\nThe PATh Facility is part of a pilot project funded by the NSF Office of Advanced Cyberinfrastructure. NSF-funded researchers can apply to request credits for the PATh facility. In that aspect, PATh is also similar to ACCESS resources in that you need to apply for credits to run on these resources.\nComposed of current gen hardware, the PATh facility provides users access to\n\n30,000 cores and\n36 A100 GPUs\n\nPATh user registration can be found on the PATh website.\n\n\nOS Pool vs PATh\nA researcher’s experience on PATh Facility and OSG’s Open Science Pool (OSPool) compute systems is similar: both offer thousands of CPU resources, as well as GPUs, disk space for saving actively-used data, and support technologies such as containerized software and checkpointing. They also both use a HTCondor Software Suite as a job scheduling software, which specializes in managing large high-throughput workflows.\nThe main differences between the two are\n\nOSPool is open to any US-affiliated academic, non-profit, or government research projects while PATh is available to only researchers with accepted or active NSF award with selected affiliated programs.\nPATh uses compute credit system to request resources.\nOn PATh researchers can request larger amounts of CPU/GPUs, more memory, disk space, and a longer runtime and are guaranteed these resources until their job completes.\n\nFull details on these differences can be found here."
  },
  {
    "objectID": "newsite/theme3/DC102/using-science-gateways.html",
    "href": "newsite/theme3/DC102/using-science-gateways.html",
    "title": "Using Science Gateways",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme1/PE102/index.html",
    "href": "newsite/theme1/PE102/index.html",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "In the previous section, modules for Python was introduced. In this section, we’ll take a much more detailed look at one of the most useful to scientists: NumPy. This module contains numerous routinues and support frameworks for numerical computing. The routinues in it are very carefully tested for correctness and are crafted for speed. Any time you can use something from this package, it’s a good idea to.\nPython is built for versatility and ease of programming. Unfortunately, it is not built for speed. Over the years Python has gotten faster and faster but there is still a speed penalty compared to classic compiled languages like C, C++, or Fortran.\nEnter NumPy: a package of mathematical routines written in C and Fortran and made to work with Python via a “glue” or “shim” layer. This interface is invisible to the programmer. NumPy looks and behaves just like any other Python package. Under the surface, though, lies a very fast and efficient library of algorithms.\n\n\nLet’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)\n\n\n\n\n\nThe trivial example: add a scalar (“a single number”) to every element of the matrix:\n\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]\n\n\n\n\n\nLet’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nSource: NumPy - A Mathematical Toolkit for Python"
  },
  {
    "objectID": "newsite/theme1/PE102/index.html#a-first-glimpse",
    "href": "newsite/theme1/PE102/index.html#a-first-glimpse",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "Let’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)"
  },
  {
    "objectID": "newsite/theme1/PE102/index.html#lets-do-some-actual-math-shall-we",
    "href": "newsite/theme1/PE102/index.html#lets-do-some-actual-math-shall-we",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "The trivial example: add a scalar (“a single number”) to every element of the matrix:\n\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]"
  },
  {
    "objectID": "newsite/theme1/PE102/index.html#linear-algebra-anyone",
    "href": "newsite/theme1/PE102/index.html#linear-algebra-anyone",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "Let’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html",
    "href": "newsite/theme1/PE100/index.html",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "",
    "text": "Note\n\n\n\nThe content in this page is written in the form of Jupyter notebooks. You can read the HTML version of the notebooks here. However, you will likely want to open and run the notebooks in a JupyterHub instance, such as CLASSE JupyterHub. See CLASSE Wiki for details."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#operators",
    "href": "newsite/theme1/PE100/index.html#operators",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Operators",
    "text": "Operators\nLike any programming language, Python lets you “do math” and lots of other things. Let’s take a look at some of the basic “operators”. In all of the code-containing cells through this course, try to predict what will happen first, and then run the code.\n\n\n2\n\n\nBesides the “classic” operators, there are some handy extras:\nWhat happened there? The // operator does integer division - it returns the whole number part of the answer, just like when we learned division in elementary school.\nThe % operator returns the remainder. This is also called “modulo”, and the above would be pronounced “sixteen mod 3”.\n\n\n256\n\n\nThe ** operator does exponentiation. The arguments can be integers or they can be real numbers. Naturally, operators can be combined into arbitrarily long expressions.\nNotice what happens when we use different operators. They are applied in the “My Dear Aunt Sally” order of precendence (multiplication, division, addition, subtraction).\nOrder of operations: * Exponentiation: ** * Multiplication, Division, Remainder: * / // % * Addition and Subtraction: + -\nWithin the same level, operators are applied left-to-right. 8-5+2 is evaluated as 3+2 and yields 5. The exception is exponentiation: 2 ** 3 ** 4 is treated as 2 ** 81 and yeilds an annoyingly large number"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#variables",
    "href": "newsite/theme1/PE100/index.html#variables",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Variables",
    "text": "Variables\nUnless we just use Jupyter as a big, expensive scientific calculator, we need a way to store data. Variables were invented for just that purpose, and virtually every language has them. Think of them as a place to store data of some kind, and that place has a name. They behave in Python just like you’d expect.\n\n\n42\n\n\nWe just created a variable named answer and gave it the value 42. Variables are long-lived - later we’ll talk about just how long when we start writing our own functions, but until then our variables last as long as Python (or in our case, Jupyter) is running. Take a look - answer is still there.\nThe value stored in a variable can change. It can even change type:\nWe can declare many variables, and we can “do things” with them just like we can when we type in numbers or strings.\nIn the last line, we just put watts because Jupyter automatically prints what the last line evaluates to.\nWe can use variables to change the order of operations. Let’s see the average price of two people’s meals:\nThat’s the right answer. If we hadn’t done that, we would have gotten\nwhich is utterly wrong. Beware of the order of operations… it is a frequent source of bugs in scientific programming.\n\nVariable Naming Rules\nFor the most part, you can pick whatever name makes sense for a variable, but there are some rules. When choosing a name: 1. No keywords (False won’t work.) 1. No spaces (sample thickness is invalid) 1. The first character must be one of * a-z, or A-Z, or _. (the underscore character) * As a result, no numbers (3rd_sample_holder is invalid) 1. After the first character, you can then have numbers (sample_holder_3 is perfectly valid) 1. No other symbols are allowed (exploded&destroyed_spectrometers is invalid, and probably suggests it’s time to review lab safety procedures).\nNote: Uppercase vs. Lowercase matters! Bevatron is not the same variable as bevatron"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#types",
    "href": "newsite/theme1/PE100/index.html#types",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Types",
    "text": "Types\nWe’ve hinted that variables have a “type”, and that the type can change if it needs to. The way it works is that variables keep track of what type they are (integer, real number, or string) and what their “value” is. We can even interrogate a variable as see what type it is:\n\n\nreading:\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\nThe type of a variable matters. Let’s create a variable with an integer in it and another with a string. Then let’s do some math:\nHow do we handle situations like that, where second_thing held a string representing a seven, but because it was a string variable it couldn’t be used as an integer? Python provides a few functions to convert values from one type to another. The str() function takes a variable and converts it to a string. The float() and int() functions convert their arguments to floating-point and to integer numbers, respectively.\n\n\n13\n13.0\n\n\nBeing able to convert values from one type to another is often called type coercion. These conversions are extremely important for situation where you need to get input from a user, even more so if you need to do it repetitively."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#continuation-character",
    "href": "newsite/theme1/PE100/index.html#continuation-character",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Continuation Character",
    "text": "Continuation Character\nSometimes the expressions we need to evaluate can be very long. It would be nice if we could split up a long expression and spread it out over a few lines. As a small example, we’ll take a look at 4+2+3. Many programming languages will let us split an expression anywhere we want, such as:\n…but that result isn’t right in Python. The last line, +3, was evaluated and printed as the result of running that cell. In Python,it turns out, if we need to continue an expression on the next line we must end the current line with a backslash \\ and press enter. It has to be a backslash, by the way, and cannot be the forward slash like we use for division.\n\n\n9\n\n\nTime for an exercise! Try to predict what will be printed when you run the next cell. Then, run the next cell and see how you did. If you miss one, make sure you figure out what happened before you go. I know, we’re professionals, I shouldn’t have to say that…\nNow write an expression to average three numbers (12, 14, and 66), divide the result by three, and square it. You can use the code cell right below here:"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#the-string-type",
    "href": "newsite/theme1/PE100/index.html#the-string-type",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "The String Type",
    "text": "The String Type\nAt the beginning of this notebook, we casually mentioned “strings” without saying what they are. They’re just “sequences of characters”. And these can be any kind of characters - the English alphabet, the Hungarian alphabet, hiragana… it doesn’t matter.\nSome, probably most, languages contain strings inside “double quotes”, \", which is shift+apostrophe on US English keyboards. Other languages (SQL and Pascal are the only two I can think of) use single quotes: '. Python lets you use either one. You do have to be consistent in each string, but it can vary from one string to the next:\nBecause we can use either type of quotation mark, we can exploit that to let us put quotation marks into strings:\n\n\nDon't put explosive mixtures in the spectrometer, please.\nOf course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.\n\n\nThat lets us embed whichever kind of quotation mark we need into a string.\nBut what if we need to embed both kinds of quotes into one string? We’re in luck: we can use the backslash character again to “quote” our quotation mark. In fact, we can quote any character with it if we need to.\n\n\nWe told him \"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\", but we're pretty sure he ignored us.\n\n\nThat sentence contains three things, inside the string itself: 1. Double Quotes to surround a direct quotation 2. A single quote, also called an apostrophe depending on how it’s used, to make a contraction, and 3. A totally awesome/terrifying molecule you have to google to believe.\nOK, I’ll save you the trouble. Prepare to lose most of a day’s productivity. You’re welcome.\n(Derek has written gobs of articles on fun substances. Here are some more. )\nThere is one last kind of string literal. Sometimes you need a string that is several lines long. The “triple quote” is a way to do it. You have to use three double-quotes in a row:\nTriple quotes are also an easier way to embed mixed kinds of quotation marks into strings:\n\n\nI know people who say \"The Avengers\" isn’t a good movie, but I don’t agree."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#coming-up-next",
    "href": "newsite/theme1/PE100/index.html#coming-up-next",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Coming Up Next",
    "text": "Coming Up Next\nWe just looked at enough of Python and Jupyter notebooks to use it as a basic calculator, but so far we can’t do any real, general-purpose programming with it. The “flow of control” sob far as been a straight line from top to bottom and we can’t change what we’re doing in response to different inputs. That’s about to change. In the next section we’ll look at the if statement and how to use it."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#the-simplest-if-statement",
    "href": "newsite/theme1/PE100/index.html#the-simplest-if-statement",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "The Simplest “if” Statement",
    "text": "The Simplest “if” Statement\nIn almost any real Jupyter notebook or standalone program we write, there will have to be places where different code paths are taken depending on what has happened leading up to there. Suppose we’re looking at absorption at one specific wavelength and we know that some of our instruments are a little bit too sensitive to changes in humidity. Maybe the first spectrometer has some insulation that is just a little too porous and reads a bit high, but the second one is even worse. We have calibration constants we can apply, but we have to apply the right constant for each individual instrument.\n\n\n7.539441569999999\n\n\nHere we have the first Decision Structure (also called control flow statement) that we’ll look at. Taking the above code apart, we see several important things.\n\nThis is an “if statement”.\nTesting to see if two things are equal is done with two equals signs, not one (==). There’s a historical reason for this, and it’s a good reason, but it always trips up newcomers. You have been warned. You’re welcome.\nThe last character on the if line is : (a colon ).\nThe “body” of the if statement, the part that is run if and only if the tested condition is met, is indented.\n\nIn the case of the above if statement, what the code does is check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called “useful_result”."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#else-the-catch-all-specialist",
    "href": "newsite/theme1/PE100/index.html#else-the-catch-all-specialist",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Else: the catch-all specialist",
    "text": "Else: the catch-all specialist\nIf that was all an if statement could do then it would be really useful. But that’s not all it can do. We need to do something reasonable when we get readings from the second instrument. Such as:\n\n\n8.3304879\n\n\nHere we have added an “else clause”. The above code is interpreted as “check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called useful_result. Otherwise, set useful_result to whatever is saved in”reading” plus 19%.\nSo far, so good. But there’s more! Suppose we need to handle several of these not-quite-top-quality spectrometers. How do you suppose we could deal with that? We could resort to putting if-else statements inside if-else statements in sort of a brute force fashion…\n\n\n6.4403771999999995\n\n\nThe above code looks a little intimidating, but all there is to it is just a series of if statements. The logic of it goes like this: “If the instrument number is 1, then adjust it 7.7% and we’re done. Otherwise, it must be some other instrument number, so run our else clause”. Then in the else clause, it does the same thing, except checking for the second instrument and adjusting by 19%. If there was nothing to do there (because the instrument number was 3) then we run the else clause of that second if statement. This else clause houses an if statement that checks to see if the instrument is number three. This time it is, so the body of the if statement is executed. We set useful_reading equal to 92% of reading."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#elif",
    "href": "newsite/theme1/PE100/index.html#elif",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Elif",
    "text": "Elif\nThis is fine if we only have three instruments, but what do we do if we have 20 of them? We could, in principle, type in 60 lines of code, but that would be tedious, error prone, and would take a while to read and find any mistakes. Of course there’s a better way.\nThat better way is the “elif” keyword.\nLet’s see an example with 5 instruments…\n\n\n7.210422299999999\n\n\nThe final else clause is the one that runs if no other clauses ran. If no clause’s conditional statement is true so no clause runs, whether it’s the if clause or any of the elif clauses, then the else clause runs. It’s really easy to spot else clauses even from across the room - they’re the ones that don’t have a conditional test.\nNote that the if, elif, and else lines must end with a colon. True confession time: I forget the colons about half the time. Python catches it as an error, I fix it, and life goes on."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#slightly-more-complicated",
    "href": "newsite/theme1/PE100/index.html#slightly-more-complicated",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Slightly More Complicated",
    "text": "Slightly More Complicated\nYou can run more than one line of code in response to the tested conditions, but they have to be indented the same amount:\n\n\n7.00041 True\n\n\nThere are four interesting things going on here. The first and most important thing to notice is that we’ve got more than one line of code running in response to an “if”, “elif”, or “else” clause. A collection of lines that should be run together as a whole is called a code block. Unlike many languages that mark the start and end of code blocks with special words or characters, Python just does it by using indentation. Everything that is indented the same amount is considered to be in the same code block. We’ll look at this in more detail in a few minutes.\nSecondly, we’ve added lines to set a variable named “trustworthy” to a value depending on whether we had to adjust the reading. Evidently, if we have to compensate for old, dry, cracking insulators then we don’t really trust the instrument.\nThe third interesting thing is the values True and False. These are “Boolean” values, and when we put them into the “trustworthy” variable then it takes on the Boolean type. There are only two values, True and False. The capitalization is important.\nThe fourth thing to notice is that we’re sending two values into the print statement and it’s printing both of them. In general, we can give the print statement any number of arguments, separated by commas, and it will print all of them separated by one space."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#conditional-aka-relational-operators",
    "href": "newsite/theme1/PE100/index.html#conditional-aka-relational-operators",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Conditional (aka Relational) Operators",
    "text": "Conditional (aka Relational) Operators\nThe conditional test in each part of an if statement is an expression that results in a Boolean value. So far, the only conditional operator (or relational operator) we’ve seen is ==. There are others, though. For the sake of completeness, I’ll include == here:\n\n\n\noperator\ntested condition\n\n\n\n\n==\nequals\n\n\n!=\nnot equals\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal\n\n\n\n“Relational” has at least two meanings in computing. Relational Operators have nothing to do with Releational Databases.\n\nTry This\nFor each of the following code cells, decide what the result is, run the cell, and see how you did:\n\n\nTrue\n\n\n\n\nTrue\n\n\n\n\nFalse\n\n\n\n\nFalse\n\n\n\n\nFalse\n\n\nRelational operators also work with strings.\n\n\nequals Alice.\nThe person is not Bob.\nAlice comes before Bob in alphabetical order.\nAlice comes before or in the same place as Alice in sorted order\nWorking left to right, the M, the a, and the r match on\nboth strings, but when we finally get to the y and the k, y comes\nafter k in alphabetical order.\n\n\nA couple words of caution: the comparisons are based on the ASCII codes for each character. The “A” in ASCII stands for “American”, and as you might expect that means it only works for English language text. If you need to handle other languages, even potentially, then there is a better way to do it and we’ll see that in the lesson on strings.\nAlso, Capital letters are always less than lowercase letters, and not in the way you might think. “A” is less than “Z”, as you might expect, but “Z” is greater than “a”. The numbers 0-9 are the lowest of all. Punctuation is sprinkled around and the only way to know for sure is to look up “ASCII Chart”."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#code-blocks",
    "href": "newsite/theme1/PE100/index.html#code-blocks",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Code Blocks",
    "text": "Code Blocks\nLet’s go back to that part about running several lines of code but they have to be indented the same amount. Python always runs “blocks” of code. That block might be as short as one line:\n\n\n125.6636\n\n\nor it might be arbitrarily long:\n\n\n1511396.1762899999\n\n\nWhether it was the one line example or the eight line one, Python will set out to run all of those lines in one shot, and as long as there aren’t any errors it’ll do it. These are known as code blocks.\nThe decision structures (again, also called control flow statements) in Python all do basically the same thing: they evaluate an expression and depending on whether it turns out True or False, they execute a code block in some manner. This means that wherever we can have a single line of code running in a decision structure we can have as many lines as we want.\nTake a look at the following example. For the four possible combinations of potentially_hazardous and explody, decide what would be printed out. Then try out the combinations and make sure you know why each combination was handled the way it was.\n\n\nTotal Available Kaboom (TAK) to ruin your day is 1511396.1762899999\n\n\nDid you notice potentially_hazardous and explody? and is a boolean operator. We’ve seen the arithmetic operators already (+, -, *, /, etc.) and now here are the boolean operators. They’re named after Boolean algebra, the algebra of logic, and are used to make larger logical expressions from smaller ones. There are three boolean operators: and, or, and not.\nThe and operator evaluates to True if both of its arguments are True. The or operator evaluates to True if either or both of its arguments are true. The not operator takes only one argument and reverses it: not turns True into False and False into True.\n\n\nDoctor of Medical Dentistry (DMD)"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#coming-up-next-loops",
    "href": "newsite/theme1/PE100/index.html#coming-up-next-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Coming Up Next: Loops",
    "text": "Coming Up Next: Loops\nAt this point, we’ve seen the most basic way to alter the flow of control in Python: the if statement. We can write Python code to solve non-trivial problems now, but there are still some things we need in order to use Python as a truly general-purpose language. In the next notebook we’re going to make our code do something over and over."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#while-loops",
    "href": "newsite/theme1/PE100/index.html#while-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "While Loops",
    "text": "While Loops\nThe syntax of a while loop looks a bit like an if statement. Take a look:\n\n\nLooking at instrument number 1\nand then maybe we'll look at the next one.\nLooking at instrument number 2\nand then maybe we'll look at the next one.\nDone with all that looping.\n...and ready to do something else now.\n\n\nHere’s what the above code does. First, it creates a variable named “instrument” and sets it to 1. Then it goes into the while loop. The first time through, it checks to see if instrument is less than or equal to 2. It is (because we set it to 1 just a moment ago) so the while loop will execute the code block. This block prints out two lines and then it adds 1 to instrument. That means instrument now equals 2.\nThe second time through the loop, instrument equals 2. That satisfies the conditional statement of the while loop (2 is less than or equal to 2) so the code block runs again. Two more lines are printed out and then instrument is incremented one more time.\nThe while loop runs for a third time now. This time, 3 is not less than or equal to 2, so the conditional statement is false. This means the while loop is done - it won’t run its code block again, and the flow of control will go on to the next line after the while loop. It will run the two print statements explaining that the looping is over and it can go on to other tasks.\nLet’s look at another example. Let’s print out all the powers of two that are less than 928.\n\n\n2 to the 0 equals 1\n2 to the 1 equals 2\n2 to the 2 equals 4\n2 to the 3 equals 8\n2 to the 4 equals 16\n2 to the 5 equals 32\n2 to the 6 equals 64\n2 to the 7 equals 128\n2 to the 8 equals 256\n2 to the 9 equals 512\n2 to the 10 is too big.\n\n\nDid you notice I sneaked something in there we haven’t talked about yet? See the “#” character on the line with the while statement? That indicates the rest of the line is a comment. Python will totally ignore it. It’s handy for leaving little notes to yourself, like “why did I choose 928 there when I could have put 944?” This is very, very important when writing full-fledged, standalone programs. If you don’t leave some notes for yourself, you’ll never remember what you were thinking when you go back to that code six months from now. Also, the next person who comes along and has to change something in your code will greatly appreciate the hints.\nLeaving comments in the code isn’t as big a deal in Jupyter notebooks… you can write rather substantial notes in a Markdown cell complete with boldface, italics, and whatever other fanciness you desire. On the other hand, it’s also nice to be able to leave your comments in the just the right place in the code so it flows effortlessly through your comprehension as you read it. Let experience and personal opinion be your guide here."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#reading-information-from-the-outside-world",
    "href": "newsite/theme1/PE100/index.html#reading-information-from-the-outside-world",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Reading information from the outside world",
    "text": "Reading information from the outside world\nNotice that in both of those cases, we actually did know how many times the loop would run. We know that 2 to the 9th is 512 and so we know the while loop will only run that far. In fact, in every example we’ve had so far we’ve know what the output will be because we always have the same inputs. Computer software wouldn’t be terribly interesting if it could only run specific, known, canned inputs. Fortunately, Python gives us several ways to bring data into our programs.\nThe simplest way to bring data into a Python program is to edit the program and change the values we assign to variables. This is sort of the reducto ad absurdum method, but honestly it isn’t a bad way to handle very small amounts of input. It’s even easier in Jupyter notebooks since the code is just sitting there looking at us, waiting to be edited. For values that aren’t going to change very often (your name, perhaps, or the chargeback account number for using some instrument, for instance) then just assigning a value to a variable and editing it every once and a while is a fine way to go.\nAnother way to get data into a Python program is to read it in from where the user is running the program. For doing this, Python provides a function called “input” which takes an optional argument, specifically a string that is printed as a prompt. Python then waits for the user to type something as a response. When they do, that string is returned to the calling program. Here’s a simple example:\n\n\nPlease enter your name Erik\n\n\nHello, Erik\n\n\nWhen the above code runs, the prompt “Please enter your name” is displayed right below the code cell and a text entry box is placed beside it. When you enter your name, it greets you.\nIf we were running this tiny little snippet of code as a regular program, the interaction would be in the terminal emulator window that we ran the program in. Because this is running in Jupyter, though, the interaction is directly in the notebook. The prompt and the entry blank occur just below the running code cell.\nWhat will happen when we run the following?\n\n\nEnter a number between 4 and 8 5.25\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [4], line 2\n      1 response = input(\"Enter a number between 4 and 8\")\n----&gt; 2 new_value = response + 6.5\n      3 print(new_value)\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\nWow! Python couldn’t run that and it “threw an error”. We’ll examine Python’s error handling facilities later, but for now we’ll just assume that means it came to a screeching halt. Looking at the error message, it seems there is some problem with trying to add a real number (a floting point number) to a string."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#type-casting",
    "href": "newsite/theme1/PE100/index.html#type-casting",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Type Casting",
    "text": "Type Casting\ninput() prompts the user and returns the string they entered, but what if we want the user to enter a number? What do we do then? The answer is we’ll use a process known as type casting. The act of type casting is no more than converting information from one type to another.\nThere are three very useful functions for type casting: int(), float(), and str(). Let’s see them in action…\n\n\n16.454\n34543456\n4\n\n\nWhat did the above do? First, it converted the string “9.9” (literally, three characters… it’s a string) to a “float” (a floating point number, some languages will call that a real number). The second example takes a string of 8 characters and interprets them as an integer. That value is what gets returned and stored in our variable. Finally, we copmute the number 4 by adding 2+2, and then we let the str() function convert that to a single character long string having just the character “4”.\nBy now we know enough to be able to ask the user for a number and get something back that we can actually do math with.\nThere’s an even easier way, though. Just like function composition worked when you took precalculus, the results of a Python function can be used as the argument to another. Hence:\n\n\nEnter a number between 17 and 34 26\n\n\n26.0\n\n\nSometimes, function calls can be nested really deeply. Personally, when it comes time to debug code like that I find myself printing it out and coloring each level with a different highlighter pen."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#putting-it-together-while-loops-to-get-user-input",
    "href": "newsite/theme1/PE100/index.html#putting-it-together-while-loops-to-get-user-input",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Putting it together: while loops to get user input",
    "text": "Putting it together: while loops to get user input\nThe great thing about a while statement is that it can loop zero times, one, two, or twelve trillion. Best of all, we don’t have to know how many ahead of time. We could do the following:\nprint(“Computing an average.”)\nsum=0.0 counter=0 data_point = float(input(“Enter a number, or enter negative num to stop”)) while data_point &gt;= 0.0: sum = sum+data_point counter = counter+1 data_point = float(input(“Enter a number, or enter negative num to stop”))\nprint(“Average value is”, sum/counter)\nWhen we run the code above, we’re prompted to keep entering numbers until we finally enter -999. Each time it goes through the loop it keeps track of the running total of the numbers and the count of how many numbers have been entered. Once it’s done, it divides the total by the count and displays that as the average.\nLet’s step through what happens when the user enters 1, 2, 3, and -999: 1. The sum and counter variables are initialized to zero. 1. The user is prompted to enter a number, possibly a negative number to indicate no more data, and that input is type cast to a floating point number. 1. The while loop’s condition will be met any time a positive number was input (greater than or equal to zero). 1 is a positive number, so run the loop body. 1. This first time through, we’ll add the 1 that was input to our running total, which is now 1. 1. And increment the count, now equal to 1. 1. AND PROMPT THE USER FOR ANOTHER NUMBER!!! 1. Back at the while statement again, we check the condition and, yes, 2 is a positive number, so we run the loop’s code block. 1. Update the sum and count, and then… 1. PROMPT THE USER FOR ANOTHER VALUE!!! 1. Running the while statement again, the user entered 3, and 3 is positive, so the clode block will be executed. 1. Update the sum (now 6) and count (now 3). 1. Prompt for another number 1. Back at the while statement, we check and see that -999 is not a positive number, so we skip the code block and resume by running whatever follows it. 1. Having exited the while loop entirely, print out the average value by dividing sum/count.\nAll the boldface and all-capitals lines above are there to emphasize how important it is to make sure your while loop isn’t just checking the same thing over and over. If we didn’t get a new number from the user each time through, the value of data_point would never change. That would result in an infinite loop, causing Python to never be able to complete the code in that cell. If it ever happens to you, and it probably will, the “Interrupt Kernel” command on JupyterLab’s Kernel menu will stop the looping and let you get back to work.\nThe while loop is certainly versatile… it can be used any time you need to do something repeatedly. If you know how many times you need to have the code block execute, either when you write the code or when it’s running, then keep a variable that is incremented in the block every time and exit the while loop when the counter hits the right number.\nWhere while loops really shine is when it’s impossible to know ahead of time how many times the code block should run. The example above, where we keep accepting numbers until the user signals there aren’t any more, there’s no way to know how many times to execute that loop until we see a negative number. In a case like that, the while loop is the only practical solution.\nSo if while loops are so great and solve every problem, why do we need anything else? The big reason is expressiveness: they can be a little awkward to understand, especially when you’re looking at someone else’s code. Having the conditional test separated from the action that establishes when to stop makes it a little awkward to understand (or debug!) someone else’s code. This is especially true when we need to step through something by unusual increments.\nSo what are we to do in these cases?"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#for-loops",
    "href": "newsite/theme1/PE100/index.html#for-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "For Loops",
    "text": "For Loops\nThe for loop is quite similar to the while loop. The difference is that for loops are controlled by a count whereas while loops are controlled by a condition.\nLet’s start with an example.\n\n\n1\n2\n3\n\n\nThat is the simplest for loop you’ll see. Let’s look at the pieces. 1. The for statement itself 2. The name of the target variable whose value will be changing as the loop runs (“the_value” in this case”) 3. “in” - and if this reminds you of set membership then you’re on to something 4. “range()” - this is an example of an iterable, which means “something that can be stepped through”. 5. The colon… the one I forget 50% of the time. 6. The code block, in this case just a print statement.\nMost of the time, fairly close to “always”, the code block will take advantage of the target variable changing each time through. In our example, “the_value” is our target variable, as it loops through it will take on the values 1 through 3, and the code block has a print statement that uses it.\nBefore we examine the range() function, let’s take a look at another iterable. We’ll talk about lists in a later lesson, but for now we can just wave our hands around and understand enough for the moment.\n\n\nThe sample weighed 143.6 grams.\nThe sample weighed 141.9 grams.\nThe sample weighed 139.4 grams.\nThe sample weighed 144.23219 grams.\n\n\nYou can use the target variable as many times as you want to in the code block.\nNow let’s take a more detailed look at the range() function. In its most basic form it takes one argument - the stop value.\n\n\n0\n1\n2\n3\n\n\nThis single-argument form starts at zero, counts up by one each time, and doesn’t include the stop value. This is different from every other programming language you’ll ever encounter. It’s just one of those things.\nWe’ve already seen the two-argument form. It takes a starting value and a stopping value, and iterates by one from the start until the last value that is less than the stop.\n\n\n7\n8\n9\n\n\nAnd there’s even a three-argument form. The third argument is the amount to step by.\n\n\n12\n15\n18\n\n\nThe step size doesn’t have to be a positive number…\n\n\n6\n4\n2\n0\n-2\n\n\nIn case you’re curious, the step size cannot be zero. If you really want an infinite loop, and there are cases where it makes sense, you have to use a while loop instead.\nAs a general rule, any place where you can use an explicit value (a literal) you can use a variable. Arguments to a for loop are no exception:\n\n\nwhere should we start?  13\nwhere should we run right up to and stop just short of it?  15\nwhat should we step by?  2\n\n\n13\n\n\nIf we need to do something a specific number of times, we need to pay attention to our starting and stopping conditions. I’ve messed this up so many times I know now to be careful. You’ve been warned.\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\n\n\nThey add up to 9\n\n\nNotice something wrong? If you ask it to total 3 numbers, it only prompts for two of them. There are a couple of ways to solve this. The easiest is to just use the one-argument form of range().\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nThat offers a little insight into why Python has it’s funny “up to but not including” semantics: zero is a perfectly legitimate number and a very natural starting point.\nThe only problem with the single-argument method is that the values that the target variable goes through include zero. This may or may not be a problem if that value is used inside the code block. If you really need to count from one instead of zero, you can increment the stopping value:\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nAnd that behaved just like we expected.\nYou may have noticed a pattern already. We frequently need to compute a new value for an existing variable. What we’ve done so far has been along the lines of grand_total = grand_total + new_reading. Python gives us a shorthand way to write that. We could instead express that as grand_total += new_reading. There is no space between the plus and equals signs. The only reason this exists is to save you some typing. As you might expect, there are a few more of these Augmented Assignment Operators…\n\n\n\nOperator\nExample\nEquivalent\n\n\n\n\n+=\ncount += 1\ncount = count + 1\n\n\n-=\nx -= offset\nx = x - offset\n\n\n*=\nproduct *= val\nproduct = product * val\n\n\n/=\ny /= 3\ny = y / 3\n\n\n%=\nval %= 2\nval = val % 2\n\n\n\nOut of all of them, += is far and away the most commonly used one."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#nested-loops",
    "href": "newsite/theme1/PE100/index.html#nested-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Nested Loops",
    "text": "Nested Loops\nYou know what’s fun to put in a loop’s code block? Another loop! Best of all, it comes in pretty handy when dealing with high-dimensional data. Plenty of algorithms rely on nested loops, too. Take a look at this:\n\n\nx= 0  y= 0\nx= 0  y= 1\nx= 0  y= 2\nx= 0  y= 3\nx= 1  y= 0\nx= 1  y= 1\nx= 1  y= 2\nx= 1  y= 3\nx= 2  y= 0\nx= 2  y= 1\nx= 2  y= 2\nx= 2  y= 3\nx= 3  y= 0\nx= 3  y= 1\nx= 3  y= 2\nx= 3  y= 3\nx= 4  y= 0\nx= 4  y= 1\nx= 4  y= 2\nx= 4  y= 3\n\n\nWhat’s going on here? Initially, the outer loop, the one that iterates zero through four and assigns it’s value to x, runs. When it starts running its code block for the x=0 pass, the for loop for the y variable starts. ‘y’ assumes the values 0 through 3, so the first four lines printed out are for x=0, y=0, then x=0, y=1, and so on through x=0, y=3. Once that inner for loop completes, the outer for loop gets to iterate again. Now the inside for loop runs again, only this time we have x=1. That’s why the next four lines are “x=1, y=0” through “x=1, y=3”. Every time the outer loop runs another iteration, the inner loop gets to run all the way from start to finish.\nIn later lessons, we’ll have a few opportunities to play with nested loops. In fact, we’ll get to do that in the very next lesson: Functions!"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#encapsulation",
    "href": "newsite/theme1/PE100/index.html#encapsulation",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Encapsulation",
    "text": "Encapsulation\nFunctions are useful in programming for the same reason they’re useful in math - ours encapsulate a chunk of code so you don’t have to think about what is in it every time. Imagine how tedious it would be to write a program that needed to compute cosine in a lot of different places in the code. You could, I suppose, type in a Taylor series expansion for cosine in each of the places where we need to compute a cosign. That would be irritating, error prone, and confusing to anyone else who has to read it. Instead, we can write a function exactly once to compute cosine and then call that function from many places in our code. Once we have the function tested and debugged, we don’t have to think about it again. That frees up mental energy for more productive uses.\nFunctions can be classified into one of two types. Void Functions exist for encapsulation and don’t actually return a value. print() is an example of a void function. Value-Returning Functions, as the name strongly implies, return a value to the calling code. inductiveReactance() is an example of one.\nHere’s another example. This time, we’ll define a function that calls another function.\n\n\nArea of a circle with a radius of 2 is 12.56636\n\n\nWe defined a function to compute the area of a circle. It needed to square a number and so we decided to write a function to do that. Functions can call other functions ad infinitum. In fact, functions can even call themselves! When that happens the function is said to be recursive. Recursive functions are very useful for solving some hard problems but they’re a little beyond an introductory module like this one."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#function-and-variable-naming",
    "href": "newsite/theme1/PE100/index.html#function-and-variable-naming",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Function (and Variable) Naming",
    "text": "Function (and Variable) Naming\nWhat kinds of names can we use for functions? The same ones we can use for variables! More specifically, * No keywords (e.g., False is invalid) * No spaces (e.g., my function is invalid) * The first character must be: * a-z, A-Z, or _ (the underscore character) * No numbers (e.g., 1st_function is invalid) * After the first character, the following are allowed: * a-z, A-Z, _, and 0-9 * No other symbols (e.g., get_room&board is invalid)\nAs a widely agreed upon best practice, names should be meaningful and be composed of lowercase characters with underscores as separators."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#function-arguments",
    "href": "newsite/theme1/PE100/index.html#function-arguments",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Function Arguments",
    "text": "Function Arguments\nInput Parameters to functions are called arguments. They are the primary and best way to put information into a function, and definitely the way that causes the fewest problems. Arguments to a function in Python are mostly analagous to what we’re used to in math, but of course Python has some extensions.\nA function can have any number of arguments, including zero. “A function of zero arguments” might sound like a mathematician’s idea of “humor”, but it can actually make sense in programming. Sometimes you just need to encapsulate part of your code so you don’t have to worry with it again. For instance:\n\n\n==============================\n==============================\nGreetings, User. I'll start \nloading the instrument config\nfiles and opening connections\nto them. It'll take a minute.\n==============================\n==============================\n\n\nNow the code to print that banner is hidden away inside a function we’ll never have to look at again. Less mental clutter means fewer bugs.\nAnd for the sake of completeness, functions can also take one or more arguments:\n\n\nThe race was 6.213712 miles long and my ankles were hurting the ENTIRE way.\nThe polynomial evaluates to: 1284.04\n\n\nWhen arguments are passed into a function, they become parameter variables and can be referred to inside the function just like any other variable. This handy because the variables inside a function are called local variables and they have special properties: nothing outside of the function can modify their value, they’re destroyed and re-created every time the function is called, and these local variables supercede any outside variables with the same name.\nTake a look for yourself:\n\n\nTwice the wavelength is 40\nTwice the wavelength is 40\nTwice the wavelength is 40\n\n\nDoes that seem odd to you? What happened is this: four lines from the bottom we created a variable named “wavelength” and set it to 20. We then called the function to print it out doubled. We passed the global variable “wavelength” to our function which took it as its only argument. That argument became a parameter variable that was coincidentally named “wavelength”. That “wavelength” parameter variable has nothing to do with the “wavelength” variable in the main part of the program. Our function doubles that parameter variable and prints it out. At that point, the function completes and the flow of control goes back to the main body.\nThe next time our function is called an entirely new, fresh set of variables and parameter variables is created. This is important - it means that if we call the function with the same value every time then we always get the same result. Functions are unable to save their “state”. Like a football player on a stretcher, they have no memory of what happened before.\n(OK, yes, there are ways for them to save their state. Sometimes it’s unavoidable and you just have to do it, but doing so makes more places for bugs to creep into your programs and makes it harder to understand later. Try to avoid it. We’ll talk about it later.)"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#variable-scope",
    "href": "newsite/theme1/PE100/index.html#variable-scope",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Variable Scope",
    "text": "Variable Scope\nThe degree to which your programs can “see” a variable is called scope. There are two levels of scope in most Python programming:\n\nGlobal Scope\n\nDefined in main Python file\nOutside of ANY function\nTry to avoid these!\n\nConsidered poor design\nDangerous to use: any part of the program anywhere can change these\nBug Magnet!\n\n\nLocal Scope\n\nVariables defined within a function\nOnly visible and useable from inside their own function!\nUse these if at all possible.\n\n\nThe danger in global variables comes from two things. The first is the fact that the value can be changed anywhere in your program, either in the main program or inside of a function, and it’s devilishly hard to keep track of where that might be.\nThe second danger is more subtle. When a function saves a value into a global variable, the function is now said to have side effects. Side effects break the idea of isolation that functions are meant to give us. Imagine a mathematical function, such as tangent, if it had side effects. Calling tan(.0125) would not only result in the tangent of .0125, but it would have some other effect on some unrelated part of math. Imagine if calling tan caused your coordinate system to change every time? That would be insane.\nIt gets worse, though. What if our tangent function also read from a global variable and changed its behavior based on that. Then each time we called tan(.0125) we might get a different value.\nIn other words, we basically broke math.\nSimilarly, when we write programs, if our functions have side effects then we’ve complicated them tremendously. And more complication means more places for bugs to sneak into our code and they’ll also be harder to find.\nAs an aside, there is a style of programming that eliminates global variables and, to an extent, even local variables. It’s called functional programming, and Python has some support for that style. There is usually more than one way to do anything in Python, and experienced Pythonistas will usually try to choose the most Pythonic way. Part of being in Pythonic style means to use (at least partially) a functional style."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#constants",
    "href": "newsite/theme1/PE100/index.html#constants",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Constants",
    "text": "Constants\nThere is an exception to the “no globals” rule: Constants. Just like in math, a constant is given a value once and never changed again. “Never changing” means “no side effects” so everything is OK. It is good practice to define your constants using ALL CAPITAL LETTERS.\n\n\n1.9878e-19"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#abstraction",
    "href": "newsite/theme1/PE100/index.html#abstraction",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Abstraction",
    "text": "Abstraction\nA valuable property of functions is how they isolate the code and variables inside of them from being manipulated elsewhere in your software. A consequence of that is their ability to “hide” detail from us. We’ve already talked about writing a function, debugging it, and never having to look at the code inside of it again. What is every bit as useful, if not more so, is using functions to provide abstraction.\nAbstraction is something we’ve used every day even if we haven’t thought it. Remember learning math? You started off counting things, and yes, that counts as math. If you had four bottle caps in one hand and three in the other, you could toss them all on the table, count them, and know that you have seven in total.\nThere are two problems with having to count everything. One is that the amount of stuff can get big in a hurry. Try using two hands and table to count sand grains. The other problem is that if there are any insights to be had, it’s hard to find them when you’re stuck down in the details. Fortunately, we learned arithmetic.\nArithmetic is great. We don’t have to deal with handfuls of stuff anymore. We can just use numbers and operators and get an answer without a bunch of messing around. We can start to see patterns we never would have just tossing bottle caps on the table. If we need to add 12 to something, we can instead add 10 and then add 2 more. This is so handy. Of course, it would be nice if we could just do something to analyze entire families of arithmetic problems.\nAlgebra lets us analyze entire families of arithmetic problems. We don’t have to fool with numbers if we don’t have to - we can just substitute variables in their place. We’ve hidden some of the complexity, like the petty little details of numbers, and abstracted that complexity away.\nSimilarly, a lot of problem solving is perfectly amenable to using abstraction. Let’s write a bit of code to run an experiment…\nThat function is a (admittedly fanciful) representation of running an experiment. It makes sense, anyone can understand it, and if there’s a bug in there then it’s going to be really obvious. The only problem: if we try to run it, it’ll crash because those other functions haven’t been defined yet. Shall we fix that?\nNotice how the program is broken up into several functions? The best part is that you don’t have to keep everything in your head. All you have to remember is the part you’re working on. Smaller pieces, fewer bugs."
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#modules",
    "href": "newsite/theme1/PE100/index.html#modules",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Modules",
    "text": "Modules\nOne reason Python has become so popular is the sheer amount of code that has been written in it and made available for public use. We’ve seen a few functions already that were built in to Python - int(), float(), and str(), for example - but there are many tens of thousands of modules that are freely available for use in your own software. Just picking five common ones at random:\n\nmath\nrandom\nos\nPyMySql\npsycopg\n\nThe first two contain functions for general-purpose math and for producing random numbers. The “os” module interfaces Python with the operating system the code is running on. PyMySql and psycopg provide connectivity to relational databases.\nRemember at the beginning of this lesson when we wrote a function to calculate inductive reactance? I put the value of pi in there as 3.14159, but that really isn’t anywhere near enough digits for some problems. Let’s fix that:\n\n\n3.141592653589793\n\n\nThere are two things to note here. First, the keyword import is used to tell Python to go find a module with the right name and load it. The name we want it to find is the word right after the import. And secondly, just looking at the output we can see that there are a lot more digits than when we did something by hand in our Inductive Impedance example (top of this page). In general, using a module that was (a) written by someone else and (b) is widely used and has been checked by a lot of people is going to avoid a lot of bugs. For instance, I would never code my own Fast Fourier Transform. Instead, I would use the one in the “numpy” module. I know how easy it is to make a mistake and I trust their work a lot more than my own. They have tens or hundreds of thousands of users and scores of developers. I have… a copy of Numerical Recipes that’s old enough to run for President.\nSince we used the “math” module already, here’s a very incomplete list of what is in there: * sin(), cos(), tan(), acos(), asin(), atan()… - “acos” is “arc cosine”, etc. * log(), log10(), sqrt() - square root * radians(), degrees() - converts between them\nAnd lots more stuff. How do you know what’s in it? Go to the online documentation: https://docs.python.org/3/library/math.html\n\nRandom Numbers\nAnother module that is heavily used is “random”. It generates random numbers, yes, but it can also do things like take a list of things and shuffle them randomly.\n\n\nThe random integer between 10 and 100 was: 20\nThe random float between 0 and 1 is: 0.6474502367565016\n\n\nThere are more functions available in the “random” module, including ones to select a real number from a non-uniform distribution. Take a look at https://docs.python.org/3/library/random.html\nHere’s a slightly more complicated example:\n\n\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nProposal number 23 was funded!\nDon't feel bad... proposal number 24 didn't get funded either.\n\n\n\nLet’s try out what we’ve learned so far. Use the next code cell to write a bit of Python that simulates rolling a pair of dice and adds the two values. Print the value out.\nLet’s add to that… add a loop so that we keep doing that over and over until we get the same sum twice in a row. Some questions to ask yourself are “What kind of loop do I need?” and “How can I compare what happened between two different loop iterations?”"
  },
  {
    "objectID": "newsite/theme1/PE100/index.html#files",
    "href": "newsite/theme1/PE100/index.html#files",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Files",
    "text": "Files\nPractically everyone is more-or-less familiar with the idea of a file, even if fairly few people know how they work. We’re going to ignore a lot of details for the moment and say this: a file is a long-lasting collection of bytes. It has a first byte, a last byte, and every one in between stays in the same order.\nThis begs the question “What is a byte?” A byte is just a small number from 0 to 255 (inclusive). We can assign meaning to those numbers, and if we’re smart about how we do it then we can represent any information a computer can process as long as we use enough of these bytes.\nWe like to think of files as being one of two types: binary files and text files. Binary files are pure data. We decide how to write bytes to a file to represent data. Then when we’re ready to read it in again, we read the bytes, process them somehow, and reconstruct the original data. It’s a great technique - it’s fast and efficient.\nWe won’t be talking about binary files in this notebook or even in this module. Fifteen years ago we wouldn’t have had a choice, we would have had to. These days, it’s unusual to have to deal with binary files, especially in Python, because there is so often a library function already available to do the work for us.\nText files, on the other hand, are probably something you’re already familar with - they are what you get when you edit a “plain text” file in “notepad” or “textedit”. In a text file, every one of the letter, number, and punctuation mark characters is assigned its own number. For instance, capital “A” is 65. “B” is 66. Not that it should ever matter, but here’s a complete list and then some!\nLet’s say you open an editor and type “CAT”. When you save that to a file, there will be a file that is three bytes long and contains the three bytes 67, 65, and 84. Actually there will usually be a fourth byte, 10, which is the character you get when you press “Enter” or “Return”.\nFor now, at least for a few minutes, we’re going to pretend the only language on earth is English. We’ll talk about other languages when we talk about networks.\nIt’s about time for an example, don’t you think?\nThree lines of code was all it took to create a file, write to it, and tidy up after ourselves. What does each of those lines do?\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object is an object variable. Think of an object as a way to store data in a variable along with some functions that only make sense to that data. They hide a lot of complexity from us. A file object is one that keeps track of a filename, how to get to it, and how to use it. It has some functions built in to it to help us do things to the file.\nPython gives us the function “open”. It gets a file ready to be used by our code. It takes two arguments. The first is the file’s name, and the second is the mode we want to use the file in. In our example, we specified that the file’s name was “first_file.txt” and that it was in the “/tmp” directory. Then in the second argument we specified “w”, meaning we wanted to write to the file. The “w” mode will cause the file to be created if it didn’t already exist. If it did already exist, on the other hand, all the contents of it will be deleted and we’ll start writing from the beginning just as if the file was created from scratch. We’ll see more modes as we go.\nmy_file_object.write(\"First Post!\")\nThis line uses one of those functions that are tucked away inside an object. In this case, we’re calling the file object’s “write” function. It does what we expect - it takes its argument, in this case “First Post!”, and causes it to be written to disk byte by byte.\nmy_file_object.close()\nFinally, we call one more of the file object’s functions: close. When we run this, Python tells the operating system “Hey, we’re done with the file. You can get rid of any of the tedious housekeeping data that operating systems keep behind the scenes!”\nClosing files is considered “good programming hygene”. You’re allowed 1024 file objects to be open and connected to files in one program on the CLASSE cluster of computers. I’ll say from my experience: if you think you need that many, you’re probably doing something the wrong way.\nWriting files, then, is fairly easy. What about reading files? I’m glad you asked.\n\n\n'First Post!'\n\n\nYou can probably tell mostly how that worked just by looking at. We used the open() function again, but this time with a “r” for our mode. This means “read”. Also, this time we used read() instead of write(). The read() function reads in an entire file and saves it a string variable. Finally, we call close() again to close the file and tidy up after ourselves.\nNote that if the file is, say, 500 megabytes long, the string variable is going to be very, very large - roughly half a gigabyte. Python can handle this, but it may not be terribly convenient. If the file is more than 100-200 gigabytes, the CLASSE servers are probably not going to be able to handle. I say “probably” because there are a lot of factors at play.\nJust writing one line to a file is probably not very useful. Let’s try writing two lines:\nWhen we run that, it will open /tmp/first_file.txt for writing and it will delete anything already in it (that’s what the “w” means, remember?). Then it will write “First line written.” and “This is my second line.”.\nLet’s read the file again and prove to ourselves that it worked…\n\n\n'First line written.This is my second line.'\n\n\nOh no! The two lines ran together!\nAnd that is one of the first differences we’ll see between write() and print(). Print() always adds a newline character after it prints out anything. Remember when I said there would usually be a byte at the end of a line, represented by the number 10? This character is called “newline” and it, as the name implies, marks where a new line starts.\nIn all likelihood, when we do two write() statements like we did, we want to put a newline character in the file to make it into two lines. Fortunately, there are several ways to do that. Here are two of them.\nThe first way is simple and direct - call write() three times instead of two and put a newline in there “by hand”, as it were:\n\n\n'First line written.\\nThis is my second line.'\n\n\nThe output looks a little strange. We put an extra write() function call, but we gave it an odd looking argument - . That is a backslash (usually between the Enter and the backspace keys on a US keyboard) immediately followed by a lowercase “n”. The combination together means “newline character”. This much is fairly straightforward.\nNext we read the contents of the file. This is just like before.\nFinally, and this is where things take an unexpected turn, we evaluate the_contents and let Jupyter print that out for us. And when Jupyter does that, we see the “” there. It seems like Python didn’t convert those two characters to a newline, just sticking them in there as-is, and still left us with one long line. But is that true? Has Python foresaken us?\nRun the code in the next cell:\n\n\nFirst line written.\nThis is my second line.\n\n\nSalvation! print() did the right thing. This is a key difference between just typing a variable or an expression at the end of a cell and letting Python evaluate it versus putting a print() in there and having absolute control over what gets sent to the notebook and on to the screen.\nThis also illustrates something else important and useful: all of the code cells in this notebook are being run by the same Python “interpreter”. This means if we set a variable to a value in one cell, we will see the same value stored in that variable in other cells. That’s how we were able to print what was stored in the_contents in the cell above even though we had set its value to the file contents two cells above that.\nIf a file only has a line or two, it’s not a big deal dealing with that with string functions. If a file has millions of lines, then it becomes a bit of a hassle. We need a way to read a file one line at a time. Fortunately, there’s readline():\n\n\nFirst line written.\n\nThis is my second line.\n\n\nThis does almost what we expect: it reads both lines from the file, one at a time, and prints them out. The only snag is that blank space between the lines. What has happened? It turns out readline() reads the entire line, even the newline character at the end. We can see this if we evaluate the string instead of just printing it:\n\n\n'First line written.\\n'\n\n\nThere’s that \\n again! What about the second line?\n\n\n'This is my second line.'\n\n\nWhen readline() reads a line, it includes the newline character at the end unless it reaches the end of the file and the file didn’t end with a newline.\nIt’s rare that we would want to read a bunch of lines in a file with the newlines included. That’s just not something we do very often, and practically never in scientific software. We’ll almost always want to trim off the newline character. And for that, we have the rstrip() function. It takes a string, strips off any newlines on the right side of it, and returns that cleaned-up string. rstrip() does that for the right side of the string, lstrip() cleans up the left side (the beginning of the string) and strip() goes crazy and does both ends at the same time.\nLet’s try it:\n\n\nFirst line written.\nThis is my second line.\n\n\nWhat’s going on here? A couple of things. The first thing to note is that rstrip() and its close companions lstrip() and strip() take one argument, which is the character to be stripped. Practically always we’ll want to get rid of the trailing newline character.\nThe other interesting things is how we called the rstrip() function in the first place. We gave the name of the string variable, a period, and the name of the function we were calling. This is just like how we called the close() function on a file object. And in fact, strings are another kind of object in Python. We’ll see a lot more on this later.\nHistorical note: The original programming language that had objects was named “Smalltalk”. In Smalltalk, the functions that were inside of objects were called “methods”. You’ll still hear people call them that. Later, the “C++” language came along and it called methods “member functions”. When programmers talk about the functions that are contained in objects, we’ll use either term interchangably, sometimes even switching in the middle of a sentence. We now return to your Python tutorial, already in progress…\nWe read both lines in the file we created. We were able to call readline() twice and know that we had all of our lines in the file because (1) we created the file ourselves and (2) we therefore knew it had precisely two lines. It wasn’t even too bad having to type those readline() and rstrip() lines twice. But what if we had a lot more lines? We would certainly want to use a loop.\nFor example, what do we do with a five-line file?\n\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nNo problem - we just use a for loop and do the readline() inside of it. It repeats the five times we asked for. In this case, after we read each line we cleaned it up a little and printed it.\nBut what if we can’t know the number of lines ahead of time? One approach is to have whatever program that creates the file write the number of lines that will be in it first. I won’t say this is a common approach in scientific software, but it isn’t exactly rare either.\n\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe overall scheme for this is probably obvious by now. In the first half, when we’re writing the file, we write a “5” on its own line, and then write five more lines. In the second part, we 1. Read the first line. 2. rstrip() to get rid of the trailing newline 3. Use the results of that as the argument to int(), converting that string (“5”) to an actual integer (5). 4. and finally go through a for loop that many times just like before\nMost of the time we won’t have the luxury of knowing how many lines are in a file, though. We need a way to read all of the lines, line by line, without limit. For that, we can loop through the file and quit when Python returns an empty string with not even a newline character.\n\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe while loop behaved just like we expected - strat by reading a line, and then every time the line isn’t empty, print it out and read another line. When you finally hit a line that is completely empty, exit the while loop and close the file.\nLooping through a file all the way to the end is such a common thing to do, Python has a shortcut for doing it. Remember when we talked about a for loop iterating over an ordered set? A file can be thought of as an ordered set of strings. They’re not in alphabetical order, but rather they are ordered by line number. That means we can:\n\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nAs you can imagine, reading isn’t the only file operation you can do with a loop. You can also write to a file that way. For instance,\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\nFinally, we don’t have to erase the contents of a file every time we write to it. It’s perfectly normal to append to an existing file, and for that the “a” mode can be used with open().\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWhen you use the append mode, the write() calls will either add to the existing file or, if it doesn’t already exist, it will be created and then written to as though we used the “w” mode.\nSo far in this lesson we’ve acted like everything just works perfectly every time. In reality, it’s not that neat. Filenames get typed in wrong, didks get full, and lines that are supposed to be numbers might contain text instead. Any of these problems is enough to bring our Python code to a grinding halt. Our next lesson is all about how to handle these problems and many, many more like them. We’re going to learn about Exceptions!"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html",
    "title": "PE100-03: Decision Structures",
    "section": "",
    "text": "In the first lesson, everything we did was sequential programming. Statements are executed one after the other in exactly the order they’re written in. As long as there aren’t any errors, every statement will be executed."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#the-simplest-if-statement",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#the-simplest-if-statement",
    "title": "PE100-03: Decision Structures",
    "section": "The Simplest “if” Statement",
    "text": "The Simplest “if” Statement\nIn almost any real Jupyter notebook or standalone program we write, there will have to be places where different code paths are taken depending on what has happened leading up to there. Suppose we’re looking at absorption at one specific wavelength and we know that some of our instruments are a little bit too sensitive to changes in humidity. Maybe the first spectrometer has some insulation that is just a little too porous and reads a bit high, but the second one is even worse. We have calibration constants we can apply, but we have to apply the right constant for each individual instrument.\n\nspectrometer_number = 1                                                        \nreading = 7.00041                                                              \n                                                                               \nif spectrometer_number == 1:                                                   \n    useful_result = reading * 1.077                                            \n                                                                               \nuseful_result\n\n7.539441569999999\n\n\nHere we have the first Decision Structure (also called control flow statement) that we’ll look at. Taking the above code apart, we see several important things.\n\nThis is an “if statement”.\nTesting to see if two things are equal is done with two equals signs, not one (==). There’s a historical reason for this, and it’s a good reason, but it always trips up newcomers. You have been warned. You’re welcome.\nThe last character on the if line is : (a colon ).\nThe “body” of the if statement, the part that is run if and only if the tested condition is met, is indented.\n\nIn the case of the above if statement, what the code does is check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called “useful_result”."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#else-the-catch-all-specialist",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#else-the-catch-all-specialist",
    "title": "PE100-03: Decision Structures",
    "section": "Else: the catch-all specialist",
    "text": "Else: the catch-all specialist\nIf that was all an if statement could do then it would be really useful. But that’s not all it can do. We need to do something reasonable when we get readings from the second instrument. Such as:\n\nspectrometer_number = 2                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelse:                                                                           \n    useful_result = reading * 1.19\n\nuseful_result\n\n8.3304879\n\n\nHere we have added an “else clause”. The above code is interpreted as “check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called useful_result. Otherwise, set useful_result to whatever is saved in”reading” plus 19%.\nSo far, so good. But there’s more! Suppose we need to handle several of these not-quite-top-quality spectrometers. How do you suppose we could deal with that? We could resort to putting if-else statements inside if-else statements in sort of a brute force fashion…\n\nspectrometer_number = 3                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelse:                                                                           \n    if spectrometer_number == 2:                                                \n        useful_result = reading * 1.19                                          \n    else:                                                                       \n        if spectrometer_number == 3:                                            \n            useful_result = reading * .92                                       \n                                                                                \nuseful_result\n\n6.4403771999999995\n\n\nThe above code looks a little intimidating, but all there is to it is just a series of if statements. The logic of it goes like this: “If the instrument number is 1, then adjust it 7.7% and we’re done. Otherwise, it must be some other instrument number, so run our else clause”. Then in the else clause, it does the same thing, except checking for the second instrument and adjusting by 19%. If there was nothing to do there (because the instrument number was 3) then we run the else clause of that second if statement. This else clause houses an if statement that checks to see if the instrument is number three. This time it is, so the body of the if statement is executed. We set useful_reading equal to 92% of reading."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#elif",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#elif",
    "title": "PE100-03: Decision Structures",
    "section": "Elif",
    "text": "Elif\nThis is fine if we only have three instruments, but what do we do if we have 20 of them? We could, in principle, type in 60 lines of code, but that would be tedious, error prone, and would take a while to read and find any mistakes. Of course there’s a better way.\nThat better way is the “elif” keyword.\nLet’s see an example with 5 instruments…\n\nspectrometer_number = 4                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelif spectrometer_number == 2:                                                  \n    useful_result = reading * 1.19                                              \nelif spectrometer_number == 3:                                                  \n    useful_result = reading * .92                                               \nelif spectrometer_number == 4:                                                  \n    useful_result = reading * 1.03                                              \nelif spectrometer_number == 5:                                                  \n    useful_result = reading * 1.26                                              \nelse:                                                                           \n    useful_result = reading                                                     \n    print(\"Be careful!\")    \n                                                                                \nuseful_result\n\n7.210422299999999\n\n\nThe final else clause is the one that runs if no other clauses ran. If no clause’s conditional statement is true so no clause runs, whether it’s the if clause or any of the elif clauses, then the else clause runs. It’s really easy to spot else clauses even from across the room - they’re the ones that don’t have a conditional test.\nNote that the if, elif, and else lines must end with a colon. True confession time: I forget the colons about half the time. Python catches it as an error, I fix it, and life goes on."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#slightly-more-complicated",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#slightly-more-complicated",
    "title": "PE100-03: Decision Structures",
    "section": "Slightly More Complicated",
    "text": "Slightly More Complicated\nYou can run more than one line of code in response to the tested conditions, but they have to be indented the same amount:\n\nspectrometer_number = 103\nreading = 7.00041\n\nif spectrometer_number == 1:\n    useful_result = reading * 1.077\n    trustworthy = False\nelif spectrometer_number == 2:\n    useful_result = reading * 1.19\n    trustworthy = False\nelse:\n    useful_result = reading\n    trustworthy = True\n\nprint(useful_result, trustworthy)\n\n7.00041 True\n\n\nThere are four interesting things going on here. The first and most important thing to notice is that we’ve got more than one line of code running in response to an “if”, “elif”, or “else” clause. A collection of lines that should be run together as a whole is called a code block. Unlike many languages that mark the start and end of code blocks with special words or characters, Python just does it by using indentation. Everything that is indented the same amount is considered to be in the same code block. We’ll look at this in more detail in a few minutes.\nSecondly, we’ve added lines to set a variable named “trustworthy” to a value depending on whether we had to adjust the reading. Evidently, if we have to compensate for old, dry, cracking insulators then we don’t really trust the instrument.\nThe third interesting thing is the values True and False. These are “Boolean” values, and when we put them into the “trustworthy” variable then it takes on the Boolean type. There are only two values, True and False. The capitalization is important.\nThe fourth thing to notice is that we’re sending two values into the print statement and it’s printing both of them. In general, we can give the print statement any number of arguments, separated by commas, and it will print all of them separated by one space."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#conditional-aka-relational-operators",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#conditional-aka-relational-operators",
    "title": "PE100-03: Decision Structures",
    "section": "Conditional (aka Relational) Operators",
    "text": "Conditional (aka Relational) Operators\nThe conditional test in each part of an if statement is an expression that results in a Boolean value. So far, the only conditional operator (or relational operator) we’ve seen is ==. There are others, though. For the sake of completeness, I’ll include == here:\n\n\n\noperator\ntested condition\n\n\n\n\n==\nequals\n\n\n!=\nnot equals\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal\n\n\n\n“Relational” has at least two meanings in computing. Relational Operators have nothing to do with Releational Databases.\n\nTry This\nFor each of the following code cells, decide what the result is, run the cell, and see how you did:\n\n5 &lt; 6\n\nTrue\n\n\n\n5.99 == 5.99\n\nTrue\n\n\n\n5 != 5.00\n\nFalse\n\n\n\n5+6 &lt; 11\n\nFalse\n\n\n\n6 * 6 &gt; 12 + 12 + 12\n\nFalse\n\n\nRelational operators also work with strings.\n\nname = \"Alice\"\nif name == \"Alice\":\n    print(\"equals Alice.\")\nif name != \"Bob\":\n    print(\"The person is not Bob.\")\nif \"Alice\" &lt; \"Bob\":\n    print(\"Alice comes before Bob in alphabetical order.\")\nif \"Alice\" &lt;= \"Alice\":\n    print(\"Alice comes before or in the same place as Alice in sorted order\")\nif \"Mary\" &gt; \"Mark\":\n    print('Working left to right, the M, the a, and the r match on')\n    print('both strings, but when we finally get to the y and the k, y comes')\n    print('after k in alphabetical order.')\n\nequals Alice.\nThe person is not Bob.\nAlice comes before Bob in alphabetical order.\nAlice comes before or in the same place as Alice in sorted order\nWorking left to right, the M, the a, and the r match on\nboth strings, but when we finally get to the y and the k, y comes\nafter k in alphabetical order.\n\n\nA couple words of caution: the comparisons are based on the ASCII codes for each character. The “A” in ASCII stands for “American”, and as you might expect that means it only works for English language text. If you need to handle other languages, even potentially, then there is a better way to do it and we’ll see that in the lesson on strings.\nAlso, Capital letters are always less than lowercase letters, and not in the way you might think. “A” is less than “Z”, as you might expect, but “Z” is greater than “a”. The numbers 0-9 are the lowest of all. Punctuation is sprinkled around and the only way to know for sure is to look up “ASCII Chart”."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#code-blocks",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#code-blocks",
    "title": "PE100-03: Decision Structures",
    "section": "Code Blocks",
    "text": "Code Blocks\nLet’s go back to that part about running several lines of code but they have to be indented the same amount. Python always runs “blocks” of code. That block might be as short as one line:\n\ncircumference = 40 * 3.14159\n\ncircumference\n\n125.6636\n\n\nor it might be arbitrarily long:\n\nheight = 6.01\nlength = 5.5\nwidth = 14.3\ndensity = 4.2\nvolume = height * width * length\nmass = volume * density\nenergy_per_gram = 761.3\neyebrow_altering_potential = mass * energy_per_gram\n\neyebrow_altering_potential\n\n1511396.1762899999\n\n\nWhether it was the one line example or the eight line one, Python will set out to run all of those lines in one shot, and as long as there aren’t any errors it’ll do it. These are known as code blocks.\nThe decision structures (again, also called control flow statements) in Python all do basically the same thing: they evaluate an expression and depending on whether it turns out True or False, they execute a code block in some manner. This means that wherever we can have a single line of code running in a decision structure we can have as many lines as we want.\nTake a look at the following example. For the four possible combinations of potentially_hazardous and explody, decide what would be printed out. Then try out the combinations and make sure you know why each combination was handled the way it was.\n\npotentially_hazardous = True\nexplody = True\n\nif potentially_hazardous and explody:\n    height = 6.01\n    length = 5.5\n    width = 14.3\n    density = 4.2\n    volume = height * width * length\n    mass = volume * density\n    energy_per_gram = 761.3\n    eyebrow_altering_potential_energy = mass * energy_per_gram\n    print(\"Total Available Kaboom (TAK) to ruin your day is\", eyebrow_altering_potential_energy)\nelif potentially_hazardous:\n    print(\"Not likely to go 'kaboom', but not something you want to casually eat, either.\")\n    print(\"I mean, unless you're feeling brave.\")\n    print(\"Even then, it's a bad idea.\")\nelif explody:\n    print(\"This is one of those things that will blow up but isn't actually hazardous.\")\n    print(\"I'm guessing it's a vinegar-and-baking-soda volcano.\")\nelse:\n    print(\"As far as we know, the material in quesion is no more\")\n    print(\"dangerous than takeout pizza.\")\n\nTotal Available Kaboom (TAK) to ruin your day is 1511396.1762899999\n\n\nDid you notice potentially_hazardous and explody? and is a boolean operator. We’ve seen the arithmetic operators already (+, -, *, /, etc.) and now here are the boolean operators. They’re named after Boolean algebra, the algebra of logic, and are used to make larger logical expressions from smaller ones. There are three boolean operators: and, or, and not.\nThe and operator evaluates to True if both of its arguments are True. The or operator evaluates to True if either or both of its arguments are true. The not operator takes only one argument and reverses it: not turns True into False and False into True.\n\nmedical_license = True\ndental_license = True\n\nif medical_license and dental_license:\n    print(\"Doctor of Medical Dentistry (DMD)\")\nelif dental_license and not medical_license:\n    print(\"Plain old dentist.\")\nelif not dental_license and medical_license:\n    print(\"Garden-variety doctor.\")\nelse:\n    print(\"No license at all. Run. Quickly.\")\n\nDoctor of Medical Dentistry (DMD)"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-03DecisionStructures.html#coming-up-next-loops",
    "href": "newsite/theme1/PE100/PE100-03DecisionStructures.html#coming-up-next-loops",
    "title": "PE100-03: Decision Structures",
    "section": "Coming Up Next: Loops",
    "text": "Coming Up Next: Loops\nAt this point, we’ve seen the most basic way to alter the flow of control in Python: the if statement. We can write Python code to solve non-trivial problems now, but there are still some things we need in order to use Python as a truly general-purpose language. In the next notebook we’re going to make our code do something over and over."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html",
    "title": "PE100-04: Repetition",
    "section": "",
    "text": "We started off learning Python with just simple lists of statements…\ntemperature = 100\nprint(\"It's\", temperature, \"celsius\")\nfaren_t = temperature * (9/5) + 32\nprint(\"or\", faren_t, \"in pagan units.\")\n\nIt's 100 celsius\nor 212.0 in pagan units.\nThen we added the if statement so we could control whether or not certain statements would execute or not:\nif temperature &gt;= 100:\n    print(\"Good chance it's boiling.\")\nelif temperature &gt; 3000:\n    print(\"Odds are it's plasma by now.\")\nelse:\n    print(\"You could probably run the experiment and it might even work.\")\n\nGood chance it's boiling.\nIn both of these cases, the code blocks only run once.\nThe problem is, sometimes we need things to run repeatedly. We want to look up all of the readings from an experiment or we need to compute the properties of something over dozens of temperatures each at dozens of pressures.\nPython gives us two different ways to make our programs repeat things in a loop.\nLet’s start with the while loop."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#while-loops",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#while-loops",
    "title": "PE100-04: Repetition",
    "section": "While Loops",
    "text": "While Loops\nThe syntax of a while loop looks a bit like an if statement. Take a look:\n\ninstrument = 1\nwhile instrument &lt;= 2:\n   print(\"Looking at instrument number\", instrument)\n   print(\"and then maybe we'll look at the next one.\")\n   instrument = instrument + 1\nprint(\"Done with all that looping.\")\nprint(\"...and ready to do something else now.\")\n\nLooking at instrument number 1\nand then maybe we'll look at the next one.\nLooking at instrument number 2\nand then maybe we'll look at the next one.\nDone with all that looping.\n...and ready to do something else now.\n\n\nHere’s what the above code does. First, it creates a variable named “instrument” and sets it to 1. Then it goes into the while loop. The first time through, it checks to see if instrument is less than or equal to 2. It is (because we set it to 1 just a moment ago) so the while loop will execute the code block. This block prints out two lines and then it adds 1 to instrument. That means instrument now equals 2.\nThe second time through the loop, instrument equals 2. That satisfies the conditional statement of the while loop (2 is less than or equal to 2) so the code block runs again. Two more lines are printed out and then instrument is incremented one more time.\nThe while loop runs for a third time now. This time, 3 is not less than or equal to 2, so the conditional statement is false. This means the while loop is done - it won’t run its code block again, and the flow of control will go on to the next line after the while loop. It will run the two print statements explaining that the looping is over and it can go on to other tasks.\nLet’s look at another example. Let’s print out all the powers of two that are less than 928.\n\npower=0\ntwo_to_the_power = 2**power\n\nwhile two_to_the_power &lt; 928:  # Totally not Porsche related.\n    print(\"2 to the\", power,\"equals\",two_to_the_power)\n    power = power+1\n    two_to_the_power = 2**power\nprint(\"2 to the\", power,\"is too big.\")\n\n2 to the 0 equals 1\n2 to the 1 equals 2\n2 to the 2 equals 4\n2 to the 3 equals 8\n2 to the 4 equals 16\n2 to the 5 equals 32\n2 to the 6 equals 64\n2 to the 7 equals 128\n2 to the 8 equals 256\n2 to the 9 equals 512\n2 to the 10 is too big.\n\n\nDid you notice I sneaked something in there we haven’t talked about yet? See the “#” character on the line with the while statement? That indicates the rest of the line is a comment. Python will totally ignore it. It’s handy for leaving little notes to yourself, like “why did I choose 928 there when I could have put 944?” This is very, very important when writing full-fledged, standalone programs. If you don’t leave some notes for yourself, you’ll never remember what you were thinking when you go back to that code six months from now. Also, the next person who comes along and has to change something in your code will greatly appreciate the hints.\nLeaving comments in the code isn’t as big a deal in Jupyter notebooks… you can write rather substantial notes in a Markdown cell complete with boldface, italics, and whatever other fanciness you desire. On the other hand, it’s also nice to be able to leave your comments in the just the right place in the code so it flows effortlessly through your comprehension as you read it. Let experience and personal opinion be your guide here."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#reading-information-from-the-outside-world",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#reading-information-from-the-outside-world",
    "title": "PE100-04: Repetition",
    "section": "Reading information from the outside world",
    "text": "Reading information from the outside world\nNotice that in both of those cases, we actually did know how many times the loop would run. We know that 2 to the 9th is 512 and so we know the while loop will only run that far. In fact, in every example we’ve had so far we’ve know what the output will be because we always have the same inputs. Computer software wouldn’t be terribly interesting if it could only run specific, known, canned inputs. Fortunately, Python gives us several ways to bring data into our programs.\nThe simplest way to bring data into a Python program is to edit the program and change the values we assign to variables. This is sort of the reducto ad absurdum method, but honestly it isn’t a bad way to handle very small amounts of input. It’s even easier in Jupyter notebooks since the code is just sitting there looking at us, waiting to be edited. For values that aren’t going to change very often (your name, perhaps, or the chargeback account number for using some instrument, for instance) then just assigning a value to a variable and editing it every once and a while is a fine way to go.\nAnother way to get data into a Python program is to read it in from where the user is running the program. For doing this, Python provides a function called “input” which takes an optional argument, specifically a string that is printed as a prompt. Python then waits for the user to type something as a response. When they do, that string is returned to the calling program. Here’s a simple example:\n\nyour_name = input(\"Please enter your name\")\nprint(\"Hello,\", your_name)\n\nPlease enter your name Erik\n\n\nHello, Erik\n\n\nWhen the above code runs, the prompt “Please enter your name” is displayed right below the code cell and a text entry box is placed beside it. When you enter your name, it greets you.\nIf we were running this tiny little snippet of code as a regular program, the interaction would be in the terminal emulator window that we ran the program in. Because this is running in Jupyter, though, the interaction is directly in the notebook. The prompt and the entry blank occur just below the running code cell.\nWhat will happen when we run the following?\n\nresponse = input(\"Enter a number between 4 and 8\")\nnew_value = response + 6.5\nprint(new_value)\n\nEnter a number between 4 and 8 5.25\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [4], line 2\n      1 response = input(\"Enter a number between 4 and 8\")\n----&gt; 2 new_value = response + 6.5\n      3 print(new_value)\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\nWow! Python couldn’t run that and it “threw an error”. We’ll examine Python’s error handling facilities later, but for now we’ll just assume that means it came to a screeching halt. Looking at the error message, it seems there is some problem with trying to add a real number (a floting point number) to a string."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#type-casting",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#type-casting",
    "title": "PE100-04: Repetition",
    "section": "Type Casting",
    "text": "Type Casting\ninput() prompts the user and returns the string they entered, but what if we want the user to enter a number? What do we do then? The answer is we’ll use a process known as type casting. The act of type casting is no more than converting information from one type to another.\nThere are three very useful functions for type casting: int(), float(), and str(). Let’s see them in action…\n\n# three conversions:\n\n# first, string to float:\nthe_sum = 6.554 + float(\"9.9\")\nprint(the_sum)\n\n# next, string to integer:\nthe_integer_number = int(\"34543456\")\nprint(the_integer_number)\n\n#finally, integer to string:\nhandy_string = str(2+2)\nprint(handy_string)\n\n16.454\n34543456\n4\n\n\nWhat did the above do? First, it converted the string “9.9” (literally, three characters… it’s a string) to a “float” (a floating point number, some languages will call that a real number). The second example takes a string of 8 characters and interprets them as an integer. That value is what gets returned and stored in our variable. Finally, we copmute the number 4 by adding 2+2, and then we let the str() function convert that to a single character long string having just the character “4”.\nBy now we know enough to be able to ask the user for a number and get something back that we can actually do math with.\n\nuser_response = input(\"Enter a number between 17 and 34\")\nselected_number = float(user_response)\nselected_number\n\nThere’s an even easier way, though. Just like function composition worked when you took precalculus, the results of a Python function can be used as the argument to another. Hence:\n\nselected_number = float(input(\"Enter a number between 17 and 34\"))\nselected_number\n\nEnter a number between 17 and 34 26\n\n\n26.0\n\n\nSometimes, function calls can be nested really deeply. Personally, when it comes time to debug code like that I find myself printing it out and coloring each level with a different highlighter pen."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#putting-it-together-while-loops-to-get-user-input",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#putting-it-together-while-loops-to-get-user-input",
    "title": "PE100-04: Repetition",
    "section": "Putting it together: while loops to get user input",
    "text": "Putting it together: while loops to get user input\nThe great thing about a while statement is that it can loop zero times, one, two, or twelve trillion. Best of all, we don’t have to know how many ahead of time. We could do the following:\nprint(“Computing an average.”)\nsum=0.0 counter=0 data_point = float(input(“Enter a number, or enter negative num to stop”)) while data_point &gt;= 0.0: sum = sum+data_point counter = counter+1 data_point = float(input(“Enter a number, or enter negative num to stop”))\nprint(“Average value is”, sum/counter)\nWhen we run the code above, we’re prompted to keep entering numbers until we finally enter -999. Each time it goes through the loop it keeps track of the running total of the numbers and the count of how many numbers have been entered. Once it’s done, it divides the total by the count and displays that as the average.\nLet’s step through what happens when the user enters 1, 2, 3, and -999: 1. The sum and counter variables are initialized to zero. 1. The user is prompted to enter a number, possibly a negative number to indicate no more data, and that input is type cast to a floating point number. 1. The while loop’s condition will be met any time a positive number was input (greater than or equal to zero). 1 is a positive number, so run the loop body. 1. This first time through, we’ll add the 1 that was input to our running total, which is now 1. 1. And increment the count, now equal to 1. 1. AND PROMPT THE USER FOR ANOTHER NUMBER!!! 1. Back at the while statement again, we check the condition and, yes, 2 is a positive number, so we run the loop’s code block. 1. Update the sum and count, and then… 1. PROMPT THE USER FOR ANOTHER VALUE!!! 1. Running the while statement again, the user entered 3, and 3 is positive, so the clode block will be executed. 1. Update the sum (now 6) and count (now 3). 1. Prompt for another number 1. Back at the while statement, we check and see that -999 is not a positive number, so we skip the code block and resume by running whatever follows it. 1. Having exited the while loop entirely, print out the average value by dividing sum/count.\nAll the boldface and all-capitals lines above are there to emphasize how important it is to make sure your while loop isn’t just checking the same thing over and over. If we didn’t get a new number from the user each time through, the value of data_point would never change. That would result in an infinite loop, causing Python to never be able to complete the code in that cell. If it ever happens to you, and it probably will, the “Interrupt Kernel” command on JupyterLab’s Kernel menu will stop the looping and let you get back to work.\nThe while loop is certainly versatile… it can be used any time you need to do something repeatedly. If you know how many times you need to have the code block execute, either when you write the code or when it’s running, then keep a variable that is incremented in the block every time and exit the while loop when the counter hits the right number.\nWhere while loops really shine is when it’s impossible to know ahead of time how many times the code block should run. The example above, where we keep accepting numbers until the user signals there aren’t any more, there’s no way to know how many times to execute that loop until we see a negative number. In a case like that, the while loop is the only practical solution.\nSo if while loops are so great and solve every problem, why do we need anything else? The big reason is expressiveness: they can be a little awkward to understand, especially when you’re looking at someone else’s code. Having the conditional test separated from the action that establishes when to stop makes it a little awkward to understand (or debug!) someone else’s code. This is especially true when we need to step through something by unusual increments.\nSo what are we to do in these cases?"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#for-loops",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#for-loops",
    "title": "PE100-04: Repetition",
    "section": "For Loops",
    "text": "For Loops\nThe for loop is quite similar to the while loop. The difference is that for loops are controlled by a count whereas while loops are controlled by a condition.\nLet’s start with an example.\n\nfor the_value in range(1,4):\n    print(the_value)\n\n1\n2\n3\n\n\nThat is the simplest for loop you’ll see. Let’s look at the pieces. 1. The for statement itself 2. The name of the target variable whose value will be changing as the loop runs (“the_value” in this case”) 3. “in” - and if this reminds you of set membership then you’re on to something 4. “range()” - this is an example of an iterable, which means “something that can be stepped through”. 5. The colon… the one I forget 50% of the time. 6. The code block, in this case just a print statement.\nMost of the time, fairly close to “always”, the code block will take advantage of the target variable changing each time through. In our example, “the_value” is our target variable, as it loops through it will take on the values 1 through 3, and the code block has a print statement that uses it.\nBefore we examine the range() function, let’s take a look at another iterable. We’ll talk about lists in a later lesson, but for now we can just wave our hands around and understand enough for the moment.\n\nfor sample_weight in [123.6, 121.9, 119.4, 124.23219]:\n    print(\"The sample weighed\", sample_weight, \"grams.\")\n    if sample_weight &lt; 120:\n        print(\"Be careful! This sample might not be all you hoped for.\")\n        \n\nThe sample weighed 143.6 grams.\nThe sample weighed 141.9 grams.\nThe sample weighed 139.4 grams.\nThe sample weighed 144.23219 grams.\n\n\nYou can use the target variable as many times as you want to in the code block.\nNow let’s take a more detailed look at the range() function. In its most basic form it takes one argument - the stop value.\n\nfor i in range(4):\n    print(i)\n\n0\n1\n2\n3\n\n\nThis single-argument form starts at zero, counts up by one each time, and doesn’t include the stop value. This is different from every other programming language you’ll ever encounter. It’s just one of those things.\nWe’ve already seen the two-argument form. It takes a starting value and a stopping value, and iterates by one from the start until the last value that is less than the stop.\n\nfor i in range(7,10):\n    print(i)\n\n7\n8\n9\n\n\nAnd there’s even a three-argument form. The third argument is the amount to step by.\n\nfor i in range (12,20,3):\n    print(i)\n\n12\n15\n18\n\n\nThe step size doesn’t have to be a positive number…\n\nfor i in range(6, -3, -2):\n    print(i)\n\n6\n4\n2\n0\n-2\n\n\nIn case you’re curious, the step size cannot be zero. If you really want an infinite loop, and there are cases where it makes sense, you have to use a while loop instead.\nAs a general rule, any place where you can use an explicit value (a literal) you can use a variable. Arguments to a for loop are no exception:\n\nstart_value = int(input(\"where should we start? \"))\nend_value   = int(input(\"where should we run right up to and stop just short of it? \"))\nstep_size   = int(input(\"what should we step by? \"))\n\nfor i in range(start_value, end_value, step_size):\n    print(i)\n    \n\nwhere should we start?  13\nwhere should we run right up to and stop just short of it?  15\nwhat should we step by?  2\n\n\n13\n\n\nIf we need to do something a specific number of times, we need to pay attention to our starting and stopping conditions. I’ve messed this up so many times I know now to be careful. You’ve been warned.\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(1, how_many):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\n\n\nThey add up to 9\n\n\nNotice something wrong? If you ask it to total 3 numbers, it only prompts for two of them. There are a couple of ways to solve this. The easiest is to just use the one-argument form of range().\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(how_many):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nThat offers a little insight into why Python has it’s funny “up to but not including” semantics: zero is a perfectly legitimate number and a very natural starting point.\nThe only problem with the single-argument method is that the values that the target variable goes through include zero. This may or may not be a problem if that value is used inside the code block. If you really need to count from one instead of zero, you can increment the stopping value:\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(1, how_many + 1):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nAnd that behaved just like we expected.\nYou may have noticed a pattern already. We frequently need to compute a new value for an existing variable. What we’ve done so far has been along the lines of grand_total = grand_total + new_reading. Python gives us a shorthand way to write that. We could instead express that as grand_total += new_reading. There is no space between the plus and equals signs. The only reason this exists is to save you some typing. As you might expect, there are a few more of these Augmented Assignment Operators…\n\n\n\nOperator\nExample\nEquivalent\n\n\n\n\n+=\ncount += 1\ncount = count + 1\n\n\n-=\nx -= offset\nx = x - offset\n\n\n*=\nproduct *= val\nproduct = product * val\n\n\n/=\ny /= 3\ny = y / 3\n\n\n%=\nval %= 2\nval = val % 2\n\n\n\nOut of all of them, += is far and away the most commonly used one."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-04Repetition.html#nested-loops",
    "href": "newsite/theme1/PE100/PE100-04Repetition.html#nested-loops",
    "title": "PE100-04: Repetition",
    "section": "Nested Loops",
    "text": "Nested Loops\nYou know what’s fun to put in a loop’s code block? Another loop! Best of all, it comes in pretty handy when dealing with high-dimensional data. Plenty of algorithms rely on nested loops, too. Take a look at this:\n\nfor x in range(5):\n    for y in range(4):\n        print(\"x=\",x,\" y=\", y)\n\nx= 0  y= 0\nx= 0  y= 1\nx= 0  y= 2\nx= 0  y= 3\nx= 1  y= 0\nx= 1  y= 1\nx= 1  y= 2\nx= 1  y= 3\nx= 2  y= 0\nx= 2  y= 1\nx= 2  y= 2\nx= 2  y= 3\nx= 3  y= 0\nx= 3  y= 1\nx= 3  y= 2\nx= 3  y= 3\nx= 4  y= 0\nx= 4  y= 1\nx= 4  y= 2\nx= 4  y= 3\n\n\nWhat’s going on here? Initially, the outer loop, the one that iterates zero through four and assigns it’s value to x, runs. When it starts running its code block for the x=0 pass, the for loop for the y variable starts. ‘y’ assumes the values 0 through 3, so the first four lines printed out are for x=0, y=0, then x=0, y=1, and so on through x=0, y=3. Once that inner for loop completes, the outer for loop gets to iterate again. Now the inside for loop runs again, only this time we have x=1. That’s why the next four lines are “x=1, y=0” through “x=1, y=3”. Every time the outer loop runs another iteration, the inner loop gets to run all the way from start to finish.\nIn later lessons, we’ll have a few opportunities to play with nested loops. In fact, we’ll get to do that in the very next lesson: Functions!"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-07Exceptions.html",
    "href": "newsite/theme1/PE100/PE100-07Exceptions.html",
    "title": "PE100-06: Exceptions",
    "section": "",
    "text": "Most of the time, the code we write does exactly what we expect. Our numbers are added up, files are written and read, and users type their input in neat little boxes. Sometimes, though, something goes wrong. Maybe the disk storage space filled up, or we try to write to a file in a directory we don’t have access to (or maybe the directory doesn’t even exist). When things like this happen, the Python interpreter stops the normal flow of execution.\nTake a look at an exception:\n\nfunny_number = 1/0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 funny_number = 1/0\n\nZeroDivisionError: division by zero\n\n\n\nWhen you run the above, Python will notice the error, stop the code from running, and point out that a “ZeroDivisionError” has occurred. Since this kind of thing wasn’t supposed to happen (division by zero is considered a Bad Thing(tm) by most people) we can say the situation we’re faced with is an exception. And indeed, Python’s error handling mechanisms are based on what are called “exceptions”.\nWhen Python saw the “division by zero” error, it stopped running the rest of the code. It created one of these Exceptions, and then it threw it. Nothing in our one-line example tried to do anything about that exception, so Python just let the program crash and it printed the helpful error messages for us.\nMost of the time, we want our code to be able to handle exceptions when they arrise. We want something that can catch these exceptions when they’re thrown. For that, we need to use Python’s try statement.\n\nTry, try again\n\ntry is how we safely wrap up a bit of code so that if something in there fails and an exception is thrown, we have a way to catch it. For example:\n\ntry:\n    denominator = int(input('Please enter the denominator'))\n    funny_number = 1000/denominator\n    print('the result was', funny_number)\nexcept ZeroDivisionError:\n    print('Looks like someone tried to divide by zero.')\nprint(\"Either we were able to do the division or else we successfully handled an exception.\")\n\n    \n\nPlease enter the denominator 0\n\n\nLooks like someone tried to divide by zero.\nEither we were able to do the division or else we successfully handled an exception.\n\n\nTry running the code above a few times. In the input area, try some different numbers each time. Maybe 4, 0, and -2. Notice that division by non-zero numbers works as expected. Notice also that division by zero now lets us print out an error message instead of crashing. Once we’re done handling the exception, the program resumes with the first line after the try/except structure.\nIn fact, there might be several except clauses if there are several kinds of exceptions that might be thrown. For example, let’s figure out how to share a pizza.\n\ntry:\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept ValueError:\n    print(\"The number of people must be a valid integer.\")\nexcept ZeroDivisionError:\n    print(\"Seriously? There are zero people sharing a pizza?\")\n\nprint(\"Whatever happened up there, this is the first line of code after\")\nprint(\"the try/except structure.\")\n\nHow many people: 0\n\n\nSeriously? There are zero people sharing a pizza?\nWhatever happened up there, this is the first line of code after\nthe try/except structure.\n\n\nAs you try different numbers of people, you can see that division by zero is, of course, handled. You can also enter things that aren’t integers. In response to the prompt, you could enter “Fred”. That can’t be converted to an integer, so the int() function throws an error. The except ValueError clause catches that exception and prints out a message.\nNotice that after either exception handler executes its code, the flow of control goes down to the next line after the try/except structure. In this case, that line is one that prints out a message saying it’s the first line of code after the try and all of the excepts.\nSometimes it’s hard to predict what exception might be thrown in a section of code. In that case, we can use just except: without any exception type. This serves as a “catch-all” handler.\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept ValueError:\n    print(\"The number of people must be a valid integer.\")\nexcept ZeroDivisionError:\n    print(\"Seriously? There are zero people sharing a pizza?\")\nexcept:\n    print(\"The catch-all handler has been awoken from its slumber.\")\n    print(\"I don't know what went wrong, except I can tell you it\")\n    print(\"wasn't a ValueError or a ZeroDivisionError, because\")\n    print(\"those would have been caught by more specific handlers\")\n    print(\"further up the list.\")\n\nThe catch-all handler has been awoken from its slumber.\nI don't know what went wrong, except I can tell you it\nwasn't a ValueError or a ZeroDivisionError, because\nthose would have been caught by more specific handlers\nfurther up the list.\n\n\nIndeed, if we’re lazy (or in a hurry) then we can get by with just a plain except clause and let the user figure it out later:\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept:\n    print(\"There was some sort of problem. I have no idea what.\")\n\nThere was some sort of problem. I have no idea what.\n\n\nUsing just a plain catch-all exception handler doesn’t give you much to work with, but it is slightly better than nothing. Your code won’t crash outright but you won’t much information about what went wrong. If only there was a way to examine that exception, to peer in and divine its secret nature…\nYep. Here you go…\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept Exception as err:\n    print(\"Error:\", err)\n\nError: [Errno 2] No such file or directory: '/tmp/ThisFileIsUnlikelyToExist'\n\n\nWhat we’ve done is catch any kind of exception (except Exception) and assigned it to a variable named “err”. Then we can print out err. We could even convert err to a string and search for the interesting parts (like the filename of our missing file) and do some clever error handling based on what specifically went wrong.\nPython has a few more tricks when it comes to exception handling, and these can be handy for making your code more readable.\n\n\nFancy exception handling\n\nA try/except structure can have an else clause. This clause will only be executed if no exception was thrown.\n\ntry:\n    people = int(input('How many people:'))\n    slices = 8/people\nexcept Exception as err:\n    print(\"Error:\", err)\nelse:\n    print('Each gets', format(slices, '.2f'), 'pieces.')\n\nHow many people: 0\n\n\nError: division by zero\n\n\nIf the user enters something that can be converted to an integer and is non-zero, then the program continues, finishing up the try block and executing the else block. On the other hand, if an exception of any type is thrown then the “number of pieces” message will never be printed.\nThere is also a finally clause. This one will run after everything else has happened, no matter what.\n\ntry:\n    output_file = open(\"/tmp/output\", \"w\")\n    people = int(input('How many people:'))\n    slices = 8/people\nexcept Exception as err:\n    print(\"Error:\", err)\nelse:\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nfinally:\n    output_file.close()\n\nHow many people: 0\n\n\nError: division by zero\n\n\nIn the try clause, a file opening was added. In the finally clause, the file will be closed whether an exception was thrown or not.\nHow useful are else and finally clauses? It’s true they’re not absolutely necessary. Most programming languages don’t have anything like that. You can always juggle your code around and get by with just try and except. On the other hand, these two clauses can make your code easier to read and understand. Your precise intention can be discerned.\nWe’ve seen how to write Python code that catches errors without crashing. This technique works in both regular Python programs and in Jupyter Notebooks. Next up, we’ll turn back to ways of storing information. This time we’ll look at lists."
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-09Strings.html",
    "href": "newsite/theme1/PE100/PE100-09Strings.html",
    "title": "PE100-09: Strings",
    "section": "",
    "text": "We’ve been using strings in each of the previous modules, but we’ve accepted them as a found artifact without really getting into them and seeing how they. This module will correct that deficiency and make all of better peopleprogrammers.\n\nReview time\nLet’s take a quick look at what we’ve done so far.\nString literals:   \"Doug McKenzie\"\nString variables:  comedic_genius = \"Mel Brooks\"\n\nComparison:        if my_name == your_name:\n                   if your_name != \"Chuck Woolery\":\nConcatenation:     full_name = first_name+last_name\nRepetition:        \"ABC\" * 20\nThere is a lot more we can do with strings. We can: * Index into them * Iterate over them * Slice them * Search them * Call methods that act on them…\nIn fact, when you look at what you can do with a string and how you do it, you suddenly realize that a string is just (conceptually) a tuple of letters.\nWe can index into a string:\n\nmy_name=\"John Belushi\"\nprint(my_name[0])\nprint(my_name[len(my_name)-1])\nprint(my_name[-1])\n\nJ\ni\ni\n\n\nStrings are iterables:\n\nfor char in my_name:\n    print(char)\n\nJ\no\nh\nn\n \nB\ne\nl\nu\ns\nh\ni\n\n\nJust like a tuple, the elements of a string are immutable. Once a string is created, the characters can’t be changed.\n\nmy_name=\"Bob\"\nmy_name[0]='R'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 my_name=\"Bob\"\n----&gt; 2 my_name[0]='R'\n\nTypeError: 'str' object does not support item assignment\n\n\n\nWe can slice strings:\n\nmy_name=\"Michael Jordan\"\nprint(my_name[2:4])\nprint(my_name[4:])\nprint(my_name[:2])\n\nch\nael Jordan\nMi\n\n\nWe can search into a string with the in operator:\n\ntweet_msg = \"I think synchrotrons are cool.\"\nif \"synchrotrons\" in tweet_msg:\n    print(\"found one!\")\n\nfound one!\n\n\nThere are a huge variety of string methods. We’ll look at just a few here. Some of them are handy for validating inputs…\n\nbolts = input(\"How many bolts did you install?\")\nif not bolts.isdigit():\n    print(\"I was expecting something that looked like an integer\")\nelse:\n    fasteners = int(bolts)\n\nHow many bolts did you install? op\n\n\nI was expecting something that looked like an integer"
  },
  {
    "objectID": "newsite/theme1/PE100/PE100-08Lists.html",
    "href": "newsite/theme1/PE100/PE100-08Lists.html",
    "title": "PE100-08: Lists",
    "section": "",
    "text": "All of the variables we’ve seen so far store exactly one value. If you set the variable “weight” to 74.5, then 74.5 is the only value there is in “weight”. Nice and simple. If we need to save several values then we can use several variables…\n\nweight_1 = 74.5\nweight_2 = 76.7\nweight_3 = 77.1\n\nAs you can imagine, this turns tedious in a hurry. What if you had a thousand values to deal with? And even if you did all of that typing, doing any kind of non-trivial computation with it would be difficult, too. We need a way to store a bunch of values, but doing it in a way that makes it easy to manipulate the whole thing as a whole or each individual value. For doing that, Python provides us with lists.\nPython is one of the few languages that support lists deep down in the language itself. Because of that, they’re easy to work with. Let’s take a look, shall we?\n\nnames = ['Alice', 'Bob', 'Candice', 'Dan']\nnames\n\n['Alice', 'Bob', 'Candice', 'Dan']\n\n\nLists are represented with square brackets [ ] at the beginning and end, and with the values inside the brackets separated by commas.\n\nodd_numbers = [1, 3, 5, 7, 9]\ningredients = ['flour', 'lard', 'baking powder', 'milk']\n\nThe values in a list don’t all have to be the same type.\n\nplaying_card = [9, 'Diamonds']\n\nA list can have any number of values, limited only by the amount of memory in the computer that is hosting the Jupyter (or JupyterLab) server. Lists are even allowed to have no values in them.\n\nempty_list = []\n\nSo far we’ve been creating lists using literal values, but we could use variables just as easily…\n\nnimh = 16\nlithiumPrimary = 2\ncarbonZinc = 6\n\nbattery_inventory = [nimh, lithiumPrimary, carbonZinc]\nprint(battery_inventory)\n\n[16, 2, 6]\n\n\nTo find out how many elements are in a list, use the len() function:\n\nnumber_of_ingredients = len(ingredients)\nprint(number_of_ingredients)\n\nprint(len(battery_inventory))\n\n4\n3\n\n\nThere are operators that act on lists. The * operator is used for repetition…\n\nmy_list = [1, 2, 3] * 2\nprint(my_list)\nmany_zeros = [0] * 25\nprint(many_zeros)\n\n[1, 2, 3, 1, 2, 3]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n…and the + operator combines two lists:\n\nbig_list = my_list + many_zeros\nprint(big_list)\n\n[1, 2, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nLists are iterables, just like the results of the range() function, so they can be iterated over using a for loop:\n\nfor name in ['David', 'Bill', 'Richard']:\n    print(name)\n\nDavid\nBill\nRichard\n\n\n\nfor ingr in ingredients * 3:\n    print(ingr)\n\nflour\nlard\nbaking powder\nmilk\nflour\nlard\nbaking powder\nmilk\nflour\nlard\nbaking powder\nmilk\n\n\nIt’s fairly common to iterate over a list for things like sums and averages.\n\ntotal = 0\nfor item in [4, 3, 4, 5]:\n    total += item\nprint(total)\n\n16\n\n\nThe above code steps its way over all of the values in the list. Each time it goes to a new value, it adds that value to total. When it gets to the end, all of the values have been added up. If we want an average, we don’t have to count the values ourselves. We can just use the len() function.\n\ntotal = 0\nmy_list = [4, 3, 4, 5]\nfor item in my_list:\n    total += item\navg = total / len(my_list)\nprint(avg)\n\n4.0\n\n\nSometimes you need to use a particular value in a list and you don’t want to iterate over the whole thing. For this, Python gives us indexing, letting us directly access any element of a list. The first (as it appears on screen, “leftmost”) element in numbered zero and each one after that goes up by one. The highest numbered one is therefore the length of the list minus one.\n\nprint(ingredients)\nprint()\nprint(\"Of all the biscuit ingredients,\", ingredients[0] , \"is the most important one.\")\nprint(\"The second most important one is\", ingredients[2])\n\n['flour', 'lard', 'baking powder', 'milk']\n\nOf all the biscuit ingredients, flour is the most important one.\nThe second most important one is baking powder\n\n\nLike the majority of programming languages, Python uses square brackets to indicate the index into the list. Unlike the vast majority of languages, Python allows indexes to be negative! A negative number for an index means “count backwards from the end”. my_list[-1] refers to the item at the end of the list. my_list[-3] refers to the third to last item.\n\nprint(ingredients[-1])\nprint(ingredients[-3])\n\nmilk\nlard\n\n\nWe’ve seen how to iterate over lists and also how to access individual list elements by using indexing. Python has a special indexing scheme, though, that lets us deal with small lists made from our original list. This is called List Slicing and can save you a lot of work sometimes. The overall syntax for this looks like list_name[start:end]\nAn example is definitely called for here:\n\nmy_list = [2, 4, 6, 8, 10, 12]\nprint(my_list[1:3])\n\n[4, 6]\n\n\nRemember that list indexes count from zero, and remember also that ranges in Python include the starting index (here, it’s the 1) and will continue to the last value that is smaller than the one on the right side of the colon.\nBoth the starting and the ending indexes are optional! If one of the two is missing, it will be interpreted as 0 or the list’s length, respectively.\n\nprint(my_list)\nprint()\nprint(my_list[:3])\nprint(my_list[1:])\nprint(my_list[:])\n\n[2, 4, 6, 8, 10, 12]\n\n[2, 4, 6]\n[4, 6, 8, 10, 12]\n[2, 4, 6, 8, 10, 12]\n\n\nAnd finally, the in operator is used to test list membership.\n\nlucky_numbers = [2, 7, 17, 9]\nplayer_number = int(input('Enter your favorite number'))\nif player_number in lucky_numbers:\n    print(\"Your favorite number is lucky!\")\nelse:\n    print(\"Sorry! Better luck next time!\")\n\nEnter your favorite number 8\n\n\nSorry! Better luck next time!\n\n\n\nThere’s Method to the Madness\n\nThere are two kinds of functions available for working with lists. Built-in functions are the ones that are part of Python itself. Methods, as you’ll recall from the unit on files, are special functions that are situated inside of objects and only usable with that kind of object. Python lists are objects. They’re iterable objects, in fact.\nLet’s take a look at a few of the methods available for working with lists. First up is append().\n\nprint(lucky_numbers)\nlucky_numbers.append(106)\n\nlucky_numbers\n\n[2, 7, 17, 9]\n\n\n[2, 7, 17, 9, 106]\n\n\nJust as the name implies, append() adds an element to the end of a list.\nBut what if we want to put a new element in a specific place? For that, there is insert().\n\nprint(lucky_numbers)\nlucky_numbers.insert(2, 202)\nlucky_numbers\n\n[2, 7, 17, 9, 106]\n\n\n[2, 7, 202, 17, 9, 106]\n\n\nThe insert function takes two arguments. The first is the position in the list where the insertion should happen. In the example above, it was at position 2. Remember, list indexes start at zero! The second argument is the element to insert. And when we look at the resulting list, we see that 202 is in position 2 now (which is the third position!) and all the other elements have been shifted to the right.\nWe’ve been fetching elements from the list by location number, so far. How do we find something by searching for it? The index() method does that.\n\nwhere_found = lucky_numbers.index(202)\nprint(where_found)\n\n2\n\n\nWe passed the argument 202 to the index method. It searched the list and returned the index of the first occurence. That index is 2. Makes sense because we just inserted it there a minute ago!\nIf we can insert things into a list then surely we can remove them too, right? Indeed we can with the remove() method.\n\nprint(lucky_numbers)\nlucky_numbers.remove(7)\nprint(lucky_numbers)\n\n[2, 7, 202, 17, 9, 106]\n[2, 202, 17, 9, 106]\n\n\nWatch out! remove() looks up an item, like index() does, and then removes it. It doesn’t take a position number. In other words:\n\npeople = ['David', 'Bill', 'Richard']\npeople.remove('Bill')\nprint(people)\n\n['David', 'Richard']\n\n\nYou might find yourself needing to sort the items in a list, and for that the sort() method exists:\n\nprint(lucky_numbers)\nlucky_numbers.sort()\nprint(lucky_numbers)\n\n[2, 202, 17, 9, 106]\n[2, 9, 17, 106, 202]\n\n\nFinally, there are methods to find the greatest and smallest values in a list.\n\nprint(min(lucky_numbers))\nprint(max(lucky_numbers))\n\n2\n202\n\n\nEarlier we saw the use of len() to find out how many items are in a list. This is a built-in function and works on many types of variables, not just lists. There are two more built-in functions that are useful for working with lists: min() and max().\n\nsiblings = ['David', 'Bill', 'Shirley', 'Richard', 'Laverne']\nprint(min(siblings))\nprint(max(siblings))\n\nBill\nShirley\n\n\n\n\nLists and Functions\n\nFunctions have no problem accepting lists as arguments and they can also return lists as the function’s value. There is a subtle “gotcha” when passing lists as an argument, though.\nFirst, let’s look at a simple example:\n\noriginal_list = [1, 2, 3, 9]\n\ndef sum_of_list(list_to_sum):\n    sum = 0\n    for i in list_to_sum:\n        sum = sum + i\n    return sum\n\nthe_sum = sum_of_list(original_list)\nthe_sum\n\n15\n\n\nThat worked as expected - there’s no problem passing lists into functions. What about returning lists from functions?\n\ndef pet_factory(how_many_pairs):\n    pets = ['goldfish', 'catfish'] * how_many_pairs\n    return pets\n\nmany_fish = pet_factory(5)\nprint(many_fish)\n\n['goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish']\n\n\nEarlier, when we talked about functions in section 5, we said that if a function changes the value of one of its arguments then the effects of that change stay inside the function and aren’t visible to anything when the function exits. That statement was mostly true. If you pass a list as an argument to a function and if that function changes the list then the change made there will be visible outside. Strings, floats, and integers asre protected, but lists are more exposed.\n\noriginal_list = [1, 2, 3, 9]\n\ndef doubler(numbers):\n    for i in range(len(numbers)):\n        numbers[i]=numbers[i]*2\n\nprint(original_list)\ndoubler(original_list)\nprint(original_list)\n\n[1, 2, 3, 9]\n[2, 4, 6, 18]\n\n\nChanging the value of an argument inside of a function usually isn’t a great idea, but in the case of lists it can be useful.\n\n\nNo Funny Glasses Required\n\nThe lists we have worked with up to this point have all been one dimensional. Lists get a lot more interesting as the number of dimensions goes up.\nUnlike most programming languages, Python does not have a multi-dimensional list or array construction, per se. What Python does have is a list that is versatile enough to contain anything - and that includes containing other lists! A two-dimensional list in Python is just a “list of lists”.\nTake a look:\n\nfirst_presidents = [['George', 'Washington'], ['John', 'Adams'], ['Thomas', 'Jefferson']]\n\nAbove, on that very long line, we’ve created a list with square brackets. Inside that list, we’ve put three more lists inside square brackets of their own. So we’ve made a list of lists.\nThat long line is hard to read, isn’t it? Python won’t let us just split a long line of code across multiple lines… unless we explicitly tell it what we’re doing. That is done by ending each line with a backslash and immediately pressing enter. It looks like this:\n\nfirst_presidents = [['George', 'Washington'],\\\n                    ['John', 'Adams'],\\\n                    ['Thomas', 'Jefferson']]\n\nJupyter even goes to the trouble to line up the columns for us.\nAnyway, let’s see what we’ve created.\n\nprint(first_presidents)\n\n[['George', 'Washington'], ['John', 'Adams'], ['Thomas', 'Jefferson']]\n\n\n\nprint(first_presidents[0])\n\n['George', 'Washington']\n\n\n\nprint(first_presidents[2])\n\n['Thomas', 'Jefferson']\n\n\nWe can index into the outer array, the one that contains the smaller lists, just like we normally would. We can also index into the inner array two different ways. The long way…\n\npresident_number_one = first_presidents[0]\nfirst_name = president_number_one[0]\nfirst_name\n\n'George'\n\n\n… or we can take the shortcut:\n\nfirst_name = first_presidents[0][0]\nfirst_name\n\n'George'\n\n\nThe first zero got us to the “George”, “Washington” element, and the second zero indexed into that and gave us ‘George’. Let’s try some other combinations:\n\nnext_first_name = first_presidents[1][0]\nnext_first_name\n\n'John'\n\n\n\nanother_name = first_presidents[1][1]\nanother_name\n\n'Adams'\n\n\nIt’s easy to see how we’re indexing into this two-dimensional list. In fact, it works roughly the same way as a 2-D array in most programming languages.\nIt’s so similar, in fact, that you’re probably feeling the urge to do some Linear Algebra right now.\nDon’t. Not yet.\nPython’s multidimensional list support is exactly that: support for lists. It can be pressed into service for arrays (in the linear algebraic sense of the term) but performance is pretty bad. In Programming Elements 101 we’ll see a software library called “numpy”. It is superior for arrays where you want to do some math.\nNow let’s look at how to traverse multi-dimensional array. We’ll create a 2-D list that look like this:\n        Column 0  Column 1  Column 2  Column3\nRow 0.     A         B         C         D\nRow 1.     E         F         G         H\nRow 2.     I         J         K         L\nRow 3.     M         N         O         P\n\nletter_table = [['A', 'B', 'C', 'D'],\\\n['E', 'F', 'G', 'H'],\\\n['I', 'J', 'K', 'L'],\\\n['M', 'N', 'O', 'P']]\n\nWe can get a whole row:\n\nprint(letter_table[1])\n\n['E', 'F', 'G', 'H']\n\n\nor we can get a specific cell (the order is row, then column):\n\nprint(letter_table[2][1])\n\nJ\n\n\nWe can access the table by column, but it’s not as easy. We’ll have to write a loop that steps down a column and reads the values:\n\nfor i in range(len(letter_table)):\n    print(letter_table[i][3])\n\nD\nH\nL\nP\n\n\nWhat if we want to access all of the cells in the array? For that, nested loops work.\n\nfor row in range(len(letter_table)):\n    for col in range(len(letter_table[row])):\n        print(letter_table[row][col])\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\n\n\n“But wait!”, I hear you say. “I need to store higher-dimensionality data!” No problem. Python will allow arbitrarily deep nesting. We can have lists of lists of lists (3 dimensions) or lists of lists of lists of lists for four dimensions. Accessing the cells is just a matter of adding more array indexes to the end of the name.\n\nfirst_vice_presidents = [[\"John\", \"Adams\"],\\\n                         [\"Thomas\", \"Jefferson\"],\\\n                         [\"Aaron\", \"Burr\"]]\n\nearly_us_leaders = [first_presidents, first_vice_presidents]\nprint(early_us_leaders[1][0][1])\nprint(early_us_leaders[1][2][0])\n\nAdams\nAaron\n\n\nIt’s easy to get confused with deeply nested lists. Three dimensions isn’t bad, four is managable, but as the structures get deeper and deeper I have to resort to drawing pictures and frequent testing every step of the way.\ntl;dr: If you’re a string theorist working in 21 dimensions or whatever, Python lists probably aren’t the way to go. You should use numpy.\n\n\nTuples\n\nA “double”, mathematically speaking, is two of something. A “triple” is three of them. If you don’t know how many, or you don’t want to specify, then it’s generically called a “tuple” (pronounced “Too pull”, according the The American Heritage Dictionary and, more importantly, everyone who has ever taught the database class).\nPython gracious provides us with tuples. Their syntax is just like a list, only using parentheses instead of square brackets. For instance:\n\nmy_tuple = (2, 8, 256)\nprint(my_tuple[1])\n\n8\n\n\nTuples have some restrictions when compared to lists. * You can’t sort them. * You can’t insert or delete from them * You can’t change the values in them\nWhy would we want tuples if they’re so similar to lists, only somewhat disabled? In a word, “speed”. They’re very fast compared to lists. That’s why some Python functions require them. The most likely time you’ll see tuples is when you’re accessing data from a database. The second most common use is when you need to return multiple values from a function.\nSince tuples have the speed advantage but lists are more versatile, it’s not unusual to see programmers use the list() and tuple() functions to convert between the two types:\n\nmy_tuple = (2, 8, 256)\nlist_version = list(my_tuple)\nlist_version\n\n[2, 8, 256]\n\n\n\nmy_list=[2, 4, 6, 8]\ntuple_version = tuple(my_list)\ntuple_version\n\n(2, 4, 6, 8)\n\n\nReturning multiple values from a function feels like cheating the first time you do it. After all, sin(x) returns exactly one number, right?\nWhat if you wrote a function that returns a complex number, like 1.105+7.3i ? That’s one number (albeit one on the complex plane) but it’s written like two pieces of data being returned.\nWhat if you got really fancy and wrote a function that returned a column vector? That would be like returning a lot of numbers all at once, wouldn’t it?\nSo returning multiple values at once isn’t that bad, is it? Especially if the values all have related meaning and “belong” together.\n\ndef get_extremes(number_list):\n    min_val = min(number_list)\n    max_val = max(number_list)\n    return (min_val, max_val)\n\nnumbers = [5, 3, 2, 7, 2, 5]\nlow, high = get_extremes(numbers)\nprint(low)\nprint(high)\n\n2\n7\n\n\nA couple of things to note. First, notice how the function creates a tuple and returns it. The parentheses indicate a tuple is being constructed and the min_val and max_val variables are put into the tuple as the first and second elements.\nSecond, look at how that tuple is returned to the caller, taken apart, and stored in a pair of variables. You’ll see the syntax first_variable, second_variable, third_variable = func() when a tuple is returned from a function. The first element of the tuple is placed in first_variable and so on.\n\n\nComing Up Next\n\nWe’ve made it to the end of this section. Take a moment, breathe, and relax… this is the longest module in the “Python and Jupyter” series. Next up we have lots of information on strings. We’ve been using strings a lot already without really looking at what they are and what they can do. It’s time to remedy that.\n(pssst. Want a hint? Strings are just tuples of letters!)"
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html",
    "href": "newsite/theme1/PE103/vcs.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems (or VCS) are software tools that are used to track changes to source code or other collections of files.\nWikipedia has a fairly long list of version control systems, each of them varying in practical usage and in implementation details. These days Git is the most popular version control system in use. Git appears to have “won”, and so Git is what we will be discussing here."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#why-do-we-need-version-control",
    "href": "newsite/theme1/PE103/vcs.html#why-do-we-need-version-control",
    "title": "Version Control",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\nWhen you work on a project, you often will want to save the state of your code at various points, such that you can go back and forth between these various points. When you work on a project as part of a team, you will also want to know who wrote some part of the code, when, and why.\nWhether you work alone or in a team, using a version control system will help you achieve the above goals. Sometimes your team member is the past you (who should help you), or the future you (whom you should help).\nIn the absence of a version control system, you will often will end up with a chaotic mess which achieves the above goals in a poorer manner. You will likely resort to several almost-same-but-not-quite files variously named like so:\n\nnotebook.ipynb\nnotebook-2024-05-01.ipynb\nnotebook-2024-05-01-final.ipynb\nnotebook-working.ipynb\nnotebook-test.ipynb\nnotebook-final.ipynb\n\n(Or think of the situation depicted by this PHD Comics strip.)\n\n\nThis scheme is basically a messy reinvention of a version control system. That might work in the simple cases, but it will soon break down as you do more work on your project.\nYou want to avoid the cognitive overload of dealing with messy schemes based on file names. You want to use a version control system properly.\nConceptually, you can think of the evolution of versions over time as a directed graph:\n\nSometimes you branch out, say, when exploring a new direction:\n\nAnd you might eventually want to merge the branch to the “main” body of your graph:\n\nWith a version control system, you can to do these things in a less ad-hoc manner."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#version-control-in-practice-git",
    "href": "newsite/theme1/PE103/vcs.html#version-control-in-practice-git",
    "title": "Version Control",
    "section": "Version control in practice: Git",
    "text": "Version control in practice: Git\nGit is a command-line program that runs on all popular operating systems. If you use macOS or Linux, you probably have Git installed already. Here we assume that you are using the account that you have with CLASSE.\nLet us prime ourselves for Git with an XKCD strip:\n\n\n\n© Randall Munroe, Creative Commons Attibution-NonCommercial license\n\n\nAs the cartoon suggests, Git has a (perhaps well-deserved) reputation for being rather unfriendly or inscrutable. With some familiarity and practice, it can be tamed.\n\n\n\n\n\n\nNote\n\n\n\nDepending on your background, you might find that learning Git by first understanding the data model more helpful. Version control module of the MIT course “The Missing Semester of Your CS Education” and Git from the Bottom Up by John Wiegley take this route. These notes that you are currently reading, however, take the more traditional path of introducing you to the more frequently used Git commands.\n\n\n\nFirst steps with Git\nYou can start trying out git by running the below in a terminal:\n$ git help\nThis should print some common Git commands used in various situations.\n\n\n\n\n\n\nNote\n\n\n\nThe specific output from git help might vary depending on the version of Git that you are using. You can find the version of Git that you’re using by running git version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might find git help -g (or git help --guides) useful:\n$ git help -g\nThe common Git guides are:\n\n   attributes   Defining attributes per path\n   glossary     A Git glossary\n   ignore       Specifies intentionally untracked files to ignore\n   modules      Defining submodule properties\n   revisions    Specifying revisions and ranges for Git\n   tutorial     A tutorial introduction to Git (for version 1.5.1 or newer)\n   workflows    An overview of recommended workflows with Git\n\n'git help -a' and 'git help -g' lists available subcommands and some\nconcept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'\nto read about a specific subcommand or concept."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#getting-started-with-git",
    "href": "newsite/theme1/PE103/vcs.html#getting-started-with-git",
    "title": "Version Control",
    "section": "Getting started with Git",
    "text": "Getting started with Git\nGit commands are generally of the form git &lt;subcommand&gt;, where &lt;subcommand&gt; is for the specific operation you want to do. We will discuss them in the following sections.\nThere is also a little bit of configuration that you should do, before you are able to git add and git commit your changes. Let us start with this configuration.\n\nInitial configuration\nGit keeps track of who makes changes. For this to work, you’ll need to configure Git using git config subcommand:\n$ git config --global user.name \"Your Name\"\n$ git config --global user.email \"you@example.com\"\nThis will write configuration to a file named .gitconfig in your home directory.\n$ cat ~/.gitconfig\n[user]\n    name = Your Name\n    email = you@example.com\nOf course, you should use “real” values instead of Your Name and you@example.com.\n\n\nStarting a new repository\nLet us start with a very simple example, just for practice. We will create a brand new Git repository, and commit some changes to it.\nNow, what is a repository, and what are commits?\n\nA Git repository is essentially a directory where Git tracks your files and manages changes to them. It is a database that stores your project’s history. This history includes every version of every file in your project, and who made those changes, and when, among other things.\nA Git commit is a snapshot of your project at some point in time. You can think of them as versions.\n\nOn lnx201, let us create a new directory (with mkdir hello-world), change to that directory (with cd hello-world), create a file in that repository, and initialize a git repository there (with git init):\n$ mkdir hello-world\n$ cd hello-world/\n$ echo \"hello $USER\"\nhello ssasidharan\n$ echo \"hello $USER\" &gt; hello.txt\n$ cat hello.txt\nhello ssasidharan\n$ git init\nInitialized empty Git repository in /home/ssasidharan/hello-world/.git/\nThat created an empty repository, meaning, nothing has been added to it. As you can see, git init created a directory named .git/ inside hello-world/. This .git directory is the repository’s “database” – this is where Git stores information about your project, including version history, configuration settings, and references to commits, and more.\nThe \".\" prefix in .git/ means that it is a “hidden” directory. It won’t appear in the output of commands such as ls. You will need to use ls --all or ls -a.\nRunning git status will show an “untracked file”:\n$ git status\n# On branch master\n#\n# Initial commit\n#\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hello.txt\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\n\n\n\nNote\n\n\n\nWhat does On branch master mean?\nThink of a branch as a line of development. Your work almost always happen on a branch. When you start a project, Git uses a default branch called a master. With newer versions of Git (since , the default branch is called main.\n\n\n\n\n\n\nAn “untracked file” means Git do not know about the file. We will need to add that file to Git.\n\n\nAdding changes\nLet us add hello.txt to the repository, and check the status again:\n$ git add hello.txt\n$ git status\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#\n#   new file:   hello.txt\n#\nThe command git add hello.txt adds the file hello.txt to the repository. You could also have done a git add . to add all files in the directory to the repository.\nNote that git add hello.txt does not commit hello.txt to the repository; it just tells Git to pay attention to the file. With git status, we can see that Git is aware of the fact that there are some changes to be committed in the repository.\n\n\nCommitting changes\nUse git commit to actually commit the tracked file to the repository:\n$ git commit -m \"Add hello.txt\"\n[master (root-commit) 708bfca] Add hello.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 hello.txt\nThe string following -m option (-m is short for --message) is a commit message. You use commit messages to describe the change in a single short line.\nNote that commit messages are not required to be single lines. On lnx201, if you run git commit without -m &lt;message&gt;, an editor will be launched where you can write a more detailed commit message. You can configure a different editor for writing commit messages, if you want to do so. As a general rule, your commit message should start with a short description of the commit in a single line, followed by a blank line, followed by a more detailed description of the commit.\nOver time, your commit messages will tell the story about how your project evolved. More details in commit messages would be quite useful when you or someone else revisit the change at some point in the future.\n\n\n\n© Randall Munroe, Creative Commons Attibution-NonCommercial license\n\n\nNow we can use git status to re-check status of the repository:\n$ git status\n# On branch master\nnothing to commit, working directory clean\nWe can use git log to view the commit history:\n$ git log\ncommit 708bfcafe32528e90e1d52fd6b94f0c44476518a\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:10:02 2024 -0400\n\n    Add hello.txt\nThe commit 708bfcafe32528e90e1d52fd6b94f0c44476518a part is what is known as a “commmit hash” or a “commit ID”. It is a 40-character hexadecimal string generated using a cryptographic hashing algorithm. Each commits get a unique commit hash, and represents the state of the repository at a given point of time.\nLet us add some more changes, and commit them:\n$ echo \"hello from $HOSTNAME\" &gt;&gt; hello.txt\n$ git status\n# On branch master\n# Changes not staged for commit:\n#   (use \"git add &lt;file&gt;...\" to update what will be committed)\n#   (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n#\n#   modified:   hello.txt\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git add hello.txt\n$ git commit -m \"Update hello.txt\"\n[master 233c748] Update hello.txt\n 1 file changed, 1 insertion(+)\n$ git status\n# On branch master\nnothing to commit, working directory clean\nTry git log to show commit history again:\n$ git log\ncommit 233c748ad3dd31c11a3bc12d0cf106d7fe888fc3\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:22:29 2024 -0400\n\n    Update hello.txt\n\ncommit 708bfcafe32528e90e1d52fd6b94f0c44476518a\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:10:02 2024 -0400\n\n    Add hello.txt\n\n\nSome more terminology\nFiles that are tracked by Git can be in one of these stages:\n\nModified: you have changed the file, but has not added or committed it.\nStaged: you have done a git add on the file, thus moving into the “staging area” of the repository.\nCommitted: you have done a git commit, thus storing the file in your local Git database.\n\nLet us see what this means with a diagram:\n\nThe working tree is what you get when you initialize repository (with git init) or check out a repository (with git clone). The files you work with are here. When you modify a file, you will need to move it to the staging area before committing it.\nThe staging area stores information about what will go into your next commit. In Git terminology, staging area is also called the index.\nWhen you do git commit, your staged changes will be committed to the database in .git directory. With these two-step actions, you get more control over what set of changes you get to commit to the project.\nWith git commit -a (or git commit --all), you can stage the changes and commit them at once:\n$ echo \"third line\" &gt;&gt; hello.txt\n$ git commit -am \"Add a third line\"\nNote that git commit -am \"Add a third line\" is the same as git commit --all --message \"Add a third line\".\n\n\nReviewing changes\nWe have git status and git log in action already. Another command is git diff, which is used to find the difference between two commits:\n$ git diff 708bfcafe32528e90e1d52fd6b94f0c44476518a 233c748ad3dd31c11a3bc12d0cf106d7fe888fc3\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\nA useful shortcut is git diff HEAD~:\n$ git diff HEAD~\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\nIn Git parlance, HEAD implies the last commit on the current branch, and HEAD~ is the commit before that, and git diff HEAD~ would print the difference between the latest commit and the one before that.\nEventually, once there are more commits in the repository, you can view the difference with an arbitrary number of commits in history with git diff HEAD~~~ (or, more conveniently: git diff HEAD~3), and so on. You get the idea.\nAnother shortcut for those really long commit hashes is using a smaller prefix of them. You can find these “short hashes” with git log --abbrev-commit or git log --oneline:\n$ git log --oneline\n233c748 Update hello.txt\n708bfca Add hello.txt\n$ git diff 708bfca 233c748\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\n\n\nRemoving files\nTo remove files from your project, use git rm &lt;path&gt; followed by git commit:\n$ git rm hello.txt\n$ git commit -m \"Remove hello.txt\""
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#undoing-changes",
    "href": "newsite/theme1/PE103/vcs.html#undoing-changes",
    "title": "Version Control",
    "section": "Undoing changes",
    "text": "Undoing changes\nYou have made some changes, but you don’t actually want to keep those changes. You do not want to add them or commit them. To discard all such changes in your working directory, use git reset:\n$ git reset --hard HEAD\nIf you want to keep the changes but continue working on them without committing yet, you probably want to do:\n$ git reset --soft HEAD\nOften you can do a git reset &lt;commit&gt; to restore the state of your working tree to what is reflected by &lt;commit&gt;.\nAt any point, you can use git reflog (or “reference log”) to find where you were previously:\n$  git reflog \nddec657 HEAD@{0}: commit: Remove hello.txt\nd73c5c8 HEAD@{1}: commit: Add a third line\n233c748 HEAD@{2}: commit: Update hello.txt\n708bfca HEAD@{3}: commit (initial): Add hello.txt\nThis output is useful when you want to set the state of your working tree using git reset or git checkout.\nOnce you have made a commit, you can undo that with git revert, which will create a new commit which will revert undesired changes:\n$ git revert &lt;commit&gt;\n\nIgnoring (some) files\nSome files just should not be under version control. Examples would be:\n\nAnything generated by a build process, or a compiler, or a test suite, or some such. You do not want to commit the byte-compiled .pyc files, for example.\nSecrets, or files containing secrets (such as passwords, or tokens).\nEditor configuration files.\n\nYou should tell Git to ignore these files by adding their names to a .gitignore file, with one name on a line, so that git status will ignore them. You can use wildcard patterns such as *.pyc.\nOn lnx201, you probably should ignore the .directoryhash files automatically created by the file system; when you use macOS, you want to ignore .DS_Store files. So your .gitignore should have:\n.directoryhash\n.DS_Store\nTake a look at X-CITE course’s .gitignore for another example."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#working-with-branches",
    "href": "newsite/theme1/PE103/vcs.html#working-with-branches",
    "title": "Version Control",
    "section": "Working with branches",
    "text": "Working with branches\nA branch is a line of development. The default branch for Git is master (or main, for newer versions of Git). If you are working on a small project by yourself, you probably can commit your changes on the default branch.\nHowever, when working on bigger projects or working with others, you will want to use branches. This will ensure that there is less “churn” in the default branch, and keep things manageable.\nYou will create a branch when you implement a feature, for example, and when you are done implementing the feature, you will merge that branch to the default branch.\n\nNothing stops you from branching off from your non-main branches and merging back to the branch either. Branching and merging are (usually!) quick and easy with Git. (Well, except when there are merge conflicts. This happens when the changes on your branches diverge from the changes on the branch that you are trying to merge to.)\n\n\n\nTo find the branch on which you currently are, use:\n$ git branch\nTo create a new branch named feature-branch, use:\n$ git branch feature-branch\nTo switch to the newly created feature-branch, do:\n$ git checkout feature-branch\nYou can also create a branch and check it out in a single command:\n$ git checkout -b feature-branch\n\nMerging branches\nWhen you are ready to merge feature-branch to main branch, you will do:\n$ git checkout main \n$ git merge feature-branch\nIf the merge is not successful, you will encounter an error message:\n$ git merge feature-branch \nAuto-merging test.txt\nCONFLICT (content): Merge conflict in test.txt\nAutomatic merge failed; fix conflicts and then commit the result.\nNow git status would indicate that you have a situation:\n$ git status \n# On branch master\n# You have unmerged paths.\n#   (fix conflicts and run \"git commit\")\n#\n# Unmerged paths:\n#   (use \"git add &lt;file&gt;...\" to mark resolution)\n#\n#   both modified:      test.txt\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit would have added some conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt; and ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;) to indicate the places where it had trouble merging.\n\n\ntest.txt\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nline 1\nline 2\n=======\nline 3\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\n\nThis is when you open a text editor, find the conflicts, and resolve them manually. It is up to you to accept what you want and reject what you do not. You should do that, and remove the conflict markers. You will probably end up with:\n\n\ntest.txt\n\nline 1\nline 2\nline 3\n\nNow you can commit this with:\n$ git commit -am \"Resolve conflict\"\nIn more complicated situations, you will probably have to use a merge tool. See git-mergetool for details."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#working-with-tags",
    "href": "newsite/theme1/PE103/vcs.html#working-with-tags",
    "title": "Version Control",
    "section": "Working with tags",
    "text": "Working with tags\nTags are used to mark certain points in a repository’s history as important in some manner. In the case of a release, you will want to tag the commit associated with the release with v1.0.0 or v1.0.1, for example.\nYou can list the tags present in your repository with:\n$ git tag\nYou can create a tag with:\n$ git tag 1.0.0\nYou can have a tag point at a specific commit:\n$ git tag 1.0.1 3de49cc\nTo delete a tag:\n$ git tag -d 1.0.1"
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#working-with-remote-repositories",
    "href": "newsite/theme1/PE103/vcs.html#working-with-remote-repositories",
    "title": "Version Control",
    "section": "Working with remote repositories",
    "text": "Working with remote repositories\nThis far, we’ve talked about how you work in a local repository.\nYou will want to be able to share your work with others, and be able to check out the projects others have created. This is typically done with hosting your repository somewhere in a server. These are called remote repositories.\n\nYou will git clone remote repositories, and you will git fetch or git pull updates them from, and you will git push your changes to them. In practice, you will very likely use an account on a code hosting site such as GitHub, and push your code to a repository there.\n\n\n\n\n\n\nNote\n\n\n\nWhile working with a central remote repository is the most usual Git workflow, it is worth noting that Git enables other kinds of workflows also. See the chapter Distributed Git - Distributed Workflows from Pro Git for some examples.\n\n\nThese very notes that you are currently reading are version controlled using Git, and they are hosted at the GitHub repository at https://github.com/RENCI-NRIG/X-CITE/. You can get a local copy of that repository using git clone command:\n$ git clone https://github.com/RENCI-NRIG/X-CITE.git\nCloning into 'X-CITE'...\nremote: Enumerating objects: 948, done.\nremote: Counting objects: 100% (485/485), done.\nremote: Compressing objects: 100% (266/266), done.\nremote: Total 948 (delta 228), reused 421 (delta 172), pack-reused 463\nReceiving objects: 100% (948/948), 4.52 MiB | 0 bytes/s, done.\nResolving deltas: 100% (446/446), done.\nYou can cd into that directory now with cd X-CITE, and then run some git commands such as git status and git log there:\n$ cd X-CITE/\n$ git status\n# On branch main\nnothing to commit, working directory clean\n$ git log\ncommit fd95497e30827d52dd99855a0e1be99b3db4282e\nAuthor: Sajith Sasidharan &lt;sajith@hcoop.net&gt;\nDate:   Mon Apr 22 09:27:27 2024 -0500\n\n    Mention the trace module\n\ncommit 7d55b104b50f8b1f5b8201765ceac8541f9543df\nAuthor: Sajith Sasidharan &lt;sajith@hcoop.net&gt;\nDate:   Mon Apr 22 09:16:44 2024 -0500\n\n    Add a line\n\n[... more output, elided for brevity ...]\n\nGetting updates from a remote repository\nAfter you originally cloned a remote repository, other people might have pushed new commits or branches or tags there. Using git fetch command will retrieve those updates, but without committing them to your local repository.\n$ git fetch origin\nHere origin refers to the remote repository. You can list remote repository with git remote, and git remote -v will print some more details about them.\nYou can then do git diff origin/main to view the changes, and do git merge origin/main (or git rebase origin/main) to get those changes in your local copy.\nWhen you want to fetch updates from the remote repository and automatically merge them into your current branch in one step, you will do a git pull:\n$ git pull\nHere you are fetching and merging changes from the main branch in the remote repository to your current local branch.\n\n\nGetting updates to a remote repository\nThe inverse of a git pull is git push. You use a git push to get your changes to a remote repository:\n$ git push origin main\nIf you are trying to push to X-CITE repository, this will ask for your username and password, and even if they are correct, git push will most likely fail, because you do not have the necessary permissions to write to the remote repository.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to add your changes to X-CITE repository on GitHub, you will want to fork the repository, make your changes in your fork of the repository, and send a pull request to X-CITE repository. This is a somewhat separate discussion. Start with GitHub’s documentation to learn more."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#software-forges",
    "href": "newsite/theme1/PE103/vcs.html#software-forges",
    "title": "Version Control",
    "section": "Software forges",
    "text": "Software forges\nA software “forge” is a hosting service that can host your Git repositories, and provide many additional services (such as bug tracking, code reviews, continuous integration, etc) that helps you collaborate with other people.\n\nGitHub.com happens to be the most popular one.\nGitlab.com\n\nMany organizations and projects choose to self-host a version of GitLab. See CLASSE GitLab, for example.\n\nCodeberg.org, which runs Forgejo software.\nSourceHut.org\nBitBucket.com\n\nSome organizations and people prefer to self-host a forge, and some people prefer no forge at all. Since Git is a distributed version control system, you should be able to collaborate with no forge at all: you can share your changes as email attachments, if you want. Git was originally developed for Linux kernel development, which uses no forge at all."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#exercises",
    "href": "newsite/theme1/PE103/vcs.html#exercises",
    "title": "Version Control",
    "section": "Exercises",
    "text": "Exercises\n\nCreate an account on GitHub.com (or another forge of your choice), if you do not have an account there already. Create a new repository on the forge of your choice. Push some code that you are working on to that repository.\nMake some more changes to your project, and commit them. Push those commits also to the Git repository.\nCreate a Git tag (based on today’s date, or a version number), and push the tag to your repository.\nIf you want to add a new feature to your project, create a branch, and make your changes on the branch. When you are done, merge the feature branch to your main or master branch.\nIf you find errors in these notes, or want to suggest improvements, report them by creating an issue on https://github.com/RENCI-NRIG/X-CITE/.\nIf you want to propose some changes to these notes (such a fixing a mistake or improving these notes), create a pull request against https://github.com/RENCI-NRIG/X-CITE/."
  },
  {
    "objectID": "newsite/theme1/PE103/vcs.html#references",
    "href": "newsite/theme1/PE103/vcs.html#references",
    "title": "Version Control",
    "section": "References",
    "text": "References\n\nPro Git\nGit cheat sheet\nVersion Control module of The Missing Semester of Your CS Education.\nAbout Git section of GitHub documentation."
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html",
    "href": "newsite/theme1/PE103/debugging.html",
    "title": "Debugging",
    "section": "",
    "text": "Debugging is the process of finding errors, unexpected behavior, or performance issues in software, and fixing them.\nSometimes the problems may seem inscrutable or mysterious. Often in those cases, the real problem would turn out that your mental model of how the thing works is not quite accurate. It would be helpful to deploy the Feynman Algorithm:\n\nWrite down the problem.\nThink real hard.\nWrite down the solution.\n\nObviously you have to follow these steps in sequence. But what if you do not understand the problem quite well yet?\nWhen figuring out problems with software written in Python, there are multiple tools at your disposal. You might find one of these approaches or a combination of several approaches helpful.\n\n\nYou will add print() statements at various points in your code, in order to help you understand the flow of execution and find out where issues occur.\nLet us add some print statements to our rather contrived example.\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    print(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n    print(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nprint(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n\nNow you can watch the execution of the code:\n$ python3 temperature.py\ninput in deg C: 0\noutput in deg F: 32.0\n0 deg C is 32.0 deg F\n\n\n\nPython standard library provides a logging module, which you can use to log various events in your code. One benefit of using logging is that your application’s logs can include log messages from the libraries you use (if they are set up to use logging), so you will have more information to work with.\nThe module provides enough knobs to tune things like: level of logging (you can choose to log statements based on their severity, from all messages to just the critical messages), the format of log statements, the location of log files, time stamps of log statements, etc.\nYou can set up your module to log its actions like so:\n\n\ntemperature.py\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef celsius_to_fahrenheit(celsius):\n    logger.info(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n\n    logger.info(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nAnd you should do some setup of logging module in your main method:\n\n\nmain.py\n\nimport logging\nfrom temperature import celsius_to_fahrenheit\nlogger = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(filename=\"temperature.log\", level=logging.INFO)\n    logger.info(\"Started\")\n    celsius_to_fahrenheit(0)\n    logger.info(\"Finished\")\n\nif __name__ == \"__main__\":\n    main()\n\nRunning the code above with python3 main.py will write log statements to a file named temperature.log.\n\n\ntemperature.log\n\nINFO:__main__:Started\nINFO:__main__:Started\nINFO:temperature:input in deg C: 0\nINFO:temperature:output in deg F: 32.0\nINFO:__main__:Finished\n\n\n\n\nPython standard library has a pdb module, which provides an interactive debugging tool, or a “debugger”. Debuggers allow you to examine code while it is running.\nUsing a debugger, you can set breakpoints where the execution will stop, you can print values, you can step into through the execution of methods, etc.\nYou can run a program under pdb with python3 -m pdb &lt;program.py&gt;, like so:\npython3 -m pdb temperature.py\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) help\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    c          d        h         list      q        rv       undisplay\na      cl         debug    help      ll        quit     s        unt\nalias  clear      disable  ignore    longlist  r        source   until\nargs   commands   display  interact  n         restart  step     up\nb      condition  down     j         next      return   tbreak   w\nbreak  cont       enable   jump      p         retval   u        whatis\nbt     continue   exit     l         pp        run      unalias  where\n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\n(Pdb) next\n&gt; /tmp/temperature.py(5)&lt;module&gt;()\n-&gt; print(f\"0 deg C is {celsius_to_fahrenheit(0)}\")\n(Pdb) step\n--Call--\n&gt; /tmp/temperature.py(1)celsius_to_fahrenheit()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) p celsius\n0\n(Pdb) next\n&gt; /tmp/temperature.py(2)celsius_to_fahrenheit()\n-&gt; fahrenheit = (celsius * 9 / 5) + 32\n(Pdb) next\n&gt; /tmp/temperature.py(3)celsius_to_fahrenheit()\n-&gt; return fahrenheit\n(Pdb) p fahrenheit\n32.0\n(Pdb) continue\n0 deg C is 32.0\nThe program finished and will be restarted\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) exit\nAnother typical usage to break into the debugger is to insert an import pdb; pdb.set_trace() line into your code:\n\n\ntemperature.py\n\nimport pdb; pdb.set_trace()\n\ndef celsius_to_fahrenheit(celsius):\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the code with python3 temperature-pdb.py, and use various pdb commands.\nOr you can insert a breakpoint() statement at the location you want to break into the debugger:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    breakpoint()\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the program with python3 -m temperature.py. Once the execution reaches the line with breakpoint(), you will be dropped into the pdb shell.\n\n\n\nPython’s trace module allows you to trace program execution:\n$ python3 -m trace --trace code/temperature.py \n --- modulename: temperature, funcname: &lt;module&gt;\ntemperature.py(1): def celsius_to_fahrenheit(celsius):\ntemperature.py(12): print(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n --- modulename: temperature, funcname: celsius_to_fahrenheit\ntemperature.py(9):     fahrenheit = (celsius * 9 / 5) + 32\ntemperature.py(10):     return fahrenheit\n0 deg C is 32.0 deg F\nTo learn more, run python3 -m trace --help, and read the module documentation.\n\n\n\nIt is much easier to debug code when you have tests. The tests you write should help you test individual components of your code, and isolate points of failures. You can use a combination of print() statements, logging, and pdb.\nSee Testing for some examples.\n\n\n\nGet a friend or colleagues to review your code to identify potential issues, provide feedback, and suggest improvements. Another set of eyes can often spot problems that you might have overlooked. Explaining your code line by line to someone else is often helpful in finding the flaws in it.\nIf no human is immediately available, explain your code to a rubber duck! This very powerful technique is called rubber duck debugging, or simply, “rubberducking”.\n\n\n\nIDEs such as PyCharm and VS Code have built-in debugging facilities.\n\nJupyterLab also has a built-in debugger:\n\nHow to use these are left as an exercise to the reader. ;-)"
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-print-statements",
    "href": "newsite/theme1/PE103/debugging.html#using-print-statements",
    "title": "Debugging",
    "section": "",
    "text": "You will add print() statements at various points in your code, in order to help you understand the flow of execution and find out where issues occur.\nLet us add some print statements to our rather contrived example.\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    print(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n    print(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nprint(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n\nNow you can watch the execution of the code:\n$ python3 temperature.py\ninput in deg C: 0\noutput in deg F: 32.0\n0 deg C is 32.0 deg F"
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-logging",
    "href": "newsite/theme1/PE103/debugging.html#using-logging",
    "title": "Debugging",
    "section": "",
    "text": "Python standard library provides a logging module, which you can use to log various events in your code. One benefit of using logging is that your application’s logs can include log messages from the libraries you use (if they are set up to use logging), so you will have more information to work with.\nThe module provides enough knobs to tune things like: level of logging (you can choose to log statements based on their severity, from all messages to just the critical messages), the format of log statements, the location of log files, time stamps of log statements, etc.\nYou can set up your module to log its actions like so:\n\n\ntemperature.py\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef celsius_to_fahrenheit(celsius):\n    logger.info(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n\n    logger.info(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nAnd you should do some setup of logging module in your main method:\n\n\nmain.py\n\nimport logging\nfrom temperature import celsius_to_fahrenheit\nlogger = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(filename=\"temperature.log\", level=logging.INFO)\n    logger.info(\"Started\")\n    celsius_to_fahrenheit(0)\n    logger.info(\"Finished\")\n\nif __name__ == \"__main__\":\n    main()\n\nRunning the code above with python3 main.py will write log statements to a file named temperature.log.\n\n\ntemperature.log\n\nINFO:__main__:Started\nINFO:__main__:Started\nINFO:temperature:input in deg C: 0\nINFO:temperature:output in deg F: 32.0\nINFO:__main__:Finished"
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-the-pdb-module",
    "href": "newsite/theme1/PE103/debugging.html#using-the-pdb-module",
    "title": "Debugging",
    "section": "",
    "text": "Python standard library has a pdb module, which provides an interactive debugging tool, or a “debugger”. Debuggers allow you to examine code while it is running.\nUsing a debugger, you can set breakpoints where the execution will stop, you can print values, you can step into through the execution of methods, etc.\nYou can run a program under pdb with python3 -m pdb &lt;program.py&gt;, like so:\npython3 -m pdb temperature.py\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) help\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    c          d        h         list      q        rv       undisplay\na      cl         debug    help      ll        quit     s        unt\nalias  clear      disable  ignore    longlist  r        source   until\nargs   commands   display  interact  n         restart  step     up\nb      condition  down     j         next      return   tbreak   w\nbreak  cont       enable   jump      p         retval   u        whatis\nbt     continue   exit     l         pp        run      unalias  where\n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\n(Pdb) next\n&gt; /tmp/temperature.py(5)&lt;module&gt;()\n-&gt; print(f\"0 deg C is {celsius_to_fahrenheit(0)}\")\n(Pdb) step\n--Call--\n&gt; /tmp/temperature.py(1)celsius_to_fahrenheit()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) p celsius\n0\n(Pdb) next\n&gt; /tmp/temperature.py(2)celsius_to_fahrenheit()\n-&gt; fahrenheit = (celsius * 9 / 5) + 32\n(Pdb) next\n&gt; /tmp/temperature.py(3)celsius_to_fahrenheit()\n-&gt; return fahrenheit\n(Pdb) p fahrenheit\n32.0\n(Pdb) continue\n0 deg C is 32.0\nThe program finished and will be restarted\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) exit\nAnother typical usage to break into the debugger is to insert an import pdb; pdb.set_trace() line into your code:\n\n\ntemperature.py\n\nimport pdb; pdb.set_trace()\n\ndef celsius_to_fahrenheit(celsius):\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the code with python3 temperature-pdb.py, and use various pdb commands.\nOr you can insert a breakpoint() statement at the location you want to break into the debugger:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    breakpoint()\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the program with python3 -m temperature.py. Once the execution reaches the line with breakpoint(), you will be dropped into the pdb shell."
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-the-trace-module",
    "href": "newsite/theme1/PE103/debugging.html#using-the-trace-module",
    "title": "Debugging",
    "section": "",
    "text": "Python’s trace module allows you to trace program execution:\n$ python3 -m trace --trace code/temperature.py \n --- modulename: temperature, funcname: &lt;module&gt;\ntemperature.py(1): def celsius_to_fahrenheit(celsius):\ntemperature.py(12): print(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n --- modulename: temperature, funcname: celsius_to_fahrenheit\ntemperature.py(9):     fahrenheit = (celsius * 9 / 5) + 32\ntemperature.py(10):     return fahrenheit\n0 deg C is 32.0 deg F\nTo learn more, run python3 -m trace --help, and read the module documentation."
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-unit-tests",
    "href": "newsite/theme1/PE103/debugging.html#using-unit-tests",
    "title": "Debugging",
    "section": "",
    "text": "It is much easier to debug code when you have tests. The tests you write should help you test individual components of your code, and isolate points of failures. You can use a combination of print() statements, logging, and pdb.\nSee Testing for some examples."
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#talk-to-a-friend-or-a-rubber-duck",
    "href": "newsite/theme1/PE103/debugging.html#talk-to-a-friend-or-a-rubber-duck",
    "title": "Debugging",
    "section": "",
    "text": "Get a friend or colleagues to review your code to identify potential issues, provide feedback, and suggest improvements. Another set of eyes can often spot problems that you might have overlooked. Explaining your code line by line to someone else is often helpful in finding the flaws in it.\nIf no human is immediately available, explain your code to a rubber duck! This very powerful technique is called rubber duck debugging, or simply, “rubberducking”."
  },
  {
    "objectID": "newsite/theme1/PE103/debugging.html#using-ides",
    "href": "newsite/theme1/PE103/debugging.html#using-ides",
    "title": "Debugging",
    "section": "",
    "text": "IDEs such as PyCharm and VS Code have built-in debugging facilities.\n\nJupyterLab also has a built-in debugger:\n\nHow to use these are left as an exercise to the reader. ;-)"
  },
  {
    "objectID": "newsite/theme1/PE101/index.html",
    "href": "newsite/theme1/PE101/index.html",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "By itself, Python provides everything you need to write programs. These programs won’t have a fancy user interface and they may not run very fast, but they’ll work. If that’s all Python offered, it might have become a popular language but it wouldn’t have taken over most of the world the way it has. No, what Python has going for it is a simple way to take commonly-used chunks of code, wrap them up neatly into sharable budles, and distribute those bundles far and wide. The mechanism for doing this in Python is called packages.\nIn this training unit, PE101-01, we’re going to look at some of the packages that come with Python. These are packages that you can count on being available anywhere you can run Python. In the next unit, PE101-02, we’ll look at how to find and use packages hosted in repositories available to anyone but not necessarily already installed where you’re running your programs.\nPython is, by itself, a rather simple language. The PE100 series of units has introduced you to almost all of the language. The language is kept small by moving the “nice to have, but not really necessary” parts into their own independent packages. Let’s start with an example:\n\n\npi equals 3.141592653589793\nThere are 2652 possible outcomes when drawing two cards from a deck\nThe natural logarithm of 7.994 is 2.0786912602891316\n\n\n\n\nThere are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details.\n\n\n\nSo far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory.\n\n\n\nAs we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks.\n\nSource: PE101-01: Using Python Packages and Modules"
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#packages",
    "href": "newsite/theme1/PE101/index.html#packages",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "There are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details."
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#modules",
    "href": "newsite/theme1/PE101/index.html#modules",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "So far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory."
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#coming-up-next-packages-from-the-outside-world",
    "href": "newsite/theme1/PE101/index.html#coming-up-next-packages-from-the-outside-world",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "As we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks."
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#conda---hard-problems-made-solvable",
    "href": "newsite/theme1/PE101/index.html#conda---hard-problems-made-solvable",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Conda - hard problems made solvable",
    "text": "Conda - hard problems made solvable\nFortunately, there is Conda, a software tool and a repository of its own. The conda developers keep a subset of the half billion packages that are available and they ensure that their repository reflects a combination of versions that should work together. They do the hard work, we take advantage of it. They stay in business by selling their tools to commercial users but, being a research organization, we’re not required to pay.\nConda also has another useful trick: it can take advantage of Python’s virtual environments to let you load outside packages into a completely private space. This way, when you download and install the “instantnobelprize” package (I made that up), it’s only written to your own directories. Other users, and the system as a whole, are protected from whatever it might contain.\nSetting up Conda and using it with Jupyter notebooks takes a little bit of work and has to be done from the command line, but so often it’s worth it. If you haven’t used the command line yet, take a look at the training units in SF100 on the Linux command line and scripting.\nWhat follows is taken directly from the CLASSE wiki entry for JupyterHub with just a few modifications."
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#python-environments",
    "href": "newsite/theme1/PE101/index.html#python-environments",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Python Environments",
    "text": "Python Environments\nA Python environment is a local, unique to a user, repository plus a copy of the Python interpreter itself. Having a private environment is how we can load specific versions of packages even when the server has a different one. It even lets us install specific versions of python without affecting anyone else.\nWhen you launch a new notebook, you are presented with a dropdown to select your desired python kernel. The default Python 3 kernel is a CLASSE-IT maintained conda environment in /nfs/opt/anaconda3/envs/python3"
  },
  {
    "objectID": "newsite/theme1/PE101/index.html#adding-new-environments",
    "href": "newsite/theme1/PE101/index.html#adding-new-environments",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Adding New Environments",
    "text": "Adding New Environments\nIn addition, you can install your own python environments and have them added as an option when creating new notebooks.\nCreate your own python environment using your desired python installation. Please see LinuxSoftwareDevelopment for a list of centrally maintained python environments, and further down LinuxSoftwareDevelopment for tips on creating your own conda installation.\nInstall anything you like in the environment, but you MUST at least install ipykernel. For example\npip install ipykernel\nActivate the new environment. If using conda, this would look something like:\nsource /path/to/conda/install/bin/activate conda activate my-python-env\nAdd the virtual environment as a jupyter kernel using\npython -m ipykernel install --user --name=my-python-env --display-name \"My Python Env\"\nThis adds the kernel to ~/.local/share/jupyter/kernels/ and now it can be used by Jupyter. When you create a new notebook now, “My Python Env” will be one of your choices."
  },
  {
    "objectID": "newsite/theme2/SF200/parallel-computing.html",
    "href": "newsite/theme2/SF200/parallel-computing.html",
    "title": "Parallel Computing Concepts",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme2/SF100/getting-started.html",
    "href": "newsite/theme2/SF100/getting-started.html",
    "title": "Getting Started - Accessing the Linux Cluster with NoMachine and SSH",
    "section": "",
    "text": "In the course of your research at CHESS, you’ll need to do a lot of computation. What constitutes “a lot” is discipline-specific and varies tremendously depending on what the exact processing is. It’s entirely possible that much of your work can be completed on your laptop. On the other hand, it’s also likely that at least some, and possibly a great deal, of your computing will need to be done on the Linux cluster. In simple terms, the cluster is a collection of machines running Linux (a variant of Unix) and able to coordinate workflows, assigning the running of “stuff” to an appropriate server at the right time.\nWhile there are many servers in the cluster, one of them is special. It’s name is “LNX201”, and it’s responsible for accepting new jobs and dealing with the output. It’s also the right server to use for developing your programs. Keep in mind that dozens of people may be using LNX201 at the same time, so don’t use it for running lengthy computations. If your job runs for more than a few seconds, it’s probably time to learn how to run it on some of the worker nodes. We’ll talk about that subject in SF201 when we get our first look at batch processing. Until then, we’ll keep our examples comfortably small and LNX201 will be the perfect home.\n\nRemote Desktop Access\nWhy do we need remote access at all? My laptop has a pretty decent processor, nice graphics, and a tolerable keyboard. Why don’t I just use that? The answer, or one of the answers, is that the computers in the cluster are larger and more powerful. LNX201 has 16 CPUs (“cores”, sort of) and 128 gigabytes of RAM. Each of the cores in the server are several times faster than the ones in a laptop. In short, the compute cluster can handle jobs that would bring a laptop to its knees, fans screaming, too hot to handle.\nThere are two kinds of remote access and both have their strengths and weaknesses. The first approach we’ll look at is the “virtual desktop”. This strategy takes the full graphical experience of using a computer, potentially in a far-flung destination, and shows what is going on right in front of you. The effect is to make your computer’s screen, keyboard, and mouse act like they’re connected to the remote machine, albeit with a really, really long set of cables. The second remote access approach is to use a program on your laptop or desktop to connect over the network to a remote machine and just pass characters back and forth. It gives you, for all practical purposes, something that looks like a window for using the command line, only the computer running those commands is remote.\n\n\nVirtual Desktops - NoMachine\nCHESS uses a software system called “NoMachine” for remote Virtual Desktop access. NoMachine has oodles (a scientific term) of features and configuration settings, but if you just want a basic Virtual Desktop that is good enough for all but a few special cases then you can just play on Easy Mode: use the web client that runs inside a browser and just take the default settings.\nThe rest of this lesson assumes you have: 1. A CLASSE username and password 2. The DUO app installed on your phone and configured for access to CLASSE 3. The “Pritunl” Virtual Private Network software installed on your laptop.\nThe instructions for each of these steps are in the very first steps shown in CLASSE-IT 2025 Summer Student Guide.\nGo ahead and connect to the CLASSE internal network through the Pritunl VPN software. Then go to your web browser and put in the destination URL https://nomachine.classe.cornell.edu. In just a moment, you should be seeing the initial login screen for NoMachine itself. Put in your username and password, then click on “OK” in the lower right corner.\n\nThis starts the login process, the next part of which is using DUO for two-factor authentication:\n\nOn this screen, enter a “1” if you want to use the Duo app on your phone. Otherwise, you can enter “2” and the Duo servers will call you to verify that the person trying to log in at least controls that phone number. If you selected “1”, then in just a moment your phone will beep, you can go to the Duo app, and click the green button to approve of the login request. Completing the Duo process will take you to the next screen.\n\nIn this screenshot, I’ve already selected “lnx201” so you can spot it right away. This is the one you’ll want to use. Go on and double click it to start logging in to LNX201. You’ll see a screen listing the available session types. In principle you could confighure different graphical environments here, but in reality you should just take the default.\n\nClicking OK on that will take you a quick reminder of some commands that might be useful when you’re connected. There is a checkbox you can select when you get familiar with this and that will keep this screen from showing up again.\n\nIn the meantime, click OK to continue to the next helpful screen:\n\nAnd then OK again to see a screen with some helpful advice, to be sure, but also a chance to select how the screen resolution should be handled.\n\nOn this screen I have selected the first option, “Scale the remote desktop to fit into the window”, because I like being able to change the size of my browser window and yet still see the same amount of screen space on the remote machine. Try different options here - you won’t break anything. Clicking “OK” will take you to the final information-and-setup screen:\n\nThis screen gives options for setting up the screen resolution on the remote computer. Because LNX201 is a server and not a machine with an actual graphics card and a monitor, all the sessions use simulated (virtual) desktops and that means they can be set to any resolution you want. Selecting “Don’t resize” is a reasonable starting point. You might want to experiment with the others later. In any case, we can finally click on “OK” for this screen and connect to the desktop session!\n\nIt may take a moment for everything to start up, and another moment for the remote screen drawing to catch up, but pretty soon you’ll see a Linux desktop environment. Desktop environments have mostly converged on roughly the same concepts. The one you see here is “Xfce”, and it traces its heritage to the great Unix Workstations of the 1980s. MacOS, similarly, is derived from NeXTstep which is based on work on Unix Workstations in the 1980s. Various versions of Windows are based on… you get the idea. They’re all more alike than different.\nIronically, perhaps, the most common thing we’ll do on a remote desktop session is “start a terminal window”. In this case, the task is dead simple: click on the “Applications” menu and select the “Terminal Emulator” item.\n\nIn just a moment, the terminal window appears:\n\nJust as you would expect, we have a file browser available and it’s more or less equivalent to Windows’ Explorer or to the Mac’s Finder. Double Click on the “Home” folder…\n\nQuite a few applications are already installed in the Linux cluster, some of which can really take advantage of having graphics through a remote desktop environment. One example is Mathematica:\n\nAt this point, we’ve seen how to log in to the Linux cluster through a virtual desktop session, seen how to move around just a bit in that graphical environment, and teased ourselves with a bit of mathematical loveliness. All good things must come to an end, eventually, and this session is no exception. When you’re ready to go, click on your name in the upper right corner of the desktop, select the “Log Out…” menu item, and log off. This will cause the session to end and you’ll go back to the initial login screen for NoMachine.\n\nNow that we’ve gone through the basics, there might be (should be?) one nagging… consideration. Doesn’t this seem like a fairly complicated and not terribly fast way to use to use a Linux machine remotely if all I want to do is work in a terminal session?\nYes. Yes it is.\nFortunately, there’s another way. It’s called “SSH”.\n\n\nRemote Terminal Access with SSH\nTo have a good understanding of SSH, you need to know just a little bit about the history of Unix (and Linux is just a modern version of Unix). In the Old Days (1974-ish) Unix ran on a machine the size of home refrigerator and the server didn’t even have a keyboard. To interact with it, teletypes were plugged into serial ports on the back on the computers. Users performed all of their interaction with the machine by pressing a key on the teletype (sending a character) and waiting for the computer to send back characters to be printed. By the late 1970s, teletypes were getting rare quickly because they had been replaced by “terminals” (a keyboard, a picture tube, and enough electronics to draw the characters on the screen). The connection to the computer was still a serial cable. To this day, you can plug a USB-to-Serial converter into a Linux machine, plug in a teletype or a terminal, set up a configuration file, and use the computer like it’s 1979. It’s fun a time or two, but the novelty wears off.\nSerial cables had a length limitation and a speed one. Half a mile was easy (9600 bits per second, even!) but a mile was getting unreliable at any speed much over 1200 bits per second. Beyond a mile, longer distances required all sorts of interesting hardware and lots of special cables. Covering a whole campus required a network of some sort, and what the research and academic world standardized on was “Internet Protocol” (IP). One of the first programs ever written for IP was a program called “telnet”. This program used the network to set up a stream of characters to and from a remote machine to the local one. It was like having a serial cable, except it was literally a thousand times faster and could reach around the world. To this day, telnet is still the “official” way to use Internet Relay Chat (IRC). Plus, it makes you look cool.\nTelnet was great in the early days of networking. But we were so naive back then. Anyone who could get access to the network could eavesdrop on whatever communications was going over these telnet connections. Most of the traffic was utterly boring - email, Unix commands, that sort of thing. But there was one thing worth snooping on, and it was the first two things a user typed when they set up a connection: their username and their password. In the late 1980s and early 1990s, this was mostly a theoretical concern. The problem was the explosive growth of the Internet in the “Dot Com” era (1995-2000). Once there were a critical mass of Internet users, and once these users started using the Internet for things that were worth actual money, security started moving front and center. By this point, sending anything over the internet without encrypting it became a VBA (Very Bad Idea). Thus was born the Secure SHell (SSH).\nThe simple explanation of SSH is “it’s just telnet, except all the communication is encrypted”. This statement is true, in the sense that SSH replicates every telnet function, but SSH also does some other tricks. For our purposes, we’re going to keep it simple and just use it like a “better telnet”.\nTo use SSH on your computer, you need to bring up a “Terminal Window” (Mac) or “Command Window” (Windows). On a Mac, use Finder to browse the Applications folder. In there, open the Utilities folder, then double click on Terminal. (After a while, you might decide to just use Spotlight to search for Terminal.app, but it’s up to you). On a PC, the easiest thing to do is press and release the “Windows” key, then type “cmd” and press enter. That will bring up a command window.\nEither way, you’re now looking at a terminal window of some sort. Characters are entered and sent to programs, and programs send characters back out and the terminal window draws them on the screen. Just like a teletype, except no paper and no oily smell. Windows users will probably see something similar to C:\\&gt;. Mac users will see something more akin to escott@RENCI_LP91DX62MX ~ %. Both of these are “prompts”, meaning “you can type a command here”. In both cases the command to type is ssh lnx201.classe.cornell.edu. The lower case letters in “ssh” are important. Linux commands almost never contain uppercase letters. Go ahead, enter that ssh command, and press Enter. If this is the very first time you’ve ssh’ed into lnx201 from the machine you’re at, you’ll get some warnings just to remind you to be wary:\nThe authenticity of host 'lnx201.classe.cornell.edu (128.84.45.81)' can't be established.\nED25519 key fingerprint is SHA256:cIplmL7rqVGlAKYlTwtfml+KiSvUuBhgKuyjkPbde7E.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYou can type “yes” here and press enter, and ssh will tell us it’s storing that cryptographic fingerprint. The “fingerprint”, by the way, refers to a encrypted, secure way to tell we’re really using the server we think we are. It has nothing to do with ridges and swirls on our fingertips. We’re going to assume no one has hijacked the actual name of the server and put up a fake one. There is a way to protect against this, but it’s a (minor?) hassle so it’s rarely done. Since we’re doing this the rough-and-ready way, ssh informs us it’s going to store a cryptographic “fingerprint” for the server. It will check that fingerprint every time we try to connect in the future and make sure it matches. If it doesn’t, it’ll warn us to be suspicious and not continue until we’re at least comfortable we know why.\nWhether it needs to notify us of anything or not, it will always prompt with the following:\n(escott@lnx201.classe.cornell.edu) Password:\nAt this point, type your password. Nothing will appear as you’re typing, not even any little dots. When you think about it, teletypes didn’t have any little dots.\nIf you type your password correctly, you’ll see one more prompt:\n(escott@lnx201.classe.cornell.edu) Duo two-factor login for escott\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-3880\n 2. Phone call to XXX-XXX-3880\n\nPasscode or option (1-2):\nEnter a “1” and press enter. You’ll hear Duo chiming on your phone. Approve your login, let all of that percolate through the system for a second, and you’ll see your command line prompt on LNX201:\n[escott@lnx201 ~]$\nAt this point, you can run all of your favorite commands: ls, mkdir, the whole bunch. When you are done, don’t forget to log out. The “exit” command logs you out and breaks down the SSH connection.\n\n\nParting Shot, and a Note to the Nerds\nWe’ve just looked at two very different ways to remotely access the CLASSE Linux servers that CHESS uses. The remote desktop approach, using NoMachine, has the advantage of being relatively easy to use and of fully supporting graphical programs but at the expense of potentially quite a bit of lag time between doing something and seeing the results on the screen. Using SSH, on the other hand, will usually feel more responsive and use less network bandwidth but is for command line use only - no graphics. Which one to use is a matter of deciding what tradeoffs are acceptable. If you’re on campus then bandwidth isn’t an issue. Use both - no one will judge you. If you’re out in the boondocks and getting network connectivity through a bad cellphone connection then you might want to use SSH.\nSSH has a few more tricks up its sleeve. One is that it can copy files. When you copy files this way you use the “scp” command instead of “ssh”, but it’s really the same program behind the scenes. Another trick is that it can do “port forwarding” - a poor substitute for a VPN, but sometimes you’re in a situation where you’re not allowed to install a VPN to get to your instrument, for instance. And finally, yes, I have to acknowledge this for the nerds who’ve been waiting to ambush me… ssh can be used to run graphical programs remotely. The “-X” and “-Y” options are the relevant ones. Results are “good enough” for simple programs over very fast networks, but slower networks make the experience miserable and complicated graphical programs (Chrome? Just saying…) are awful no matter how fast the network is.\nComing up next, we’ll use these remote connections to start learning about command line tools and how to use them."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#assumptions",
    "href": "newsite/theme2/SF100/slides.html#assumptions",
    "title": "Systems Fundamentals",
    "section": "Assumptions",
    "text": "Assumptions\n\nYou have your CLASSE accounts set up.\n\nTalk to CLASSE staff otherwise!\n\nOptions:\n\nUse ssh\nUse NoMachine\nUse JupyterLab"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#use-ssh",
    "href": "newsite/theme2/SF100/slides.html#use-ssh",
    "title": "Systems Fundamentals",
    "section": "Use SSH",
    "text": "Use SSH\n\nssh ${username}@lnx201.classe.cornell.edu\n\nUse terminal or iTerm on macOS.\nUse whichever terminal you want on Linux.\nUse PuTTY (https://putty.org/) on Windows.\n\n\n\nUse terminal or iTerm on macOS. There are other choices too, but these seem popular.\nUse whatever terminal you want on Linux. You know what you are doing, and you know how to figure out stuff.\nUse PuTTY on Windows. I am not up to speed on Windows."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#use-nomachine",
    "href": "newsite/theme2/SF100/slides.html#use-nomachine",
    "title": "Systems Fundamentals",
    "section": "Use NoMachine",
    "text": "Use NoMachine\n\nhttps://wiki.classe.cornell.edu/Computing/NoMachine\n\n\nThere is a client that you can install.\nThere is also web access, which I have used a few times. Seemed shaky."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#use-jupyterlab",
    "href": "newsite/theme2/SF100/slides.html#use-jupyterlab",
    "title": "Systems Fundamentals",
    "section": "Use JupyterLab",
    "text": "Use JupyterLab\n\nhttps://jupyter01.classe.cornell.edu/\n\n\nUse the terminal icon on the launcher.\nOr use File -&gt; New -&gt; Terminal"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#linux",
    "href": "newsite/theme2/SF100/slides.html#linux",
    "title": "Systems Fundamentals",
    "section": "Linux",
    "text": "Linux\n\nA popular operating system.\n\n(Actually an OS kernel, plus userland from various other projects. But those are details…)\n\nUnix-like, which traces back to 1969, therefore has accumulated quirks.\n\nExpect “hysterical raisins”."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#lnx201",
    "href": "newsite/theme2/SF100/slides.html#lnx201",
    "title": "Systems Fundamentals",
    "section": "lnx201",
    "text": "lnx201\n\nThe Linux host we’ll be using is lnx201.classe.cornell.edu.\nRuns a distribution called Scientific Linux.\nGood enough for general use.\nDo not run anything resource heavy on lnx201.\n\n“Heavy” in terms CPU, memory, network usage etc.\nlnx201 is a shared resource.\n\nThere is a Compute Farm to run heavy things.\n\n\n\nI am not the expert on lnx201 or compute farm. I just happen to present today!"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#the-command-line",
    "href": "newsite/theme2/SF100/slides.html#the-command-line",
    "title": "Systems Fundamentals",
    "section": "The command line",
    "text": "The command line\n\n\nYou will type commands in a shell, at the shell prompt, hit enterenter key, and then things happen.\n\n\n\nAll of this is a text user interface.\nAs opposed to clicking on GUI widgets.\nThe prompt is that piece of text that begins with [ and ends with ] $. They can look different too."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#commands",
    "href": "newsite/theme2/SF100/slides.html#commands",
    "title": "Systems Fundamentals",
    "section": "Commands",
    "text": "Commands\n\nCommands are either programs or shell builtins.\nUse one of these commands to read documentation:\n\nman ${command}\ninfo ${command}\nor ${command} --help (sometimes!)\n\n\n\n\nPrograms can be compiled or scripts.\nThey live somewhere in your $PATH."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#the-shell",
    "href": "newsite/theme2/SF100/slides.html#the-shell",
    "title": "Systems Fundamentals",
    "section": "The shell",
    "text": "The shell\n\nA program that accepts commands, and passes those commands to the OS to execute.\nA popular shell is bash, which is the default on lnx201.\n\n\nOf course there are other shells too: zsh (default on newer versions of macOS, fish, ksh, etc."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#bash",
    "href": "newsite/theme2/SF100/slides.html#bash",
    "title": "Systems Fundamentals",
    "section": "Bash",
    "text": "Bash\n\n“Bourne-again shell”\n\nBased on an earlier Bourne shell, thus the “again”.\nDeveloped by the GNU project.\nOn lnx201, /bin/bash is the program.\n\nFor documentation: info bash or man bash.\n\n\n\nGNU people made a lot of the software early on.\n\nAlso Unix people, BSD people, X11 people, and a whole bunch of other people.\nThen the Linux kernel came along, became successful, and people started to call the whole combination of all these things “Linux”."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#bash-niceties-history-and-completion",
    "href": "newsite/theme2/SF100/slides.html#bash-niceties-history-and-completion",
    "title": "Systems Fundamentals",
    "section": "Bash niceties: history and completion",
    "text": "Bash niceties: history and completion\n\nYou do not have to re-type commands that you have used in the past!\n\nUse upup and downdown arrow keys to go back and forth in your command history.\nUse Ctrl-RCtrl-R (Control+R) to “search” command history.\nUse history command to list your shell history.\n\nUse tabtab key for command completion, after typing a few characters."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#some-helpful-commands",
    "href": "newsite/theme2/SF100/slides.html#some-helpful-commands",
    "title": "Systems Fundamentals",
    "section": "Some helpful commands",
    "text": "Some helpful commands\n\n\n\n\n\n\n\n\nCommand\nTask\nExample Syntax\n\n\n\n\nls\nlist the files in a directory\nls [/tmp]\n\n\ncd\nmove into a directory\ncd [/tmp]\n\n\npwd\nshow curent working directory\npwd\n\n\ncp\ncopy a file to into another directory, or make a copy with a different name\ncp [file.txt] [/tmp/file.txt]\n\n\ncp -r\ncopy a folder to into another directory\ncp [file.txt] [/tmp/file.txt]\n\n\nmv\nrename or move a file into another directory\nmv [file.txt] [file1.txt]\n\n\nrm\ndelete a file\nrm [file.txt]\n\n\nrm -r\nremove a directory, recursively\nrm -r [dir]\n\n\nmkdir\ncreate a directory\nmkdir [dir]\n\n\nfind\nfind a file\nfind [/tmp] -name [file]\n\n\ngrep\nsearch for a text pattern inside a file\ngrep [text] [/tmp/file.txt]\n\n\nless\nto view the text of a text file, one screen at a time\nless [/tmp/file.txt]\n\n\nexit\nexit and logout of a Terminal (Terminal-xfce4) session\nexit\n\n\n\nSource: https://wiki.classe.cornell.edu/Computing/SummerStudentOrientation#Linux_Command_Line_40ls_44_cd_44_mv_44_find_44_grep_etc_41"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#directory-navigation",
    "href": "newsite/theme2/SF100/slides.html#directory-navigation",
    "title": "Systems Fundamentals",
    "section": "Directory navigation",
    "text": "Directory navigation\n[ssasidharan@lnx201 ~]$ tree -d -L 1 /\n/\n├── bin -&gt; usr/bin\n├── boot\n├── cdat\n├── cifs\n├── cvmfs\n├── dev\n├── etc\n├── home\n├── lib -&gt; usr/lib\n├── lib64 -&gt; usr/lib64\n├── media\n├── misc\n├── mnt\n├── net\n├── nfs\n├── opt\n├── proc\n├── root\n├── run\n├── sbin -&gt; usr/sbin\n├── srv\n├── sys\n├── tmp\n├── usr\n└── var\n\n25 directories\n\n\nDirectories and files are organized in a tree like structure.\nWell, an inverted tree, maybe.\nAt the bottom (or top?), you have the “/” directory."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#your-home-directory",
    "href": "newsite/theme2/SF100/slides.html#your-home-directory",
    "title": "Systems Fundamentals",
    "section": "Your home directory",
    "text": "Your home directory\n\nYou have a “home” directory.\n\nYou can write your files and create directories here.\n\nUsually, and on lnx201, this will be /home/$USER\n\nAlso known as $HOME\n\n\n\n\nThe stuff that follows $ are environment variables aka env vars. They are sort of key-value pairs managed by the shell.\nThere are a set of standard env vars such as $USER, $HOME, $SHELL etc.\nDon’t write too much on your home directory.\nThere are better places top store large amounts of data.\nI am not the expert here. Ask around, or wait for the next presenter."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#those-whatever-things",
    "href": "newsite/theme2/SF100/slides.html#those-whatever-things",
    "title": "Systems Fundamentals",
    "section": "Those $WHATEVER things",
    "text": "Those $WHATEVER things\n\nWhat are $HOME, $USER, $PATH, $SHELL, etc.?\nThey are called environment variables, or env vars.\nEnv vars are pieces of information maintained by the shell.\nPrograms can use them during execution.\nUse printenv or env command to list them."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#the-current-working-directory",
    "href": "newsite/theme2/SF100/slides.html#the-current-working-directory",
    "title": "Systems Fundamentals",
    "section": "The current working directory",
    "text": "The current working directory\n\n\nAt any time in the shell, you are “inside” a single directory, called the current working directory.\n\nWhen you do ls, files in current working directory will be listed, etc.\n\nWhen you log in, your current working directory will be your home directory: /home/$USER aka $HOME.\nYou will use cd (change directory) to move around.\nUse the command pwd to find where you are.\n\nOr echo $PWD."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#absolute-and-relative-paths",
    "href": "newsite/theme2/SF100/slides.html#absolute-and-relative-paths",
    "title": "Systems Fundamentals",
    "section": "Absolute and relative paths",
    "text": "Absolute and relative paths\nFile/folder names are also referred to as paths.\n\nAbsolute path names begin with the root directory, /.\n\nExample: /home/ssasidharan/Documents/hello.txt\n\nRelative paths start with the working directory.\n\nExample: ./Documents/hello.txt (or just Documents/hello.txt) when I’m in my home directory."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#some-fun-facts-about-file-names",
    "href": "newsite/theme2/SF100/slides.html#some-fun-facts-about-file-names",
    "title": "Systems Fundamentals",
    "section": "Some fun facts about file names",
    "text": "Some fun facts about file names\n\nNames that begin with “.” are “hidden”.\n\nThey are omitted from directory listing when you do ls.\nDo ls -a (or ls --all) to list them.\n\n. and .. are special directory names.\n\n. stands for the current directory.\n.. stands for the directory above the current directory."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#some-more-fun-facts-about-file-names",
    "href": "newsite/theme2/SF100/slides.html#some-more-fun-facts-about-file-names",
    "title": "Systems Fundamentals",
    "section": "Some more fun facts about file names",
    "text": "Some more fun facts about file names\n\nFile and directory names are case sensitive.\n\nDepends on filesystem, but that is a detail.\n\nIt is better to avoid spaces in file names, because they are a hassle.\n\nUse _ (underscore character) instead (example: file_name), or CamelCase (example: FileName).\nQuote paths within \" and \" if they happen to have spaces, or “escape” each space with \\."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#wildcards",
    "href": "newsite/theme2/SF100/slides.html#wildcards",
    "title": "Systems Fundamentals",
    "section": "Wildcards",
    "text": "Wildcards\nSome characters are given special treatment:\n\n* matches any set of characters.\n\n[ssasidharan@lnx201 ~]$ ls /usr/bin/ab*\n/usr/bin/ab  /usr/bin/abs2rel\n\n? matches any one character.\n\n[ssasidharan@lnx201 ~]$ ls /usr/bin/?abc*\n/usr/bin/kabc2mutt  /usr/bin/kabcclient"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#standard-input-output-and-error",
    "href": "newsite/theme2/SF100/slides.html#standard-input-output-and-error",
    "title": "Systems Fundamentals",
    "section": "Standard input, output, and error",
    "text": "Standard input, output, and error\n\nInput is read from standard input (or stdin).\nOutput is written to standard output (or stdout).\nError messages are written to standard error (or stderr).\n\n\n\n\n\n\n\nNote\n\n\nThey are files too: /dev/stdin, /dev/stdout, and /dev/stderr."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#io-redirection",
    "href": "newsite/theme2/SF100/slides.html#io-redirection",
    "title": "Systems Fundamentals",
    "section": "I/O redirection",
    "text": "I/O redirection\nYou can redirect stdout to a file with &gt; operator:\n[ssasidharan@lnx201 ~]$ ls -l &gt; ls-output.txt\nOr append with &gt;&gt;:\n[ssasidharan@lnx201 ~]$ ls -l &gt;&gt; ls-output.txt\nTo direct a file to a programs input, use &lt; operator:\n[ssasidharan@lnx201 ~]$ cat &lt; sonnet18.txt\nShall I compare thee to a summer’s day?"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#pipes",
    "href": "newsite/theme2/SF100/slides.html#pipes",
    "title": "Systems Fundamentals",
    "section": "Pipes",
    "text": "Pipes\n\nUsing the | (“pipe”) operator, you can “chain” programs such that one programs output is another programs input:\n\n[ssasidharan@lnx201 ~]$ ls -l /bin/ | less\n\nYou can create longer pipes:\n\n[ssasidharan@lnx201 ~]$ ls /bin /sbin | sort | uniq | wc\n   4289    4288   46820"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#you-belong-to-groups",
    "href": "newsite/theme2/SF100/slides.html#you-belong-to-groups",
    "title": "Systems Fundamentals",
    "section": "You belong (to groups)",
    "text": "You belong (to groups)\nYour account belongs to several groups:\n[ssasidharan@lnx201 ~]$ id\nuid=63499(ssasidharan) gid=262(chess) groups=262(chess),750(classeuser)\n[ssasidharan@lnx201 ~]$ groups\nchess classeuser"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#permissions-and-ownership",
    "href": "newsite/theme2/SF100/slides.html#permissions-and-ownership",
    "title": "Systems Fundamentals",
    "section": "Permissions and ownership",
    "text": "Permissions and ownership\nDo a “long” file listing (with ls -l) and behold:\n$ ls -l\ntotal 8\ndrwxr-xr-x  2 ssasidharan chess   44 May  8 10:42 bin\ndrwxr-xr-x  2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x  2 ssasidharan chess   52 Apr  2 00:27 Desktop\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Documents\nlrwxrwxrwx  1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r--  1 ssasidharan chess   54 Jun  2 12:38 hello.sh\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Music\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Pictures\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Public\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Templates\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Videos\nWhat do those characters mean?"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#changing-permissions",
    "href": "newsite/theme2/SF100/slides.html#changing-permissions",
    "title": "Systems Fundamentals",
    "section": "Changing permissions",
    "text": "Changing permissions\n\nUse chmod command to change file mode bits (the first column in the previous listing).\n\n[ssasidharan@lnx201 ~]$ chmod +x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod -x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n\nUse chown and chgrp commands to change owner and group (the third and fourth columns in the previous listing).\n\nProbably not immediately useful; just know that they exist."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#listing-processes",
    "href": "newsite/theme2/SF100/slides.html#listing-processes",
    "title": "Systems Fundamentals",
    "section": "Listing processes",
    "text": "Listing processes\n\nList running processes using ps command:\n\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 694411 pts/81   00:00:00 ps\n3479688 pts/81   00:00:00 bash\nThe four columns:\n\nPID is process id.\nTTY is the terminal associated with the process.\nTIME is the elapsed CPU time for the process.\nCMD is the command that created the process.\n\n\n(Also see: top and htop.)"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#background-and-foreground-processes",
    "href": "newsite/theme2/SF100/slides.html#background-and-foreground-processes",
    "title": "Systems Fundamentals",
    "section": "Background and foreground processes",
    "text": "Background and foreground processes\n\nSome processes run in the foreground:\n\nThey read input, write output, etc.\nThey are “attached” to a terminal.\n\nBackground processes, well, run in the background. Send things to the background with &:\n\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 949751\n\nBring a background process to foreground using fg command, and terminate it using Ctrl-CCtrl-C:\n\n[ssasidharan@lnx201 ~]$ fg 2\nsleep 100\n^C"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#terminating-processes",
    "href": "newsite/theme2/SF100/slides.html#terminating-processes",
    "title": "Systems Fundamentals",
    "section": "Terminating processes",
    "text": "Terminating processes\n\nkill PID command to end one process.\nkillall command to end many processes.\n\nYou can’t kill other user’s processes."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#text-editors",
    "href": "newsite/theme2/SF100/slides.html#text-editors",
    "title": "Systems Fundamentals",
    "section": "Text editors",
    "text": "Text editors\nMany choices! Use:\n\nEmacs\nVim\nNano\nJupyterLab"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#terminal-multiplexers",
    "href": "newsite/theme2/SF100/slides.html#terminal-multiplexers",
    "title": "Systems Fundamentals",
    "section": "Terminal multiplexers",
    "text": "Terminal multiplexers\nscreen and tmux are two options. Here’s tmux.\n\n\n\nYou can “multiplex” your terminal.\nDifferent shells in different “windows”.\nYou can split panes vertically or horizontally.\nYou can detach and re-attach, and resume your work."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#hello-world",
    "href": "newsite/theme2/SF100/slides.html#hello-world",
    "title": "Systems Fundamentals",
    "section": "Hello world!",
    "text": "Hello world!\n\n\nhello.sh\n\n1#! /bin/bash\n\n2# A simple script.\n\n3echo \"Hello $USER!\"\n\n\n1\n\nThe “shebang”\n\n2\n\nA comment.\n\n3\n\nAn actual line of code.\n\n\n\nMake the thing executable with chmod +x hello.sh\nRun the thing with ./hello.sh"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#other-things",
    "href": "newsite/theme2/SF100/slides.html#other-things",
    "title": "Systems Fundamentals",
    "section": "Other things",
    "text": "Other things\nBash supports a programming language with:\n\nif statements\nfor, while, until statements\nfunctions\net cetera.\nTake a peek at /etc/bashrc and $HOME/.bashrc for a taste\n\nBUT\n\nThis is not really in scope of this presentation."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#a-cheat-sheet",
    "href": "newsite/theme2/SF100/slides.html#a-cheat-sheet",
    "title": "Systems Fundamentals",
    "section": "A cheat sheet",
    "text": "A cheat sheet\n\n(Via Stephen Turner.)"
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#resources-elsewhere",
    "href": "newsite/theme2/SF100/slides.html#resources-elsewhere",
    "title": "Systems Fundamentals",
    "section": "Resources elsewhere",
    "text": "Resources elsewhere\n\nThe Linux Command Line, A Complete Introduction by William E. Shotts, Jr. The book is freely available under a Creative Commons license, and contains a good discussion about shell scripting.\nThe Unix Programming Environment by Brian W. Kernighan and Rob Pike. Old classic, still useful. Places things in a historical context.\nShell Tools and Scripting module of MIT “The Missing Semester of Your CS Education” class."
  },
  {
    "objectID": "newsite/theme2/SF100/slides.html#other-resources",
    "href": "newsite/theme2/SF100/slides.html#other-resources",
    "title": "Systems Fundamentals",
    "section": "Other resources",
    "text": "Other resources\n\nThe Internet is pretty great. Use it.\nHowever: DO NOT trust everything you read on the Internet.\n\nDo not copy and paste commands from the Internet indiscriminately.\nUnderstand how things work, and then use it."
  },
  {
    "objectID": "newsite/theme2/SF201/batch-systems-and-compute-farms.html",
    "href": "newsite/theme2/SF201/batch-systems-and-compute-farms.html",
    "title": "Batch Systems and Compute Farms",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme5/CF200/curating-data.html",
    "href": "newsite/theme5/CF200/curating-data.html",
    "title": "Curating Data, Code, Workflows, and Publishing",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nYou have run your experiments, collected data, wrote code to analyze the data, and documented your work. One next possible step that you should take is sharing the fruits of your labor with the world. The fruits of your labor includes, among other things, the data you have collected.\nHowever, you can’t simply file your results away just about anywhere, promise to make them available on demand, and declare victory. There are some principles and processes to follow.\nOne such set of principles is FAIR data principles."
  },
  {
    "objectID": "newsite/theme5/CF200/curating-data.html#fair-principles-and-practices",
    "href": "newsite/theme5/CF200/curating-data.html#fair-principles-and-practices",
    "title": "Curating Data, Code, Workflows, and Publishing",
    "section": "FAIR principles and practices",
    "text": "FAIR principles and practices\nFAIR is still evolving, and there is no one canonical implementation of FAIR. Organizations implement their individual approaches to FAIR principles based on their needs and constraints.\nAlthough FAIR itself does not make concrete recommendations about implementation, some common practices have been evolving.\n\nMaking data findable\n\nData should have globally unique and persistent identifiers. You will need to use DOIs (Digital Object Identifiers), ARKs (Archival Resource Keys), or other permanent identifier systems for your datasets. People also get stable identifiers – ORCID is one such system.\n\n\nData should be described with rich metadata. Document your data comprehensively with standardized metadata schemas relevant to your field (e.g., DataCite, Dublin Core).\n\n\nMetadata should clearly include the identifier of the data it describes.\nData should be registered or indexed in a searchable resource. You will need to deposit data in appropriate domain or institutional repositories that are indexed by search engines.\n\n\n\nMaking data accessible\n\nData should be retrievable by their identifier using a standardized protocol. Select repositories that provide web access protocols (HTTP/HTTPS) and web APIs (REST). The protocol should be open, free, and universally implementable.\nThe protocol should allow for authentication and authorization where necessary. When needed, use standard authentication protocols rather than proprietary solutions.\nEven if data has restrictions, ensure the conditions for access are clearly specified.\nMetadata should remain accessible even when the data is no longer available.\n\n\n\nMaking data interoperable\n\nData should use formal, accessible, shared, and broadly applicable language for knowledge representation. Use standard formats: store data in non-proprietary, widely-used formats. For example: for structured data, you should use CSV, JSON or XML, rather than Excel sheets.\nData should use vocabularies that follow FAIR principles. Use established ontologies and terminologies from your field.\nData should include qualified references to other data. When referencing other datasets, use their persistent identifiers.\nFollow semantic web standards. Consider RDF data models and linked data approaches for complex datasets.\n\n\n\nMaking data reusable\n\nData should have accurate and relevant attributes.\nData should be released with a clear and accessible license. Apply explicit, machine-readable licenses such as Creative Commons or Open Data Commons.\nData should include detailed information about data origin, collection methods, and processing steps.\nData should adhere to domain-specific data standards and reporting guidelines. Document data quality assessment methods and results."
  },
  {
    "objectID": "newsite/theme5/CF101/dmp-best-practices.html",
    "href": "newsite/theme5/CF101/dmp-best-practices.html",
    "title": "Best practices for developing DMP",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme4/XS200/metadata.html",
    "href": "newsite/theme4/XS200/metadata.html",
    "title": "Metadata for Data Fidelity and Systematic Checks",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme4/XS102/large-scale-data-analysis.html",
    "href": "newsite/theme4/XS102/large-scale-data-analysis.html",
    "title": "Large-scale Data Analysis: from Images to Science Parameters to Interpretation",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html",
    "href": "newsite/theme4/XS101/linux-exercises.html",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Wherever you see &lt;your CLASSE username&gt; below, substitute your own CLASSE username.\n\n\nOpen a terminal on lnx201 using one of the following options: 1. ssh from your computer’s terminal (for Mac and Linux users) - Type ssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu 3. PuTTY (for Windows users) - See https://wiki.classe.cornell.edu/Computing/WinTunnelVncSSH 5. Use CLASSE’s NoMachine service - Browse to https://nomachine.classe.cornell.edu - Respond to Duo prompt - Click on lnx201 - Create a new desktop or custom session → Create a new virtual desktop - Click on Terminal icon in bottom menu bar - To log out: Applications → Log Out 1. Use CLASSE’s JupyterHub service - Browse to https://jupyterhub.classe.cornell.edu - Respond to Duo prompt - Server Options: Select a job profile: CLASSE Compute Farm, click Start - Launcher tab: Terminal - OR File → New → Terminal - To log out: File → Log Out\n\n\n\nAfter logging in, you will be in your home directory. Start navigating in the same terminal window as before. Run the following commands: 1. pwd - pwd = print working directory. This commend tells you what the current working directory is 2. ls - This command lists the contents of your current working directory 3. ls -la - This command lists the contents of the current working directory with the additional options - -l tells ls to show details about each file’s type, permissions, size, etc. (lowercase “l” = “long”) - -a tells ls to list hidden files, too (“a” = “all”) - Individual options to ls like -l and -a can be shortened to -la - How to read the output of ls -l: https://docs.nersc.gov/filesystems/unix-file-permissions/\n\n\n\nDo not make a habit of working in your home directory /home/&lt;your CLASSE username&gt;!\nInstead, use your CHESS user directory: /nfs/chess/user/&lt;your CLASSE username&gt;.\n\nType cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\ncd = change directory\nIf your CHESS user directory does not exist, first type mkdir /nfs/chess/user/&lt;your CLASSE username&gt;\n\nOR, use the premade symbolic link in your home directory:\n\nType cd /home/&lt;your CLASSE username&gt;/CLASSE_shortcuts/chess_&lt;your CLASSE username&gt;\n\nRepeat Exercise 2 and observe how your CHESS user directory differs from your home directory\nReturn to your home directory\n\nType cd /home/&lt;your CLASSE username&gt;\nOR type cd ~\n\nThe tilde symbol ~ is shorthand for your home directory\n\n\n\n\n\n\nTab completion is an extremely useful feature of the Linux command line that helps you type commands much faster. It works with commands and file paths. Simply start typing one or two characters of a command or file path, then hit the tab key. The command line will automatically fill in additional characters if it can. If it cannot fill in your command or file path all the way, hit the tab key twice to see the available options. (Unfortunately tab completion is not available in the JupyterHub terminal.)\nFor example: 1. Type cd ~/CL, then hit the tab key - The command line should automatically complete what you typed to cd ~/CLASSE_shortcuts/ (unless you have another file or directory that begins with “CL”) - Hit return. Now you are in the CLASSE_shortcuts directory within your home directory - Type ls to see the directory contents 2. Type cd ch, then hit the tab key - This time, the command line can only partially complete what you’ve typed. Hit the tab key twice to show the available options, one of which should be chess_&lt;your CLASSE username&gt;. - Now, type the first letter or two of your CLASSE username and hit tab again. - If the command line was able to complete your username, then hit return to go to your CHESS user directory - If not, keep typing letters followed by tab until your username is complete, then hit return\n\n\n\nIn Linux (UNIX), a pipe (|) is used to connect the output of one command to the input of another. This allows for powerful chaining of small utilities to perform complex tasks, following the UNIX philosophy:\n&gt; “Do one thing well.”\nFor example:\ncommand1 | command2\nmeans “take the output of command1 and pass it as input to command2”.\nFor the following set of exercises please review Linux commands like echo, cat, env, grep, more and less.\n\n\n# print string; the flag -e will properly handle backslash escapes `\\n` \necho -e \"apple\\nbanana\\ncherry\"\n\n# now combine commands via pipe\necho -e \"apple\\nbanana\\ncherry\" | grep 'an'\nWhat will be printed?\nExpected:\nbanana\n\n\n\nenv | grep SHELL\nList all environment variables, then print only the one that specifies your current shell.\n\n\n\nCombine Linux tools introduced above and redirect the output to produce a new file.\n# find who you are\nenv | grep USER\n\n# make new area on /tmp (default temporary area on ANY Linux node)\nmkdir /tmp/$USER\n\n# create new file in /tmp/$USER area\necho \"my data\" &gt; /tmp/$USER/file.dat\n\nWhat would be content of /tmp/$USER/file.dat?\nCan you print content of the /tmp/$USER/file.dat?\n\nHint: cat tool can be used both for viewing and creating files\n# create new file\ncat &gt; /tmp/$USER/file.txt &lt;&lt; EOF\none\ntwo\nthree\nEOF\n\n# view the file\ncat /tmp/$USER/file.txt\n\n# chain together multiple commands with pipe\ncat /tmp/$USER/file.txt | grep t | wc -l\n\n\n\nLinux offer two pagination tools less and more, which can be combine with the pipe concept:\n# use paginators, less and more\nls /etc | less\nUse either tool to view and navigate large files or lengthy output from a command."
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html#exercise-1-logging-in",
    "href": "newsite/theme4/XS101/linux-exercises.html#exercise-1-logging-in",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Open a terminal on lnx201 using one of the following options: 1. ssh from your computer’s terminal (for Mac and Linux users) - Type ssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu 3. PuTTY (for Windows users) - See https://wiki.classe.cornell.edu/Computing/WinTunnelVncSSH 5. Use CLASSE’s NoMachine service - Browse to https://nomachine.classe.cornell.edu - Respond to Duo prompt - Click on lnx201 - Create a new desktop or custom session → Create a new virtual desktop - Click on Terminal icon in bottom menu bar - To log out: Applications → Log Out 1. Use CLASSE’s JupyterHub service - Browse to https://jupyterhub.classe.cornell.edu - Respond to Duo prompt - Server Options: Select a job profile: CLASSE Compute Farm, click Start - Launcher tab: Terminal - OR File → New → Terminal - To log out: File → Log Out"
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html#exercise-2-basic-commands",
    "href": "newsite/theme4/XS101/linux-exercises.html#exercise-2-basic-commands",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "After logging in, you will be in your home directory. Start navigating in the same terminal window as before. Run the following commands: 1. pwd - pwd = print working directory. This commend tells you what the current working directory is 2. ls - This command lists the contents of your current working directory 3. ls -la - This command lists the contents of the current working directory with the additional options - -l tells ls to show details about each file’s type, permissions, size, etc. (lowercase “l” = “long”) - -a tells ls to list hidden files, too (“a” = “all”) - Individual options to ls like -l and -a can be shortened to -la - How to read the output of ls -l: https://docs.nersc.gov/filesystems/unix-file-permissions/"
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html#exercise-3-chess-user-directory",
    "href": "newsite/theme4/XS101/linux-exercises.html#exercise-3-chess-user-directory",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Do not make a habit of working in your home directory /home/&lt;your CLASSE username&gt;!\nInstead, use your CHESS user directory: /nfs/chess/user/&lt;your CLASSE username&gt;.\n\nType cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\ncd = change directory\nIf your CHESS user directory does not exist, first type mkdir /nfs/chess/user/&lt;your CLASSE username&gt;\n\nOR, use the premade symbolic link in your home directory:\n\nType cd /home/&lt;your CLASSE username&gt;/CLASSE_shortcuts/chess_&lt;your CLASSE username&gt;\n\nRepeat Exercise 2 and observe how your CHESS user directory differs from your home directory\nReturn to your home directory\n\nType cd /home/&lt;your CLASSE username&gt;\nOR type cd ~\n\nThe tilde symbol ~ is shorthand for your home directory"
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html#exercise-4-tab-completion",
    "href": "newsite/theme4/XS101/linux-exercises.html#exercise-4-tab-completion",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Tab completion is an extremely useful feature of the Linux command line that helps you type commands much faster. It works with commands and file paths. Simply start typing one or two characters of a command or file path, then hit the tab key. The command line will automatically fill in additional characters if it can. If it cannot fill in your command or file path all the way, hit the tab key twice to see the available options. (Unfortunately tab completion is not available in the JupyterHub terminal.)\nFor example: 1. Type cd ~/CL, then hit the tab key - The command line should automatically complete what you typed to cd ~/CLASSE_shortcuts/ (unless you have another file or directory that begins with “CL”) - Hit return. Now you are in the CLASSE_shortcuts directory within your home directory - Type ls to see the directory contents 2. Type cd ch, then hit the tab key - This time, the command line can only partially complete what you’ve typed. Hit the tab key twice to show the available options, one of which should be chess_&lt;your CLASSE username&gt;. - Now, type the first letter or two of your CLASSE username and hit tab again. - If the command line was able to complete your username, then hit return to go to your CHESS user directory - If not, keep typing letters followed by tab until your username is complete, then hit return"
  },
  {
    "objectID": "newsite/theme4/XS101/linux-exercises.html#exercise-5-use-pipe-to-combine-commands",
    "href": "newsite/theme4/XS101/linux-exercises.html#exercise-5-use-pipe-to-combine-commands",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "In Linux (UNIX), a pipe (|) is used to connect the output of one command to the input of another. This allows for powerful chaining of small utilities to perform complex tasks, following the UNIX philosophy:\n&gt; “Do one thing well.”\nFor example:\ncommand1 | command2\nmeans “take the output of command1 and pass it as input to command2”.\nFor the following set of exercises please review Linux commands like echo, cat, env, grep, more and less.\n\n\n# print string; the flag -e will properly handle backslash escapes `\\n` \necho -e \"apple\\nbanana\\ncherry\"\n\n# now combine commands via pipe\necho -e \"apple\\nbanana\\ncherry\" | grep 'an'\nWhat will be printed?\nExpected:\nbanana\n\n\n\nenv | grep SHELL\nList all environment variables, then print only the one that specifies your current shell.\n\n\n\nCombine Linux tools introduced above and redirect the output to produce a new file.\n# find who you are\nenv | grep USER\n\n# make new area on /tmp (default temporary area on ANY Linux node)\nmkdir /tmp/$USER\n\n# create new file in /tmp/$USER area\necho \"my data\" &gt; /tmp/$USER/file.dat\n\nWhat would be content of /tmp/$USER/file.dat?\nCan you print content of the /tmp/$USER/file.dat?\n\nHint: cat tool can be used both for viewing and creating files\n# create new file\ncat &gt; /tmp/$USER/file.txt &lt;&lt; EOF\none\ntwo\nthree\nEOF\n\n# view the file\ncat /tmp/$USER/file.txt\n\n# chain together multiple commands with pipe\ncat /tmp/$USER/file.txt | grep t | wc -l\n\n\n\nLinux offer two pagination tools less and more, which can be combine with the pipe concept:\n# use paginators, less and more\nls /etc | less\nUse either tool to view and navigate large files or lengthy output from a command."
  },
  {
    "objectID": "theme3/DC200/computing-with-ci-ecosystem.html",
    "href": "theme3/DC200/computing-with-ci-ecosystem.html",
    "title": "Computing with CI Ecosystem",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nThis module provides an overview of the various Cyberinfrastructure (CI) resources available to a CHESS researcher and more broadly to all the researchers based in the US.\nBefore we go into the various offerings available, it is useful to recap what we mean by CI. A widely used and accepted definition of CI by Craig A. Stewart et al is as follows:\nCyberinfrastructure consists of computing systems, data storage systems, advanced instruments and data repositories, visualization environments, and people, all linked together by software and high performance networks to improve research productivity and enable breakthroughs not otherwise possible.\nAs a CHESS researcher, you have access to the following CI resources that can help you with computing"
  },
  {
    "objectID": "theme3/DC200/computing-with-ci-ecosystem.html#overview-of-chess-operational-workflow",
    "href": "theme3/DC200/computing-with-ci-ecosystem.html#overview-of-chess-operational-workflow",
    "title": "Computing with CI Ecosystem",
    "section": "Overview of CHESS Operational Workflow",
    "text": "Overview of CHESS Operational Workflow\nBelow is a high level overview of an overall chess operational workflow from the time a researcher puts in a proposal to use a beamline at CHESS to data collection from the beamline; and then it’s subsequent data processing and analysis before final data curation on the outputs is done.\n\n\n\nOverview of a general CHESS Operational Workflow\n\n\nCHESS’ workflow complexity is the length of time spent performing a typical data analysis. According to a CHESS survey conducted a few years back, CHESS’ workflow complexity ranges from 1 month to 24 months among survey respondents (with the average being 7.2 months). For the most complex workflows (12 months or longer), researchers often run into challenges regarding completing their data processing within the defined time constraints.\nUse of National CI resources, can especially help CHESS researchers in the following areas\n\nProcessing data (reduction, analysis, simulation, interpretation), handling large data sets, leveraging existing software and CI.\nMetadata management, Open Science/FAIR and data curation."
  },
  {
    "objectID": "theme3/DC200/computing-with-ci-ecosystem.html#chess-ci-resources-at-cornell",
    "href": "theme3/DC200/computing-with-ci-ecosystem.html#chess-ci-resources-at-cornell",
    "title": "Computing with CI Ecosystem",
    "section": "CHESS CI Resources at Cornell",
    "text": "CHESS CI Resources at Cornell\nThe CLASSE cyberinfrastructure (CI) consists of an interconnected series of h high-availability server clusters (HACs), data acquisition systems, control systems, compute farms, and workstations. Most of these systems run either Scientific Linux or Windows on commodity 64-bit Intel-based hardware and are centrally managed using Puppet. The median age of key CI components is approximately 5 years, with an average refresh rate of once every 10 years. The picture below provides an overview of the CHESS CI\n\n\n\nCHESS CI immediately available for processing/analysis by users on the Compute Farm\n\n\n\nGetting access\nIn order to access the Cornell CI resources, you need a Cornell Laboratory for Accelerator-based ScienceS and Education(CLASSE) account.\nCHESS users can request a CLASSE account through the CHESS User Dashboard (BeamPASS).\nMore info in the user guide, under “Activate your CLASSE Account”.\n\n\nDAQ Cluster\nDAQ Cluster is the Data Acquisition System that runs on a dedicated server cluster. Makes available to researchers about 2PB of storage for raw data collection, analysis, and simulation. Data collected at the stations written directly to the DAQ over either NFS or Samba and is immediately available for processing/analysis by users on the Compute Farm.\nCHESS users can also download their data remotely using the CLASSE Globus Connect Server or via SFTP.\n\n\nCompute Farm\nThe HPC Cluster at CHESS/CLASSE is a central resource consisting of\n\ncentral resource of 60+ enterprise-class Linux nodes (with around 800 CPUs)\n4.5TB of memory\nuses SGE as a front-end queueing system\nsupports interactive, batch, parallel, and GPU jobs\nensures equal access to the Compute Farm for all users.\n\nThis cluster is suited for doing both HPC and HTC type jobs.\n\nLogin Node and other information\nThe CHESS cluster login node is to use ssh to login to lnx201.classe.cornell.edu which is the headnode of the cluster.\nssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu\nThere is also a shared filesystem available across all the nodes in the cluster. This can be accessed at /nfs/chess/user/ .\nA complete list of various filesystems accessible can be found at CLASSE Wiki.\n\n\n\nJob Submission\nThe CHESS cluster uses SGE as the front end scheduling system to submit jobs.\nIn general, there are two basic steps to submitting a job\n\nCreate a shell script containing the commands to be run\nSubmit this script to the Compute Farm using the qsub command.\n\nBelow is a simple shell script myscript.sh that you can submit to the SLURM cluster if you are logged onto lnx201.\n[user@lnx201 ~]$ cat myscript.sh\n#!/bin/bash\necho Running on host: `hostname`.\necho Starting on: `date`.\nsleep 10\necho Ending on: `date`.\n#$ -q all.q\n#$ -S /bin/bash\n#$ -l mem_free=8G\nIn order to submit it to the cluster you can use qsub command\n[user@lnx201 ~]$ qsub -q all.q myscript.sh\nDetailed instructions about this can be found at CLASSE Wiki.\nIf you want to submit job to another queue, you can change the -q option. In order to see all the queues on the cluster, you can run the following command.\n[user@lnx201 ~]$ qconf -sql\nall.q\nbenchmark.q\nchess.q\nchess_fast.q\n...\nchess_xleap_interactive.q\ninteractive.q\nThe above output is shortened for display purposes."
  },
  {
    "objectID": "theme3/DC200/computing-with-ci-ecosystem.html#access",
    "href": "theme3/DC200/computing-with-ci-ecosystem.html#access",
    "title": "Computing with CI Ecosystem",
    "section": "ACCESS",
    "text": "ACCESS\nACCESS CI is a program established and funded by the National Science Foundation (NSF) to help researchers and educators utilize the nation’s advanced computing systems and services. ACCESS is a collection of both large and experimental HPC resources.\nACCESS provides a wide range of resources and services:\n\nSystems ranging from supercomputers to smaller specialized compute clusters, each with a different focus and unique set of capabilities\nData and storage services\nExpertise to help you make effective use of resources, remove barriers, and achieve your goals\nScientific applications\nScience gateways\n\nSome of the resources available are listed below\n\n\n\nLeadership Class\n\n\n\n\n\nFrontera\nUniversity of Texas, Austin\n\n\n\n\n\n\nInnovative Production Systems\n\n\n\n\n\nAnvil\nPurdue University\n\n\nBridges 2\nCarnegie-Mellon University\n\n\nDelta/ DeltaAI\nU of Illinois Urbana-Champaign\n\n\nExpanse\nU of California, San Diego\n\n\nJetstream 2\nUniversity of Indiana + Partners\n\n\nStampede 2\nU of Texas, Austin\n\n\n\n\n\n\nPrototypes/Testbeds\n\n\n\n\n\nNeocortex\nCarnegie-Mellon University\n\n\nVoyager\nU of California, San Diego\n\n\nOokami\nStonybrook University\n\n\nNRP\nU of California, San Diego\n\n\nACES\nTexas A&M University\n\n\n\n\n\n\nCloud Technology Resources\n\n\n\n\n\nCloudbank\nU of California, San Diego\n\n\nCloudLab\nUniversity of Utah\n\n\nChameleon\nUniversity of Chicago\n\n\n\n\n\n\nLocation of various ACCESS Resources\n\n\nOf the resources above, the leadership class system and most of the Innovative Production Systems are HPC clusters, very similar to the CHESS HPC cluster at Cornell. The main difference being that they use SLURM as their frontend queueing system instead of SGE.\n\nACCESS User Registration\nIn order to use ACCESS resources, you first need to request an ACCESS user account. This account is required for you to login to the ACCESS website, manage your allocations (more on it in the next section).\nGetting an ACCESS user account is fairly straightforward. Instructions for getting it can be found here. Using an existing University account when registering with ACCESS simplifies the sign-up process and enables you to log in to ACCESS using that existing account.\n\n\nGetting an Allocation\nTo get started, you need an ACCESS project and some resource units you can spend. Your ACCESS project and resource units are what we refer to as an Allocation. An allocation is your project to use a portion of a shared resource.\nThrough ACCESS, you can get an allocation to use computing and data resources to accomplish your research or classroom objectives.\nYou can get allocation for 4 different types of projects which are listed below.\n\nEXPLORE — Great for resource evaluation, graduate student projects,\nsmall classes and training events, benchmarking, code development and porting, and similar small-scale uses.\nDISCOVER — Designed for research grants with modest resource needs, Campus Champions, large classes and training events, graduate student projects, benchmarking and code testing at scale, and gateway development.\nACCELERATE — Best for experienced users with mid-scale resource needs, consolidating multi-grant programs, collaborative projects, preparing for Maximize ACCESS requests, and gateways with growing communities.\nMAXIMIZE — The choice for large-scale research activities that need more resources than the limit for Accelerate ACCESS projects.\n\nThe EXPLORE requests are easiest to get with MAXIMIZE the hardest (in terms of supporting documentation required). The EXPLORE request will get you 400,000 ACCESS credits/SUs to start with, which is enough to explore suitability of these resources for your processing needs.\nMore details including a detailed comparison table can be found here.\n\n\nLogging to the ACCESS resource\nOnce, you have your allocation approved then in the Allocations portal you can assign your credits to a particular resource that you are interested in. Once that is done, the Resource Provider will contact you to setup your local HPC accounts.\nOnce that is done, you can * register your ssh keys to login to your account on the HPC resource.\nAdditionally, most of the ACCESS resources have Open OnDemand installed, that allow you to access and login to the clusters using a web front-end. Open OnDemand is an easy-to-use web portal that is being deployed on ACCESS resources to allow researchers to compute from anywhere without client software or command-line interface, and significantly speed up the time to science.\n\n\nSubmitting Jobs\nYou can login to the headnodes of these resources, and submit jobs to the SLURM clusters in a similar fashion that you submit jobs on the CHESS cluster.\nAdditionally, all the ACCESS resources that have an Open OnDemand install, allow you to launch Jupyter notebooks to run your analysis.\nYou can also use workflow systems such as Pegasus to run your analysis pipelines on ACCESS resources. One easy way to explore Pegasus workflows is to use ACCESS Pegasus.\n\n\nACCESS Support\nACCESS does provide support for researchers looking to leverage their resources. The support comes in various flavors as illustrated in the picture below.\nACCESS Support provides researchers with access to\n\nTools, growing knowledge base\nMatch-making with experts\nStudent engagement\nEngagement from community\n\nACCESS provides Match services (Tier 3 and Tier 4) which connects researchers with experts to help you select the right system, run on a supercomputer, and solve basic code and research problems.\n\n\n\nACCESS Support Tiers"
  },
  {
    "objectID": "theme3/DC200/computing-with-ci-ecosystem.html#osg-and-path-facilities",
    "href": "theme3/DC200/computing-with-ci-ecosystem.html#osg-and-path-facilities",
    "title": "Computing with CI Ecosystem",
    "section": "OSG and PATh Facilities",
    "text": "OSG and PATh Facilities\nPATh and OSG make up the OSG Consortium that builds and operates a set of pools of shared computing and data capacity for distributed high-throughput computing (dHTC).\nThe OSG Consortium builds and operates a set of HTCondor pools of shared computing and data capacity for distributed high-throughput computing (dHTC). Each pool is organized and operated to serve a particular research community (e.g. a campus, multi-institutional collaboration, etc.), using technologies and services provided by the core OSG Team. One of these pools, known as the Open Science Pool is operated for all of US-associated open science.\n\n\n\nOSPools Geographic Distribution\n\n\nImage Credit: OSG Research Facilitation Team\nOne of the most attractive features of using the OSPool is that it has a No Proposal, No Allocation, No Cost principle. It povides its users with fair-share access to compute and storage capacity contributed by university campuses, and government-supported supercomputing institutions.\n\nWhat type of jobs are good fit for OSPool\nThe OSPool is made up of mostly opportunistic capacity - contributing clusters may interrupt jobs at any time. Thus, the OSPool supports workloads of numerous jobs that individually complete or checkpoint within 20 hours.\nThe OSG consortium provides guidance on what type of jobs are a good fit. It is replicated from the OSPool website below.\n\n\n\n\n\n\n\n\n\n\nIdeal Jobs!\nStill very advantageous\nMaybe not, but get in touch!\n\n\n\n\nExpected Throughput, per user\n1000s concurrent cores\n100s concurrent cores\nLet’s discuss!\n\n\nCPU\n1 per job\n&lt; 8 per job\n&gt; 8 per job\n\n\nWalltime\n&lt; 10 hrs*\n&lt; 20 hrs*\n&gt; 20 hrs\n\n\nRAM\n&lt; few GB\n&lt; 40 GB\n&gt; 40 GB\n\n\nInput\n&lt; 500 MB\n&lt; 10 GB\n&gt; 10 GB**\n\n\nOutput\n&lt; 1 GB\n&lt; 10 GB\n&gt; 10 GB**\n\n\nSoftware\npre-compiled binaries, containers\nMost other than →\nLicensed Software, non-Linux\n\n\n\n\n\nOSG User Registration\nTo get started with the OSPool, please complete the account application here. After submitting the form, you’ll receive an invitation to a new user meeting with an OSG facilitator. During the meeting, you’ll get an overview of OSG and discuss how to run your computations on the platform. Your account and project will be set up following this meeting.\n\n\nSubmitting jobs\nSubmitting jobs involves logging into one of the OSG Access points, from where you can submit a job using HTCondor or a workflow tool like Pegasus to the OSPool. Submitting the job is similar to how you submit a job on your local HPC cluster. The main difference is that you are interacting with HTCondor not SLURM or SGE.\nAnother important difference to keep in mind is that unlike HPC clusters, you cannot expect a shared filesystem between the login node (from where you submit the job) and the nodes on which your jobs execute. The nodes on which your jobs run are geographically distributed as illustrated in the figure below.\n\n\n\nOSPools Job Submission\n\n\nImage Credit: OSG Research Facilitation Team\n\n\nPATh\nThe PATh Facility is part of a pilot project funded by the NSF Office of Advanced Cyberinfrastructure. NSF-funded researchers can apply to request credits for the PATh facility. In that aspect, PATh is also similar to ACCESS resources in that you need to apply for credits to run on these resources.\nComposed of current gen hardware, the PATh facility provides users access to\n\n30,000 cores and\n36 A100 GPUs\n\nPATh user registration can be found on the PATh website.\n\n\nOS Pool vs PATh\nA researcher’s experience on PATh Facility and OSG’s Open Science Pool (OSPool) compute systems is similar: both offer thousands of CPU resources, as well as GPUs, disk space for saving actively-used data, and support technologies such as containerized software and checkpointing. They also both use a HTCondor Software Suite as a job scheduling software, which specializes in managing large high-throughput workflows.\nThe main differences between the two are\n\nOSPool is open to any US-affiliated academic, non-profit, or government research projects while PATh is available to only researchers with accepted or active NSF award with selected affiliated programs.\nPATh uses compute credit system to request resources.\nOn PATh researchers can request larger amounts of CPU/GPUs, more memory, disk space, and a longer runtime and are guaranteed these resources until their job completes.\n\nFull details on these differences can be found here."
  },
  {
    "objectID": "theme3/DC102/using-science-gateways.html",
    "href": "theme3/DC102/using-science-gateways.html",
    "title": "Using Science Gateways",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme1/PE102/index.html",
    "href": "theme1/PE102/index.html",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "In the previous section, modules for Python was introduced. In this section, we’ll take a much more detailed look at one of the most useful to scientists: NumPy. This module contains numerous routinues and support frameworks for numerical computing. The routinues in it are very carefully tested for correctness and are crafted for speed. Any time you can use something from this package, it’s a good idea to.\nPython is built for versatility and ease of programming. Unfortunately, it is not built for speed. Over the years Python has gotten faster and faster but there is still a speed penalty compared to classic compiled languages like C, C++, or Fortran.\nEnter NumPy: a package of mathematical routines written in C and Fortran and made to work with Python via a “glue” or “shim” layer. This interface is invisible to the programmer. NumPy looks and behaves just like any other Python package. Under the surface, though, lies a very fast and efficient library of algorithms.\n\n\nLet’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)\n\n\n\n\n\nThe trivial example: add a scalar (“a single number”) to every element of the matrix:\n\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]\n\n\n\n\n\nLet’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])\n\n\n\nSource: NumPy - A Mathematical Toolkit for Python"
  },
  {
    "objectID": "theme1/PE102/index.html#a-first-glimpse",
    "href": "theme1/PE102/index.html#a-first-glimpse",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "Let’s take a quick look at NumPy and see a few of the things it can do. NumPy is a package, not part of Python proper, so we have to tell Python to load it. It’s traditional to import numpy and give it the alias “np” - it’s less typing that way, and if you’re cutting and pasting code from other sources then it’s handy to follow the convention.\nPython, you’ll recall, doesn’t have an “array” data type. The closest it can come is the “list”. Lists are certainly useful, but they aren’t all that fast to read and even slower to write to. To make matters worse, a 2-D array is represented by a list of lists. This is great for representing complicated data but it’s lousy for doing math.\nThe critical NumPy data type is the array: “NumPy arrays are faster and more compact than Python lists. An array consumes less memory and is convenient to use.” (source) The one caveat with NumPy arrays is that all the elements inside an array need to have the same data type (e.g. integer, float, double). In practice this is rarely, if ever, a problem.\nLet’s make an array of integers:\n\n\narray([[ 1,  2,  3,  4],\n       [ 5,  6,  7,  8],\n       [ 9, 10, 11, 12]])\n\n\nThe array a is now a 4x3 array of integers. The array method was called with one argument - a Python “list of lists” representation of the array. The dimensions of the array are inferred from the list of lists used to initialize it.\nThere are other ways to create arrays. Here are two more common methods:\n\n\narray([0., 0., 0.])\n\n\nNotice the decimal points after the zeros. These indicate that we’re seeing floating point numbers.\n\n\narray([[1., 1., 1.],\n       [1., 1., 1.],\n       [1., 1., 1.]])\n\n\nThis one will throw you off if you aren’t paying attention. Notice how many parantheses there are… probably more than you expected! What is going on is that the outer parentheses are there to indicate function arguments, just like calling any other functions. The inner parentheses are used to generate a tuple, in this case one with two values, both of which are threes. This tuple can be arbitrarily long:\n\n\narray([[[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]],\n\n\n       [[[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]],\n\n        [[1., 1., 1.],\n         [1., 1., 1.],\n         [1., 1., 1.]]]])\n\n\nThe output isn’t terribly easy to read, but then again representing a four dimensional array on a flat page is challenging at best.\nIf we ever need to see the dimensions of an array, we can use the shape() method.\n\n\nz:\n(3,)\n\nm:\n(3, 3)\n\nbig_m\n(3, 3, 3, 3)"
  },
  {
    "objectID": "theme1/PE102/index.html#lets-do-some-actual-math-shall-we",
    "href": "theme1/PE102/index.html#lets-do-some-actual-math-shall-we",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "The trivial example: add a scalar (“a single number”) to every element of the matrix:\n\n\n[0. 0. 0.]\n[3. 3. 3.]\n\n\nYou can use any of the Python operators, of course: +, -, *, /, %, **…\n\n\n[[1 0 1 0]\n [1 0 1 0]\n [1 0 1 0]]\n\n\nComparison operators (like &gt;, &lt;, and so forth) are legitimate operators, so they work too:\n\n\n[[ 1  2  3  4]\n [ 5  6  7  8]\n [ 9 10 11 12]]\n\n[[False False False False]\n [False  True  True  True]\n [ True  True  True  True]]"
  },
  {
    "objectID": "theme1/PE102/index.html#linear-algebra-anyone",
    "href": "theme1/PE102/index.html#linear-algebra-anyone",
    "title": "NumPy - A Mathematical Toolkit for Python",
    "section": "",
    "text": "Let’s use NumPy to do some basic linear algebra. First, we’ll need another module in the NumPy package:\nThat import statement went out to where Python packages are stored and found the “linalg” module of the numpy package. This module was imported into the Python interpreter under the name “nl” (as in “NumPy linear algebra”). Using the “nl” alias saves a lot of typing and even makes the code easier to read.\n\n\n[[1 1 1]\n [1 1 0]\n [1 0 0]]\n\n\narray([[ 0.,  0.,  1.],\n       [-0.,  1., -1.],\n       [ 1., -1., -0.]])\n\n\nAnd given a matrix and its inverse, you probably already guessed where this is going:\n\n\narray([[1., 0., 0.],\n       [0., 1., 0.],\n       [0., 0., 1.]])"
  },
  {
    "objectID": "theme1/PE100/index.html",
    "href": "theme1/PE100/index.html",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "",
    "text": "Note\n\n\n\nThe content in this page is written in the form of Jupyter notebooks. You can read the HTML version of the notebooks here. However, you will likely want to open and run the notebooks in a JupyterHub instance, such as CLASSE JupyterHub. See CLASSE Wiki for details."
  },
  {
    "objectID": "theme1/PE100/index.html#operators",
    "href": "theme1/PE100/index.html#operators",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Operators",
    "text": "Operators\nLike any programming language, Python lets you “do math” and lots of other things. Let’s take a look at some of the basic “operators”. In all of the code-containing cells through this course, try to predict what will happen first, and then run the code.\n\n\n2\n\n\nBesides the “classic” operators, there are some handy extras:\nWhat happened there? The // operator does integer division - it returns the whole number part of the answer, just like when we learned division in elementary school.\nThe % operator returns the remainder. This is also called “modulo”, and the above would be pronounced “sixteen mod 3”.\n\n\n256\n\n\nThe ** operator does exponentiation. The arguments can be integers or they can be real numbers. Naturally, operators can be combined into arbitrarily long expressions.\nNotice what happens when we use different operators. They are applied in the “My Dear Aunt Sally” order of precendence (multiplication, division, addition, subtraction).\nOrder of operations: * Exponentiation: ** * Multiplication, Division, Remainder: * / // % * Addition and Subtraction: + -\nWithin the same level, operators are applied left-to-right. 8-5+2 is evaluated as 3+2 and yields 5. The exception is exponentiation: 2 ** 3 ** 4 is treated as 2 ** 81 and yeilds an annoyingly large number"
  },
  {
    "objectID": "theme1/PE100/index.html#variables",
    "href": "theme1/PE100/index.html#variables",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Variables",
    "text": "Variables\nUnless we just use Jupyter as a big, expensive scientific calculator, we need a way to store data. Variables were invented for just that purpose, and virtually every language has them. Think of them as a place to store data of some kind, and that place has a name. They behave in Python just like you’d expect.\n\n\n42\n\n\nWe just created a variable named answer and gave it the value 42. Variables are long-lived - later we’ll talk about just how long when we start writing our own functions, but until then our variables last as long as Python (or in our case, Jupyter) is running. Take a look - answer is still there.\nThe value stored in a variable can change. It can even change type:\nWe can declare many variables, and we can “do things” with them just like we can when we type in numbers or strings.\nIn the last line, we just put watts because Jupyter automatically prints what the last line evaluates to.\nWe can use variables to change the order of operations. Let’s see the average price of two people’s meals:\nThat’s the right answer. If we hadn’t done that, we would have gotten\nwhich is utterly wrong. Beware of the order of operations… it is a frequent source of bugs in scientific programming.\n\nVariable Naming Rules\nFor the most part, you can pick whatever name makes sense for a variable, but there are some rules. When choosing a name: 1. No keywords (False won’t work.) 1. No spaces (sample thickness is invalid) 1. The first character must be one of * a-z, or A-Z, or _. (the underscore character) * As a result, no numbers (3rd_sample_holder is invalid) 1. After the first character, you can then have numbers (sample_holder_3 is perfectly valid) 1. No other symbols are allowed (exploded&destroyed_spectrometers is invalid, and probably suggests it’s time to review lab safety procedures).\nNote: Uppercase vs. Lowercase matters! Bevatron is not the same variable as bevatron"
  },
  {
    "objectID": "theme1/PE100/index.html#types",
    "href": "theme1/PE100/index.html#types",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Types",
    "text": "Types\nWe’ve hinted that variables have a “type”, and that the type can change if it needs to. The way it works is that variables keep track of what type they are (integer, real number, or string) and what their “value” is. We can even interrogate a variable as see what type it is:\n\n\nreading:\n&lt;class 'float'&gt;\n&lt;class 'str'&gt;\n\n\nThe type of a variable matters. Let’s create a variable with an integer in it and another with a string. Then let’s do some math:\nHow do we handle situations like that, where second_thing held a string representing a seven, but because it was a string variable it couldn’t be used as an integer? Python provides a few functions to convert values from one type to another. The str() function takes a variable and converts it to a string. The float() and int() functions convert their arguments to floating-point and to integer numbers, respectively.\n\n\n13\n13.0\n\n\nBeing able to convert values from one type to another is often called type coercion. These conversions are extremely important for situation where you need to get input from a user, even more so if you need to do it repetitively."
  },
  {
    "objectID": "theme1/PE100/index.html#continuation-character",
    "href": "theme1/PE100/index.html#continuation-character",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Continuation Character",
    "text": "Continuation Character\nSometimes the expressions we need to evaluate can be very long. It would be nice if we could split up a long expression and spread it out over a few lines. As a small example, we’ll take a look at 4+2+3. Many programming languages will let us split an expression anywhere we want, such as:\n…but that result isn’t right in Python. The last line, +3, was evaluated and printed as the result of running that cell. In Python,it turns out, if we need to continue an expression on the next line we must end the current line with a backslash \\ and press enter. It has to be a backslash, by the way, and cannot be the forward slash like we use for division.\n\n\n9\n\n\nTime for an exercise! Try to predict what will be printed when you run the next cell. Then, run the next cell and see how you did. If you miss one, make sure you figure out what happened before you go. I know, we’re professionals, I shouldn’t have to say that…\nNow write an expression to average three numbers (12, 14, and 66), divide the result by three, and square it. You can use the code cell right below here:"
  },
  {
    "objectID": "theme1/PE100/index.html#the-string-type",
    "href": "theme1/PE100/index.html#the-string-type",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "The String Type",
    "text": "The String Type\nAt the beginning of this notebook, we casually mentioned “strings” without saying what they are. They’re just “sequences of characters”. And these can be any kind of characters - the English alphabet, the Hungarian alphabet, hiragana… it doesn’t matter.\nSome, probably most, languages contain strings inside “double quotes”, \", which is shift+apostrophe on US English keyboards. Other languages (SQL and Pascal are the only two I can think of) use single quotes: '. Python lets you use either one. You do have to be consistent in each string, but it can vary from one string to the next:\nBecause we can use either type of quotation mark, we can exploit that to let us put quotation marks into strings:\n\n\nDon't put explosive mixtures in the spectrometer, please.\nOf course he was warned... \"Do not turn the spectrometer into a bomb, please\" but I am sure he ignored that.\n\n\nThat lets us embed whichever kind of quotation mark we need into a string.\nBut what if we need to embed both kinds of quotes into one string? We’re in luck: we can use the backslash character again to “quote” our quotation mark. In fact, we can quote any character with it if we need to.\n\n\nWe told him \"Hexanitrohexaazaisowurtzitane and spectrometers don't mix, buddy\", but we're pretty sure he ignored us.\n\n\nThat sentence contains three things, inside the string itself: 1. Double Quotes to surround a direct quotation 2. A single quote, also called an apostrophe depending on how it’s used, to make a contraction, and 3. A totally awesome/terrifying molecule you have to google to believe.\nOK, I’ll save you the trouble. Prepare to lose most of a day’s productivity. You’re welcome.\n(Derek has written gobs of articles on fun substances. Here are some more. )\nThere is one last kind of string literal. Sometimes you need a string that is several lines long. The “triple quote” is a way to do it. You have to use three double-quotes in a row:\nTriple quotes are also an easier way to embed mixed kinds of quotation marks into strings:\n\n\nI know people who say \"The Avengers\" isn’t a good movie, but I don’t agree."
  },
  {
    "objectID": "theme1/PE100/index.html#coming-up-next",
    "href": "theme1/PE100/index.html#coming-up-next",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Coming Up Next",
    "text": "Coming Up Next\nWe just looked at enough of Python and Jupyter notebooks to use it as a basic calculator, but so far we can’t do any real, general-purpose programming with it. The “flow of control” sob far as been a straight line from top to bottom and we can’t change what we’re doing in response to different inputs. That’s about to change. In the next section we’ll look at the if statement and how to use it."
  },
  {
    "objectID": "theme1/PE100/index.html#the-simplest-if-statement",
    "href": "theme1/PE100/index.html#the-simplest-if-statement",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "The Simplest “if” Statement",
    "text": "The Simplest “if” Statement\nIn almost any real Jupyter notebook or standalone program we write, there will have to be places where different code paths are taken depending on what has happened leading up to there. Suppose we’re looking at absorption at one specific wavelength and we know that some of our instruments are a little bit too sensitive to changes in humidity. Maybe the first spectrometer has some insulation that is just a little too porous and reads a bit high, but the second one is even worse. We have calibration constants we can apply, but we have to apply the right constant for each individual instrument.\n\n\n7.539441569999999\n\n\nHere we have the first Decision Structure (also called control flow statement) that we’ll look at. Taking the above code apart, we see several important things.\n\nThis is an “if statement”.\nTesting to see if two things are equal is done with two equals signs, not one (==). There’s a historical reason for this, and it’s a good reason, but it always trips up newcomers. You have been warned. You’re welcome.\nThe last character on the if line is : (a colon ).\nThe “body” of the if statement, the part that is run if and only if the tested condition is met, is indented.\n\nIn the case of the above if statement, what the code does is check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called “useful_result”."
  },
  {
    "objectID": "theme1/PE100/index.html#else-the-catch-all-specialist",
    "href": "theme1/PE100/index.html#else-the-catch-all-specialist",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Else: the catch-all specialist",
    "text": "Else: the catch-all specialist\nIf that was all an if statement could do then it would be really useful. But that’s not all it can do. We need to do something reasonable when we get readings from the second instrument. Such as:\n\n\n8.3304879\n\n\nHere we have added an “else clause”. The above code is interpreted as “check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called useful_result. Otherwise, set useful_result to whatever is saved in”reading” plus 19%.\nSo far, so good. But there’s more! Suppose we need to handle several of these not-quite-top-quality spectrometers. How do you suppose we could deal with that? We could resort to putting if-else statements inside if-else statements in sort of a brute force fashion…\n\n\n6.4403771999999995\n\n\nThe above code looks a little intimidating, but all there is to it is just a series of if statements. The logic of it goes like this: “If the instrument number is 1, then adjust it 7.7% and we’re done. Otherwise, it must be some other instrument number, so run our else clause”. Then in the else clause, it does the same thing, except checking for the second instrument and adjusting by 19%. If there was nothing to do there (because the instrument number was 3) then we run the else clause of that second if statement. This else clause houses an if statement that checks to see if the instrument is number three. This time it is, so the body of the if statement is executed. We set useful_reading equal to 92% of reading."
  },
  {
    "objectID": "theme1/PE100/index.html#elif",
    "href": "theme1/PE100/index.html#elif",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Elif",
    "text": "Elif\nThis is fine if we only have three instruments, but what do we do if we have 20 of them? We could, in principle, type in 60 lines of code, but that would be tedious, error prone, and would take a while to read and find any mistakes. Of course there’s a better way.\nThat better way is the “elif” keyword.\nLet’s see an example with 5 instruments…\n\n\n7.210422299999999\n\n\nThe final else clause is the one that runs if no other clauses ran. If no clause’s conditional statement is true so no clause runs, whether it’s the if clause or any of the elif clauses, then the else clause runs. It’s really easy to spot else clauses even from across the room - they’re the ones that don’t have a conditional test.\nNote that the if, elif, and else lines must end with a colon. True confession time: I forget the colons about half the time. Python catches it as an error, I fix it, and life goes on."
  },
  {
    "objectID": "theme1/PE100/index.html#slightly-more-complicated",
    "href": "theme1/PE100/index.html#slightly-more-complicated",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Slightly More Complicated",
    "text": "Slightly More Complicated\nYou can run more than one line of code in response to the tested conditions, but they have to be indented the same amount:\n\n\n7.00041 True\n\n\nThere are four interesting things going on here. The first and most important thing to notice is that we’ve got more than one line of code running in response to an “if”, “elif”, or “else” clause. A collection of lines that should be run together as a whole is called a code block. Unlike many languages that mark the start and end of code blocks with special words or characters, Python just does it by using indentation. Everything that is indented the same amount is considered to be in the same code block. We’ll look at this in more detail in a few minutes.\nSecondly, we’ve added lines to set a variable named “trustworthy” to a value depending on whether we had to adjust the reading. Evidently, if we have to compensate for old, dry, cracking insulators then we don’t really trust the instrument.\nThe third interesting thing is the values True and False. These are “Boolean” values, and when we put them into the “trustworthy” variable then it takes on the Boolean type. There are only two values, True and False. The capitalization is important.\nThe fourth thing to notice is that we’re sending two values into the print statement and it’s printing both of them. In general, we can give the print statement any number of arguments, separated by commas, and it will print all of them separated by one space."
  },
  {
    "objectID": "theme1/PE100/index.html#conditional-aka-relational-operators",
    "href": "theme1/PE100/index.html#conditional-aka-relational-operators",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Conditional (aka Relational) Operators",
    "text": "Conditional (aka Relational) Operators\nThe conditional test in each part of an if statement is an expression that results in a Boolean value. So far, the only conditional operator (or relational operator) we’ve seen is ==. There are others, though. For the sake of completeness, I’ll include == here:\n\n\n\noperator\ntested condition\n\n\n\n\n==\nequals\n\n\n!=\nnot equals\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal\n\n\n\n“Relational” has at least two meanings in computing. Relational Operators have nothing to do with Releational Databases.\n\nTry This\nFor each of the following code cells, decide what the result is, run the cell, and see how you did:\n\n\nTrue\n\n\n\n\nTrue\n\n\n\n\nFalse\n\n\n\n\nFalse\n\n\n\n\nFalse\n\n\nRelational operators also work with strings.\n\n\nequals Alice.\nThe person is not Bob.\nAlice comes before Bob in alphabetical order.\nAlice comes before or in the same place as Alice in sorted order\nWorking left to right, the M, the a, and the r match on\nboth strings, but when we finally get to the y and the k, y comes\nafter k in alphabetical order.\n\n\nA couple words of caution: the comparisons are based on the ASCII codes for each character. The “A” in ASCII stands for “American”, and as you might expect that means it only works for English language text. If you need to handle other languages, even potentially, then there is a better way to do it and we’ll see that in the lesson on strings.\nAlso, Capital letters are always less than lowercase letters, and not in the way you might think. “A” is less than “Z”, as you might expect, but “Z” is greater than “a”. The numbers 0-9 are the lowest of all. Punctuation is sprinkled around and the only way to know for sure is to look up “ASCII Chart”."
  },
  {
    "objectID": "theme1/PE100/index.html#code-blocks",
    "href": "theme1/PE100/index.html#code-blocks",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Code Blocks",
    "text": "Code Blocks\nLet’s go back to that part about running several lines of code but they have to be indented the same amount. Python always runs “blocks” of code. That block might be as short as one line:\n\n\n125.6636\n\n\nor it might be arbitrarily long:\n\n\n1511396.1762899999\n\n\nWhether it was the one line example or the eight line one, Python will set out to run all of those lines in one shot, and as long as there aren’t any errors it’ll do it. These are known as code blocks.\nThe decision structures (again, also called control flow statements) in Python all do basically the same thing: they evaluate an expression and depending on whether it turns out True or False, they execute a code block in some manner. This means that wherever we can have a single line of code running in a decision structure we can have as many lines as we want.\nTake a look at the following example. For the four possible combinations of potentially_hazardous and explody, decide what would be printed out. Then try out the combinations and make sure you know why each combination was handled the way it was.\n\n\nTotal Available Kaboom (TAK) to ruin your day is 1511396.1762899999\n\n\nDid you notice potentially_hazardous and explody? and is a boolean operator. We’ve seen the arithmetic operators already (+, -, *, /, etc.) and now here are the boolean operators. They’re named after Boolean algebra, the algebra of logic, and are used to make larger logical expressions from smaller ones. There are three boolean operators: and, or, and not.\nThe and operator evaluates to True if both of its arguments are True. The or operator evaluates to True if either or both of its arguments are true. The not operator takes only one argument and reverses it: not turns True into False and False into True.\n\n\nDoctor of Medical Dentistry (DMD)"
  },
  {
    "objectID": "theme1/PE100/index.html#coming-up-next-loops",
    "href": "theme1/PE100/index.html#coming-up-next-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Coming Up Next: Loops",
    "text": "Coming Up Next: Loops\nAt this point, we’ve seen the most basic way to alter the flow of control in Python: the if statement. We can write Python code to solve non-trivial problems now, but there are still some things we need in order to use Python as a truly general-purpose language. In the next notebook we’re going to make our code do something over and over."
  },
  {
    "objectID": "theme1/PE100/index.html#while-loops",
    "href": "theme1/PE100/index.html#while-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "While Loops",
    "text": "While Loops\nThe syntax of a while loop looks a bit like an if statement. Take a look:\n\n\nLooking at instrument number 1\nand then maybe we'll look at the next one.\nLooking at instrument number 2\nand then maybe we'll look at the next one.\nDone with all that looping.\n...and ready to do something else now.\n\n\nHere’s what the above code does. First, it creates a variable named “instrument” and sets it to 1. Then it goes into the while loop. The first time through, it checks to see if instrument is less than or equal to 2. It is (because we set it to 1 just a moment ago) so the while loop will execute the code block. This block prints out two lines and then it adds 1 to instrument. That means instrument now equals 2.\nThe second time through the loop, instrument equals 2. That satisfies the conditional statement of the while loop (2 is less than or equal to 2) so the code block runs again. Two more lines are printed out and then instrument is incremented one more time.\nThe while loop runs for a third time now. This time, 3 is not less than or equal to 2, so the conditional statement is false. This means the while loop is done - it won’t run its code block again, and the flow of control will go on to the next line after the while loop. It will run the two print statements explaining that the looping is over and it can go on to other tasks.\nLet’s look at another example. Let’s print out all the powers of two that are less than 928.\n\n\n2 to the 0 equals 1\n2 to the 1 equals 2\n2 to the 2 equals 4\n2 to the 3 equals 8\n2 to the 4 equals 16\n2 to the 5 equals 32\n2 to the 6 equals 64\n2 to the 7 equals 128\n2 to the 8 equals 256\n2 to the 9 equals 512\n2 to the 10 is too big.\n\n\nDid you notice I sneaked something in there we haven’t talked about yet? See the “#” character on the line with the while statement? That indicates the rest of the line is a comment. Python will totally ignore it. It’s handy for leaving little notes to yourself, like “why did I choose 928 there when I could have put 944?” This is very, very important when writing full-fledged, standalone programs. If you don’t leave some notes for yourself, you’ll never remember what you were thinking when you go back to that code six months from now. Also, the next person who comes along and has to change something in your code will greatly appreciate the hints.\nLeaving comments in the code isn’t as big a deal in Jupyter notebooks… you can write rather substantial notes in a Markdown cell complete with boldface, italics, and whatever other fanciness you desire. On the other hand, it’s also nice to be able to leave your comments in the just the right place in the code so it flows effortlessly through your comprehension as you read it. Let experience and personal opinion be your guide here."
  },
  {
    "objectID": "theme1/PE100/index.html#reading-information-from-the-outside-world",
    "href": "theme1/PE100/index.html#reading-information-from-the-outside-world",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Reading information from the outside world",
    "text": "Reading information from the outside world\nNotice that in both of those cases, we actually did know how many times the loop would run. We know that 2 to the 9th is 512 and so we know the while loop will only run that far. In fact, in every example we’ve had so far we’ve know what the output will be because we always have the same inputs. Computer software wouldn’t be terribly interesting if it could only run specific, known, canned inputs. Fortunately, Python gives us several ways to bring data into our programs.\nThe simplest way to bring data into a Python program is to edit the program and change the values we assign to variables. This is sort of the reducto ad absurdum method, but honestly it isn’t a bad way to handle very small amounts of input. It’s even easier in Jupyter notebooks since the code is just sitting there looking at us, waiting to be edited. For values that aren’t going to change very often (your name, perhaps, or the chargeback account number for using some instrument, for instance) then just assigning a value to a variable and editing it every once and a while is a fine way to go.\nAnother way to get data into a Python program is to read it in from where the user is running the program. For doing this, Python provides a function called “input” which takes an optional argument, specifically a string that is printed as a prompt. Python then waits for the user to type something as a response. When they do, that string is returned to the calling program. Here’s a simple example:\n\n\nPlease enter your name Erik\n\n\nHello, Erik\n\n\nWhen the above code runs, the prompt “Please enter your name” is displayed right below the code cell and a text entry box is placed beside it. When you enter your name, it greets you.\nIf we were running this tiny little snippet of code as a regular program, the interaction would be in the terminal emulator window that we ran the program in. Because this is running in Jupyter, though, the interaction is directly in the notebook. The prompt and the entry blank occur just below the running code cell.\nWhat will happen when we run the following?\n\n\nEnter a number between 4 and 8 5.25\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [4], line 2\n      1 response = input(\"Enter a number between 4 and 8\")\n----&gt; 2 new_value = response + 6.5\n      3 print(new_value)\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\nWow! Python couldn’t run that and it “threw an error”. We’ll examine Python’s error handling facilities later, but for now we’ll just assume that means it came to a screeching halt. Looking at the error message, it seems there is some problem with trying to add a real number (a floting point number) to a string."
  },
  {
    "objectID": "theme1/PE100/index.html#type-casting",
    "href": "theme1/PE100/index.html#type-casting",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Type Casting",
    "text": "Type Casting\ninput() prompts the user and returns the string they entered, but what if we want the user to enter a number? What do we do then? The answer is we’ll use a process known as type casting. The act of type casting is no more than converting information from one type to another.\nThere are three very useful functions for type casting: int(), float(), and str(). Let’s see them in action…\n\n\n16.454\n34543456\n4\n\n\nWhat did the above do? First, it converted the string “9.9” (literally, three characters… it’s a string) to a “float” (a floating point number, some languages will call that a real number). The second example takes a string of 8 characters and interprets them as an integer. That value is what gets returned and stored in our variable. Finally, we copmute the number 4 by adding 2+2, and then we let the str() function convert that to a single character long string having just the character “4”.\nBy now we know enough to be able to ask the user for a number and get something back that we can actually do math with.\nThere’s an even easier way, though. Just like function composition worked when you took precalculus, the results of a Python function can be used as the argument to another. Hence:\n\n\nEnter a number between 17 and 34 26\n\n\n26.0\n\n\nSometimes, function calls can be nested really deeply. Personally, when it comes time to debug code like that I find myself printing it out and coloring each level with a different highlighter pen."
  },
  {
    "objectID": "theme1/PE100/index.html#putting-it-together-while-loops-to-get-user-input",
    "href": "theme1/PE100/index.html#putting-it-together-while-loops-to-get-user-input",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Putting it together: while loops to get user input",
    "text": "Putting it together: while loops to get user input\nThe great thing about a while statement is that it can loop zero times, one, two, or twelve trillion. Best of all, we don’t have to know how many ahead of time. We could do the following:\nprint(“Computing an average.”)\nsum=0.0 counter=0 data_point = float(input(“Enter a number, or enter negative num to stop”)) while data_point &gt;= 0.0: sum = sum+data_point counter = counter+1 data_point = float(input(“Enter a number, or enter negative num to stop”))\nprint(“Average value is”, sum/counter)\nWhen we run the code above, we’re prompted to keep entering numbers until we finally enter -999. Each time it goes through the loop it keeps track of the running total of the numbers and the count of how many numbers have been entered. Once it’s done, it divides the total by the count and displays that as the average.\nLet’s step through what happens when the user enters 1, 2, 3, and -999: 1. The sum and counter variables are initialized to zero. 1. The user is prompted to enter a number, possibly a negative number to indicate no more data, and that input is type cast to a floating point number. 1. The while loop’s condition will be met any time a positive number was input (greater than or equal to zero). 1 is a positive number, so run the loop body. 1. This first time through, we’ll add the 1 that was input to our running total, which is now 1. 1. And increment the count, now equal to 1. 1. AND PROMPT THE USER FOR ANOTHER NUMBER!!! 1. Back at the while statement again, we check the condition and, yes, 2 is a positive number, so we run the loop’s code block. 1. Update the sum and count, and then… 1. PROMPT THE USER FOR ANOTHER VALUE!!! 1. Running the while statement again, the user entered 3, and 3 is positive, so the clode block will be executed. 1. Update the sum (now 6) and count (now 3). 1. Prompt for another number 1. Back at the while statement, we check and see that -999 is not a positive number, so we skip the code block and resume by running whatever follows it. 1. Having exited the while loop entirely, print out the average value by dividing sum/count.\nAll the boldface and all-capitals lines above are there to emphasize how important it is to make sure your while loop isn’t just checking the same thing over and over. If we didn’t get a new number from the user each time through, the value of data_point would never change. That would result in an infinite loop, causing Python to never be able to complete the code in that cell. If it ever happens to you, and it probably will, the “Interrupt Kernel” command on JupyterLab’s Kernel menu will stop the looping and let you get back to work.\nThe while loop is certainly versatile… it can be used any time you need to do something repeatedly. If you know how many times you need to have the code block execute, either when you write the code or when it’s running, then keep a variable that is incremented in the block every time and exit the while loop when the counter hits the right number.\nWhere while loops really shine is when it’s impossible to know ahead of time how many times the code block should run. The example above, where we keep accepting numbers until the user signals there aren’t any more, there’s no way to know how many times to execute that loop until we see a negative number. In a case like that, the while loop is the only practical solution.\nSo if while loops are so great and solve every problem, why do we need anything else? The big reason is expressiveness: they can be a little awkward to understand, especially when you’re looking at someone else’s code. Having the conditional test separated from the action that establishes when to stop makes it a little awkward to understand (or debug!) someone else’s code. This is especially true when we need to step through something by unusual increments.\nSo what are we to do in these cases?"
  },
  {
    "objectID": "theme1/PE100/index.html#for-loops",
    "href": "theme1/PE100/index.html#for-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "For Loops",
    "text": "For Loops\nThe for loop is quite similar to the while loop. The difference is that for loops are controlled by a count whereas while loops are controlled by a condition.\nLet’s start with an example.\n\n\n1\n2\n3\n\n\nThat is the simplest for loop you’ll see. Let’s look at the pieces. 1. The for statement itself 2. The name of the target variable whose value will be changing as the loop runs (“the_value” in this case”) 3. “in” - and if this reminds you of set membership then you’re on to something 4. “range()” - this is an example of an iterable, which means “something that can be stepped through”. 5. The colon… the one I forget 50% of the time. 6. The code block, in this case just a print statement.\nMost of the time, fairly close to “always”, the code block will take advantage of the target variable changing each time through. In our example, “the_value” is our target variable, as it loops through it will take on the values 1 through 3, and the code block has a print statement that uses it.\nBefore we examine the range() function, let’s take a look at another iterable. We’ll talk about lists in a later lesson, but for now we can just wave our hands around and understand enough for the moment.\n\n\nThe sample weighed 143.6 grams.\nThe sample weighed 141.9 grams.\nThe sample weighed 139.4 grams.\nThe sample weighed 144.23219 grams.\n\n\nYou can use the target variable as many times as you want to in the code block.\nNow let’s take a more detailed look at the range() function. In its most basic form it takes one argument - the stop value.\n\n\n0\n1\n2\n3\n\n\nThis single-argument form starts at zero, counts up by one each time, and doesn’t include the stop value. This is different from every other programming language you’ll ever encounter. It’s just one of those things.\nWe’ve already seen the two-argument form. It takes a starting value and a stopping value, and iterates by one from the start until the last value that is less than the stop.\n\n\n7\n8\n9\n\n\nAnd there’s even a three-argument form. The third argument is the amount to step by.\n\n\n12\n15\n18\n\n\nThe step size doesn’t have to be a positive number…\n\n\n6\n4\n2\n0\n-2\n\n\nIn case you’re curious, the step size cannot be zero. If you really want an infinite loop, and there are cases where it makes sense, you have to use a while loop instead.\nAs a general rule, any place where you can use an explicit value (a literal) you can use a variable. Arguments to a for loop are no exception:\n\n\nwhere should we start?  13\nwhere should we run right up to and stop just short of it?  15\nwhat should we step by?  2\n\n\n13\n\n\nIf we need to do something a specific number of times, we need to pay attention to our starting and stopping conditions. I’ve messed this up so many times I know now to be careful. You’ve been warned.\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\n\n\nThey add up to 9\n\n\nNotice something wrong? If you ask it to total 3 numbers, it only prompts for two of them. There are a couple of ways to solve this. The easiest is to just use the one-argument form of range().\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nThat offers a little insight into why Python has it’s funny “up to but not including” semantics: zero is a perfectly legitimate number and a very natural starting point.\nThe only problem with the single-argument method is that the values that the target variable goes through include zero. This may or may not be a problem if that value is used inside the code block. If you really need to count from one instead of zero, you can increment the stopping value:\n\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nAnd that behaved just like we expected.\nYou may have noticed a pattern already. We frequently need to compute a new value for an existing variable. What we’ve done so far has been along the lines of grand_total = grand_total + new_reading. Python gives us a shorthand way to write that. We could instead express that as grand_total += new_reading. There is no space between the plus and equals signs. The only reason this exists is to save you some typing. As you might expect, there are a few more of these Augmented Assignment Operators…\n\n\n\nOperator\nExample\nEquivalent\n\n\n\n\n+=\ncount += 1\ncount = count + 1\n\n\n-=\nx -= offset\nx = x - offset\n\n\n*=\nproduct *= val\nproduct = product * val\n\n\n/=\ny /= 3\ny = y / 3\n\n\n%=\nval %= 2\nval = val % 2\n\n\n\nOut of all of them, += is far and away the most commonly used one."
  },
  {
    "objectID": "theme1/PE100/index.html#nested-loops",
    "href": "theme1/PE100/index.html#nested-loops",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Nested Loops",
    "text": "Nested Loops\nYou know what’s fun to put in a loop’s code block? Another loop! Best of all, it comes in pretty handy when dealing with high-dimensional data. Plenty of algorithms rely on nested loops, too. Take a look at this:\n\n\nx= 0  y= 0\nx= 0  y= 1\nx= 0  y= 2\nx= 0  y= 3\nx= 1  y= 0\nx= 1  y= 1\nx= 1  y= 2\nx= 1  y= 3\nx= 2  y= 0\nx= 2  y= 1\nx= 2  y= 2\nx= 2  y= 3\nx= 3  y= 0\nx= 3  y= 1\nx= 3  y= 2\nx= 3  y= 3\nx= 4  y= 0\nx= 4  y= 1\nx= 4  y= 2\nx= 4  y= 3\n\n\nWhat’s going on here? Initially, the outer loop, the one that iterates zero through four and assigns it’s value to x, runs. When it starts running its code block for the x=0 pass, the for loop for the y variable starts. ‘y’ assumes the values 0 through 3, so the first four lines printed out are for x=0, y=0, then x=0, y=1, and so on through x=0, y=3. Once that inner for loop completes, the outer for loop gets to iterate again. Now the inside for loop runs again, only this time we have x=1. That’s why the next four lines are “x=1, y=0” through “x=1, y=3”. Every time the outer loop runs another iteration, the inner loop gets to run all the way from start to finish.\nIn later lessons, we’ll have a few opportunities to play with nested loops. In fact, we’ll get to do that in the very next lesson: Functions!"
  },
  {
    "objectID": "theme1/PE100/index.html#encapsulation",
    "href": "theme1/PE100/index.html#encapsulation",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Encapsulation",
    "text": "Encapsulation\nFunctions are useful in programming for the same reason they’re useful in math - ours encapsulate a chunk of code so you don’t have to think about what is in it every time. Imagine how tedious it would be to write a program that needed to compute cosine in a lot of different places in the code. You could, I suppose, type in a Taylor series expansion for cosine in each of the places where we need to compute a cosign. That would be irritating, error prone, and confusing to anyone else who has to read it. Instead, we can write a function exactly once to compute cosine and then call that function from many places in our code. Once we have the function tested and debugged, we don’t have to think about it again. That frees up mental energy for more productive uses.\nFunctions can be classified into one of two types. Void Functions exist for encapsulation and don’t actually return a value. print() is an example of a void function. Value-Returning Functions, as the name strongly implies, return a value to the calling code. inductiveReactance() is an example of one.\nHere’s another example. This time, we’ll define a function that calls another function.\n\n\nArea of a circle with a radius of 2 is 12.56636\n\n\nWe defined a function to compute the area of a circle. It needed to square a number and so we decided to write a function to do that. Functions can call other functions ad infinitum. In fact, functions can even call themselves! When that happens the function is said to be recursive. Recursive functions are very useful for solving some hard problems but they’re a little beyond an introductory module like this one."
  },
  {
    "objectID": "theme1/PE100/index.html#function-and-variable-naming",
    "href": "theme1/PE100/index.html#function-and-variable-naming",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Function (and Variable) Naming",
    "text": "Function (and Variable) Naming\nWhat kinds of names can we use for functions? The same ones we can use for variables! More specifically, * No keywords (e.g., False is invalid) * No spaces (e.g., my function is invalid) * The first character must be: * a-z, A-Z, or _ (the underscore character) * No numbers (e.g., 1st_function is invalid) * After the first character, the following are allowed: * a-z, A-Z, _, and 0-9 * No other symbols (e.g., get_room&board is invalid)\nAs a widely agreed upon best practice, names should be meaningful and be composed of lowercase characters with underscores as separators."
  },
  {
    "objectID": "theme1/PE100/index.html#function-arguments",
    "href": "theme1/PE100/index.html#function-arguments",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Function Arguments",
    "text": "Function Arguments\nInput Parameters to functions are called arguments. They are the primary and best way to put information into a function, and definitely the way that causes the fewest problems. Arguments to a function in Python are mostly analagous to what we’re used to in math, but of course Python has some extensions.\nA function can have any number of arguments, including zero. “A function of zero arguments” might sound like a mathematician’s idea of “humor”, but it can actually make sense in programming. Sometimes you just need to encapsulate part of your code so you don’t have to worry with it again. For instance:\n\n\n==============================\n==============================\nGreetings, User. I'll start \nloading the instrument config\nfiles and opening connections\nto them. It'll take a minute.\n==============================\n==============================\n\n\nNow the code to print that banner is hidden away inside a function we’ll never have to look at again. Less mental clutter means fewer bugs.\nAnd for the sake of completeness, functions can also take one or more arguments:\n\n\nThe race was 6.213712 miles long and my ankles were hurting the ENTIRE way.\nThe polynomial evaluates to: 1284.04\n\n\nWhen arguments are passed into a function, they become parameter variables and can be referred to inside the function just like any other variable. This handy because the variables inside a function are called local variables and they have special properties: nothing outside of the function can modify their value, they’re destroyed and re-created every time the function is called, and these local variables supercede any outside variables with the same name.\nTake a look for yourself:\n\n\nTwice the wavelength is 40\nTwice the wavelength is 40\nTwice the wavelength is 40\n\n\nDoes that seem odd to you? What happened is this: four lines from the bottom we created a variable named “wavelength” and set it to 20. We then called the function to print it out doubled. We passed the global variable “wavelength” to our function which took it as its only argument. That argument became a parameter variable that was coincidentally named “wavelength”. That “wavelength” parameter variable has nothing to do with the “wavelength” variable in the main part of the program. Our function doubles that parameter variable and prints it out. At that point, the function completes and the flow of control goes back to the main body.\nThe next time our function is called an entirely new, fresh set of variables and parameter variables is created. This is important - it means that if we call the function with the same value every time then we always get the same result. Functions are unable to save their “state”. Like a football player on a stretcher, they have no memory of what happened before.\n(OK, yes, there are ways for them to save their state. Sometimes it’s unavoidable and you just have to do it, but doing so makes more places for bugs to creep into your programs and makes it harder to understand later. Try to avoid it. We’ll talk about it later.)"
  },
  {
    "objectID": "theme1/PE100/index.html#variable-scope",
    "href": "theme1/PE100/index.html#variable-scope",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Variable Scope",
    "text": "Variable Scope\nThe degree to which your programs can “see” a variable is called scope. There are two levels of scope in most Python programming:\n\nGlobal Scope\n\nDefined in main Python file\nOutside of ANY function\nTry to avoid these!\n\nConsidered poor design\nDangerous to use: any part of the program anywhere can change these\nBug Magnet!\n\n\nLocal Scope\n\nVariables defined within a function\nOnly visible and useable from inside their own function!\nUse these if at all possible.\n\n\nThe danger in global variables comes from two things. The first is the fact that the value can be changed anywhere in your program, either in the main program or inside of a function, and it’s devilishly hard to keep track of where that might be.\nThe second danger is more subtle. When a function saves a value into a global variable, the function is now said to have side effects. Side effects break the idea of isolation that functions are meant to give us. Imagine a mathematical function, such as tangent, if it had side effects. Calling tan(.0125) would not only result in the tangent of .0125, but it would have some other effect on some unrelated part of math. Imagine if calling tan caused your coordinate system to change every time? That would be insane.\nIt gets worse, though. What if our tangent function also read from a global variable and changed its behavior based on that. Then each time we called tan(.0125) we might get a different value.\nIn other words, we basically broke math.\nSimilarly, when we write programs, if our functions have side effects then we’ve complicated them tremendously. And more complication means more places for bugs to sneak into our code and they’ll also be harder to find.\nAs an aside, there is a style of programming that eliminates global variables and, to an extent, even local variables. It’s called functional programming, and Python has some support for that style. There is usually more than one way to do anything in Python, and experienced Pythonistas will usually try to choose the most Pythonic way. Part of being in Pythonic style means to use (at least partially) a functional style."
  },
  {
    "objectID": "theme1/PE100/index.html#constants",
    "href": "theme1/PE100/index.html#constants",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Constants",
    "text": "Constants\nThere is an exception to the “no globals” rule: Constants. Just like in math, a constant is given a value once and never changed again. “Never changing” means “no side effects” so everything is OK. It is good practice to define your constants using ALL CAPITAL LETTERS.\n\n\n1.9878e-19"
  },
  {
    "objectID": "theme1/PE100/index.html#abstraction",
    "href": "theme1/PE100/index.html#abstraction",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Abstraction",
    "text": "Abstraction\nA valuable property of functions is how they isolate the code and variables inside of them from being manipulated elsewhere in your software. A consequence of that is their ability to “hide” detail from us. We’ve already talked about writing a function, debugging it, and never having to look at the code inside of it again. What is every bit as useful, if not more so, is using functions to provide abstraction.\nAbstraction is something we’ve used every day even if we haven’t thought it. Remember learning math? You started off counting things, and yes, that counts as math. If you had four bottle caps in one hand and three in the other, you could toss them all on the table, count them, and know that you have seven in total.\nThere are two problems with having to count everything. One is that the amount of stuff can get big in a hurry. Try using two hands and table to count sand grains. The other problem is that if there are any insights to be had, it’s hard to find them when you’re stuck down in the details. Fortunately, we learned arithmetic.\nArithmetic is great. We don’t have to deal with handfuls of stuff anymore. We can just use numbers and operators and get an answer without a bunch of messing around. We can start to see patterns we never would have just tossing bottle caps on the table. If we need to add 12 to something, we can instead add 10 and then add 2 more. This is so handy. Of course, it would be nice if we could just do something to analyze entire families of arithmetic problems.\nAlgebra lets us analyze entire families of arithmetic problems. We don’t have to fool with numbers if we don’t have to - we can just substitute variables in their place. We’ve hidden some of the complexity, like the petty little details of numbers, and abstracted that complexity away.\nSimilarly, a lot of problem solving is perfectly amenable to using abstraction. Let’s write a bit of code to run an experiment…\nThat function is a (admittedly fanciful) representation of running an experiment. It makes sense, anyone can understand it, and if there’s a bug in there then it’s going to be really obvious. The only problem: if we try to run it, it’ll crash because those other functions haven’t been defined yet. Shall we fix that?\nNotice how the program is broken up into several functions? The best part is that you don’t have to keep everything in your head. All you have to remember is the part you’re working on. Smaller pieces, fewer bugs."
  },
  {
    "objectID": "theme1/PE100/index.html#modules",
    "href": "theme1/PE100/index.html#modules",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Modules",
    "text": "Modules\nOne reason Python has become so popular is the sheer amount of code that has been written in it and made available for public use. We’ve seen a few functions already that were built in to Python - int(), float(), and str(), for example - but there are many tens of thousands of modules that are freely available for use in your own software. Just picking five common ones at random:\n\nmath\nrandom\nos\nPyMySql\npsycopg\n\nThe first two contain functions for general-purpose math and for producing random numbers. The “os” module interfaces Python with the operating system the code is running on. PyMySql and psycopg provide connectivity to relational databases.\nRemember at the beginning of this lesson when we wrote a function to calculate inductive reactance? I put the value of pi in there as 3.14159, but that really isn’t anywhere near enough digits for some problems. Let’s fix that:\n\n\n3.141592653589793\n\n\nThere are two things to note here. First, the keyword import is used to tell Python to go find a module with the right name and load it. The name we want it to find is the word right after the import. And secondly, just looking at the output we can see that there are a lot more digits than when we did something by hand in our Inductive Impedance example (top of this page). In general, using a module that was (a) written by someone else and (b) is widely used and has been checked by a lot of people is going to avoid a lot of bugs. For instance, I would never code my own Fast Fourier Transform. Instead, I would use the one in the “numpy” module. I know how easy it is to make a mistake and I trust their work a lot more than my own. They have tens or hundreds of thousands of users and scores of developers. I have… a copy of Numerical Recipes that’s old enough to run for President.\nSince we used the “math” module already, here’s a very incomplete list of what is in there: * sin(), cos(), tan(), acos(), asin(), atan()… - “acos” is “arc cosine”, etc. * log(), log10(), sqrt() - square root * radians(), degrees() - converts between them\nAnd lots more stuff. How do you know what’s in it? Go to the online documentation: https://docs.python.org/3/library/math.html\n\nRandom Numbers\nAnother module that is heavily used is “random”. It generates random numbers, yes, but it can also do things like take a list of things and shuffle them randomly.\n\n\nThe random integer between 10 and 100 was: 20\nThe random float between 0 and 1 is: 0.6474502367565016\n\n\nThere are more functions available in the “random” module, including ones to select a real number from a non-uniform distribution. Take a look at https://docs.python.org/3/library/random.html\nHere’s a slightly more complicated example:\n\n\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nDon't feel bad... proposal number 24 didn't get funded either.\nProposal number 23 was funded!\nDon't feel bad... proposal number 24 didn't get funded either.\n\n\n\nLet’s try out what we’ve learned so far. Use the next code cell to write a bit of Python that simulates rolling a pair of dice and adds the two values. Print the value out.\nLet’s add to that… add a loop so that we keep doing that over and over until we get the same sum twice in a row. Some questions to ask yourself are “What kind of loop do I need?” and “How can I compare what happened between two different loop iterations?”"
  },
  {
    "objectID": "theme1/PE100/index.html#files",
    "href": "theme1/PE100/index.html#files",
    "title": "Programming Essentials - Python Programming and Jupyter Notebooks (PE100)",
    "section": "Files",
    "text": "Files\nPractically everyone is more-or-less familiar with the idea of a file, even if fairly few people know how they work. We’re going to ignore a lot of details for the moment and say this: a file is a long-lasting collection of bytes. It has a first byte, a last byte, and every one in between stays in the same order.\nThis begs the question “What is a byte?” A byte is just a small number from 0 to 255 (inclusive). We can assign meaning to those numbers, and if we’re smart about how we do it then we can represent any information a computer can process as long as we use enough of these bytes.\nWe like to think of files as being one of two types: binary files and text files. Binary files are pure data. We decide how to write bytes to a file to represent data. Then when we’re ready to read it in again, we read the bytes, process them somehow, and reconstruct the original data. It’s a great technique - it’s fast and efficient.\nWe won’t be talking about binary files in this notebook or even in this module. Fifteen years ago we wouldn’t have had a choice, we would have had to. These days, it’s unusual to have to deal with binary files, especially in Python, because there is so often a library function already available to do the work for us.\nText files, on the other hand, are probably something you’re already familar with - they are what you get when you edit a “plain text” file in “notepad” or “textedit”. In a text file, every one of the letter, number, and punctuation mark characters is assigned its own number. For instance, capital “A” is 65. “B” is 66. Not that it should ever matter, but here’s a complete list and then some!\nLet’s say you open an editor and type “CAT”. When you save that to a file, there will be a file that is three bytes long and contains the three bytes 67, 65, and 84. Actually there will usually be a fourth byte, 10, which is the character you get when you press “Enter” or “Return”.\nFor now, at least for a few minutes, we’re going to pretend the only language on earth is English. We’ll talk about other languages when we talk about networks.\nIt’s about time for an example, don’t you think?\nThree lines of code was all it took to create a file, write to it, and tidy up after ourselves. What does each of those lines do?\nmy_file_object = open(\"/tmp/first_file.txt\", \"w\")\nmy_file_object is an object variable. Think of an object as a way to store data in a variable along with some functions that only make sense to that data. They hide a lot of complexity from us. A file object is one that keeps track of a filename, how to get to it, and how to use it. It has some functions built in to it to help us do things to the file.\nPython gives us the function “open”. It gets a file ready to be used by our code. It takes two arguments. The first is the file’s name, and the second is the mode we want to use the file in. In our example, we specified that the file’s name was “first_file.txt” and that it was in the “/tmp” directory. Then in the second argument we specified “w”, meaning we wanted to write to the file. The “w” mode will cause the file to be created if it didn’t already exist. If it did already exist, on the other hand, all the contents of it will be deleted and we’ll start writing from the beginning just as if the file was created from scratch. We’ll see more modes as we go.\nmy_file_object.write(\"First Post!\")\nThis line uses one of those functions that are tucked away inside an object. In this case, we’re calling the file object’s “write” function. It does what we expect - it takes its argument, in this case “First Post!”, and causes it to be written to disk byte by byte.\nmy_file_object.close()\nFinally, we call one more of the file object’s functions: close. When we run this, Python tells the operating system “Hey, we’re done with the file. You can get rid of any of the tedious housekeeping data that operating systems keep behind the scenes!”\nClosing files is considered “good programming hygene”. You’re allowed 1024 file objects to be open and connected to files in one program on the CLASSE cluster of computers. I’ll say from my experience: if you think you need that many, you’re probably doing something the wrong way.\nWriting files, then, is fairly easy. What about reading files? I’m glad you asked.\n\n\n'First Post!'\n\n\nYou can probably tell mostly how that worked just by looking at. We used the open() function again, but this time with a “r” for our mode. This means “read”. Also, this time we used read() instead of write(). The read() function reads in an entire file and saves it a string variable. Finally, we call close() again to close the file and tidy up after ourselves.\nNote that if the file is, say, 500 megabytes long, the string variable is going to be very, very large - roughly half a gigabyte. Python can handle this, but it may not be terribly convenient. If the file is more than 100-200 gigabytes, the CLASSE servers are probably not going to be able to handle. I say “probably” because there are a lot of factors at play.\nJust writing one line to a file is probably not very useful. Let’s try writing two lines:\nWhen we run that, it will open /tmp/first_file.txt for writing and it will delete anything already in it (that’s what the “w” means, remember?). Then it will write “First line written.” and “This is my second line.”.\nLet’s read the file again and prove to ourselves that it worked…\n\n\n'First line written.This is my second line.'\n\n\nOh no! The two lines ran together!\nAnd that is one of the first differences we’ll see between write() and print(). Print() always adds a newline character after it prints out anything. Remember when I said there would usually be a byte at the end of a line, represented by the number 10? This character is called “newline” and it, as the name implies, marks where a new line starts.\nIn all likelihood, when we do two write() statements like we did, we want to put a newline character in the file to make it into two lines. Fortunately, there are several ways to do that. Here are two of them.\nThe first way is simple and direct - call write() three times instead of two and put a newline in there “by hand”, as it were:\n\n\n'First line written.\\nThis is my second line.'\n\n\nThe output looks a little strange. We put an extra write() function call, but we gave it an odd looking argument - . That is a backslash (usually between the Enter and the backspace keys on a US keyboard) immediately followed by a lowercase “n”. The combination together means “newline character”. This much is fairly straightforward.\nNext we read the contents of the file. This is just like before.\nFinally, and this is where things take an unexpected turn, we evaluate the_contents and let Jupyter print that out for us. And when Jupyter does that, we see the “” there. It seems like Python didn’t convert those two characters to a newline, just sticking them in there as-is, and still left us with one long line. But is that true? Has Python foresaken us?\nRun the code in the next cell:\n\n\nFirst line written.\nThis is my second line.\n\n\nSalvation! print() did the right thing. This is a key difference between just typing a variable or an expression at the end of a cell and letting Python evaluate it versus putting a print() in there and having absolute control over what gets sent to the notebook and on to the screen.\nThis also illustrates something else important and useful: all of the code cells in this notebook are being run by the same Python “interpreter”. This means if we set a variable to a value in one cell, we will see the same value stored in that variable in other cells. That’s how we were able to print what was stored in the_contents in the cell above even though we had set its value to the file contents two cells above that.\nIf a file only has a line or two, it’s not a big deal dealing with that with string functions. If a file has millions of lines, then it becomes a bit of a hassle. We need a way to read a file one line at a time. Fortunately, there’s readline():\n\n\nFirst line written.\n\nThis is my second line.\n\n\nThis does almost what we expect: it reads both lines from the file, one at a time, and prints them out. The only snag is that blank space between the lines. What has happened? It turns out readline() reads the entire line, even the newline character at the end. We can see this if we evaluate the string instead of just printing it:\n\n\n'First line written.\\n'\n\n\nThere’s that \\n again! What about the second line?\n\n\n'This is my second line.'\n\n\nWhen readline() reads a line, it includes the newline character at the end unless it reaches the end of the file and the file didn’t end with a newline.\nIt’s rare that we would want to read a bunch of lines in a file with the newlines included. That’s just not something we do very often, and practically never in scientific software. We’ll almost always want to trim off the newline character. And for that, we have the rstrip() function. It takes a string, strips off any newlines on the right side of it, and returns that cleaned-up string. rstrip() does that for the right side of the string, lstrip() cleans up the left side (the beginning of the string) and strip() goes crazy and does both ends at the same time.\nLet’s try it:\n\n\nFirst line written.\nThis is my second line.\n\n\nWhat’s going on here? A couple of things. The first thing to note is that rstrip() and its close companions lstrip() and strip() take one argument, which is the character to be stripped. Practically always we’ll want to get rid of the trailing newline character.\nThe other interesting things is how we called the rstrip() function in the first place. We gave the name of the string variable, a period, and the name of the function we were calling. This is just like how we called the close() function on a file object. And in fact, strings are another kind of object in Python. We’ll see a lot more on this later.\nHistorical note: The original programming language that had objects was named “Smalltalk”. In Smalltalk, the functions that were inside of objects were called “methods”. You’ll still hear people call them that. Later, the “C++” language came along and it called methods “member functions”. When programmers talk about the functions that are contained in objects, we’ll use either term interchangably, sometimes even switching in the middle of a sentence. We now return to your Python tutorial, already in progress…\nWe read both lines in the file we created. We were able to call readline() twice and know that we had all of our lines in the file because (1) we created the file ourselves and (2) we therefore knew it had precisely two lines. It wasn’t even too bad having to type those readline() and rstrip() lines twice. But what if we had a lot more lines? We would certainly want to use a loop.\nFor example, what do we do with a five-line file?\n\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nNo problem - we just use a for loop and do the readline() inside of it. It repeats the five times we asked for. In this case, after we read each line we cleaned it up a little and printed it.\nBut what if we can’t know the number of lines ahead of time? One approach is to have whatever program that creates the file write the number of lines that will be in it first. I won’t say this is a common approach in scientific software, but it isn’t exactly rare either.\n\n\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe overall scheme for this is probably obvious by now. In the first half, when we’re writing the file, we write a “5” on its own line, and then write five more lines. In the second part, we 1. Read the first line. 2. rstrip() to get rid of the trailing newline 3. Use the results of that as the argument to int(), converting that string (“5”) to an actual integer (5). 4. and finally go through a for loop that many times just like before\nMost of the time we won’t have the luxury of knowing how many lines are in a file, though. We need a way to read all of the lines, line by line, without limit. For that, we can loop through the file and quit when Python returns an empty string with not even a newline character.\n\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nThe while loop behaved just like we expected - strat by reading a line, and then every time the line isn’t empty, print it out and read another line. When you finally hit a line that is completely empty, exit the while loop and close the file.\nLooping through a file all the way to the end is such a common thing to do, Python has a shortcut for doing it. Remember when we talked about a for loop iterating over an ordered set? A file can be thought of as an ordered set of strings. They’re not in alphabetical order, but rather they are ordered by line number. That means we can:\n\n\n5\nLine 1.\nLine 2.\nLine 3.\nLine 4.\nLine 5.\n\n\nAs you can imagine, reading isn’t the only file operation you can do with a loop. You can also write to a file that way. For instance,\n\n\n0\n1\n2\n3\n4\n5\n6\n\n\nFinally, we don’t have to erase the contents of a file every time we write to it. It’s perfectly normal to append to an existing file, and for that the “a” mode can be used with open().\n\n\n0\n1\n2\n3\n4\n5\n6\n7\n8\n9\n\n\nWhen you use the append mode, the write() calls will either add to the existing file or, if it doesn’t already exist, it will be created and then written to as though we used the “w” mode.\nSo far in this lesson we’ve acted like everything just works perfectly every time. In reality, it’s not that neat. Filenames get typed in wrong, didks get full, and lines that are supposed to be numbers might contain text instead. Any of these problems is enough to bring our Python code to a grinding halt. Our next lesson is all about how to handle these problems and many, many more like them. We’re going to learn about Exceptions!"
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html",
    "href": "theme1/PE100/PE100-03DecisionStructures.html",
    "title": "PE100-03: Decision Structures",
    "section": "",
    "text": "In the first lesson, everything we did was sequential programming. Statements are executed one after the other in exactly the order they’re written in. As long as there aren’t any errors, every statement will be executed."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#the-simplest-if-statement",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#the-simplest-if-statement",
    "title": "PE100-03: Decision Structures",
    "section": "The Simplest “if” Statement",
    "text": "The Simplest “if” Statement\nIn almost any real Jupyter notebook or standalone program we write, there will have to be places where different code paths are taken depending on what has happened leading up to there. Suppose we’re looking at absorption at one specific wavelength and we know that some of our instruments are a little bit too sensitive to changes in humidity. Maybe the first spectrometer has some insulation that is just a little too porous and reads a bit high, but the second one is even worse. We have calibration constants we can apply, but we have to apply the right constant for each individual instrument.\n\nspectrometer_number = 1                                                        \nreading = 7.00041                                                              \n                                                                               \nif spectrometer_number == 1:                                                   \n    useful_result = reading * 1.077                                            \n                                                                               \nuseful_result\n\n7.539441569999999\n\n\nHere we have the first Decision Structure (also called control flow statement) that we’ll look at. Taking the above code apart, we see several important things.\n\nThis is an “if statement”.\nTesting to see if two things are equal is done with two equals signs, not one (==). There’s a historical reason for this, and it’s a good reason, but it always trips up newcomers. You have been warned. You’re welcome.\nThe last character on the if line is : (a colon ).\nThe “body” of the if statement, the part that is run if and only if the tested condition is met, is indented.\n\nIn the case of the above if statement, what the code does is check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called “useful_result”."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#else-the-catch-all-specialist",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#else-the-catch-all-specialist",
    "title": "PE100-03: Decision Structures",
    "section": "Else: the catch-all specialist",
    "text": "Else: the catch-all specialist\nIf that was all an if statement could do then it would be really useful. But that’s not all it can do. We need to do something reasonable when we get readings from the second instrument. Such as:\n\nspectrometer_number = 2                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelse:                                                                           \n    useful_result = reading * 1.19\n\nuseful_result\n\n8.3304879\n\n\nHere we have added an “else clause”. The above code is interpreted as “check to see if we’re using spectrometer number 1 and if we are then we add 7.7% to the reading and save it in a variable called useful_result. Otherwise, set useful_result to whatever is saved in”reading” plus 19%.\nSo far, so good. But there’s more! Suppose we need to handle several of these not-quite-top-quality spectrometers. How do you suppose we could deal with that? We could resort to putting if-else statements inside if-else statements in sort of a brute force fashion…\n\nspectrometer_number = 3                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelse:                                                                           \n    if spectrometer_number == 2:                                                \n        useful_result = reading * 1.19                                          \n    else:                                                                       \n        if spectrometer_number == 3:                                            \n            useful_result = reading * .92                                       \n                                                                                \nuseful_result\n\n6.4403771999999995\n\n\nThe above code looks a little intimidating, but all there is to it is just a series of if statements. The logic of it goes like this: “If the instrument number is 1, then adjust it 7.7% and we’re done. Otherwise, it must be some other instrument number, so run our else clause”. Then in the else clause, it does the same thing, except checking for the second instrument and adjusting by 19%. If there was nothing to do there (because the instrument number was 3) then we run the else clause of that second if statement. This else clause houses an if statement that checks to see if the instrument is number three. This time it is, so the body of the if statement is executed. We set useful_reading equal to 92% of reading."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#elif",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#elif",
    "title": "PE100-03: Decision Structures",
    "section": "Elif",
    "text": "Elif\nThis is fine if we only have three instruments, but what do we do if we have 20 of them? We could, in principle, type in 60 lines of code, but that would be tedious, error prone, and would take a while to read and find any mistakes. Of course there’s a better way.\nThat better way is the “elif” keyword.\nLet’s see an example with 5 instruments…\n\nspectrometer_number = 4                                                         \nreading = 7.00041                                                               \n                                                                                \nif spectrometer_number == 1:                                                    \n    useful_result = reading * 1.077                                             \nelif spectrometer_number == 2:                                                  \n    useful_result = reading * 1.19                                              \nelif spectrometer_number == 3:                                                  \n    useful_result = reading * .92                                               \nelif spectrometer_number == 4:                                                  \n    useful_result = reading * 1.03                                              \nelif spectrometer_number == 5:                                                  \n    useful_result = reading * 1.26                                              \nelse:                                                                           \n    useful_result = reading                                                     \n    print(\"Be careful!\")    \n                                                                                \nuseful_result\n\n7.210422299999999\n\n\nThe final else clause is the one that runs if no other clauses ran. If no clause’s conditional statement is true so no clause runs, whether it’s the if clause or any of the elif clauses, then the else clause runs. It’s really easy to spot else clauses even from across the room - they’re the ones that don’t have a conditional test.\nNote that the if, elif, and else lines must end with a colon. True confession time: I forget the colons about half the time. Python catches it as an error, I fix it, and life goes on."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#slightly-more-complicated",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#slightly-more-complicated",
    "title": "PE100-03: Decision Structures",
    "section": "Slightly More Complicated",
    "text": "Slightly More Complicated\nYou can run more than one line of code in response to the tested conditions, but they have to be indented the same amount:\n\nspectrometer_number = 103\nreading = 7.00041\n\nif spectrometer_number == 1:\n    useful_result = reading * 1.077\n    trustworthy = False\nelif spectrometer_number == 2:\n    useful_result = reading * 1.19\n    trustworthy = False\nelse:\n    useful_result = reading\n    trustworthy = True\n\nprint(useful_result, trustworthy)\n\n7.00041 True\n\n\nThere are four interesting things going on here. The first and most important thing to notice is that we’ve got more than one line of code running in response to an “if”, “elif”, or “else” clause. A collection of lines that should be run together as a whole is called a code block. Unlike many languages that mark the start and end of code blocks with special words or characters, Python just does it by using indentation. Everything that is indented the same amount is considered to be in the same code block. We’ll look at this in more detail in a few minutes.\nSecondly, we’ve added lines to set a variable named “trustworthy” to a value depending on whether we had to adjust the reading. Evidently, if we have to compensate for old, dry, cracking insulators then we don’t really trust the instrument.\nThe third interesting thing is the values True and False. These are “Boolean” values, and when we put them into the “trustworthy” variable then it takes on the Boolean type. There are only two values, True and False. The capitalization is important.\nThe fourth thing to notice is that we’re sending two values into the print statement and it’s printing both of them. In general, we can give the print statement any number of arguments, separated by commas, and it will print all of them separated by one space."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#conditional-aka-relational-operators",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#conditional-aka-relational-operators",
    "title": "PE100-03: Decision Structures",
    "section": "Conditional (aka Relational) Operators",
    "text": "Conditional (aka Relational) Operators\nThe conditional test in each part of an if statement is an expression that results in a Boolean value. So far, the only conditional operator (or relational operator) we’ve seen is ==. There are others, though. For the sake of completeness, I’ll include == here:\n\n\n\noperator\ntested condition\n\n\n\n\n==\nequals\n\n\n!=\nnot equals\n\n\n&gt;\ngreater than\n\n\n&gt;=\ngreater than or equal\n\n\n&lt;\nless than\n\n\n&lt;=\nless than or equal\n\n\n\n“Relational” has at least two meanings in computing. Relational Operators have nothing to do with Releational Databases.\n\nTry This\nFor each of the following code cells, decide what the result is, run the cell, and see how you did:\n\n5 &lt; 6\n\nTrue\n\n\n\n5.99 == 5.99\n\nTrue\n\n\n\n5 != 5.00\n\nFalse\n\n\n\n5+6 &lt; 11\n\nFalse\n\n\n\n6 * 6 &gt; 12 + 12 + 12\n\nFalse\n\n\nRelational operators also work with strings.\n\nname = \"Alice\"\nif name == \"Alice\":\n    print(\"equals Alice.\")\nif name != \"Bob\":\n    print(\"The person is not Bob.\")\nif \"Alice\" &lt; \"Bob\":\n    print(\"Alice comes before Bob in alphabetical order.\")\nif \"Alice\" &lt;= \"Alice\":\n    print(\"Alice comes before or in the same place as Alice in sorted order\")\nif \"Mary\" &gt; \"Mark\":\n    print('Working left to right, the M, the a, and the r match on')\n    print('both strings, but when we finally get to the y and the k, y comes')\n    print('after k in alphabetical order.')\n\nequals Alice.\nThe person is not Bob.\nAlice comes before Bob in alphabetical order.\nAlice comes before or in the same place as Alice in sorted order\nWorking left to right, the M, the a, and the r match on\nboth strings, but when we finally get to the y and the k, y comes\nafter k in alphabetical order.\n\n\nA couple words of caution: the comparisons are based on the ASCII codes for each character. The “A” in ASCII stands for “American”, and as you might expect that means it only works for English language text. If you need to handle other languages, even potentially, then there is a better way to do it and we’ll see that in the lesson on strings.\nAlso, Capital letters are always less than lowercase letters, and not in the way you might think. “A” is less than “Z”, as you might expect, but “Z” is greater than “a”. The numbers 0-9 are the lowest of all. Punctuation is sprinkled around and the only way to know for sure is to look up “ASCII Chart”."
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#code-blocks",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#code-blocks",
    "title": "PE100-03: Decision Structures",
    "section": "Code Blocks",
    "text": "Code Blocks\nLet’s go back to that part about running several lines of code but they have to be indented the same amount. Python always runs “blocks” of code. That block might be as short as one line:\n\ncircumference = 40 * 3.14159\n\ncircumference\n\n125.6636\n\n\nor it might be arbitrarily long:\n\nheight = 6.01\nlength = 5.5\nwidth = 14.3\ndensity = 4.2\nvolume = height * width * length\nmass = volume * density\nenergy_per_gram = 761.3\neyebrow_altering_potential = mass * energy_per_gram\n\neyebrow_altering_potential\n\n1511396.1762899999\n\n\nWhether it was the one line example or the eight line one, Python will set out to run all of those lines in one shot, and as long as there aren’t any errors it’ll do it. These are known as code blocks.\nThe decision structures (again, also called control flow statements) in Python all do basically the same thing: they evaluate an expression and depending on whether it turns out True or False, they execute a code block in some manner. This means that wherever we can have a single line of code running in a decision structure we can have as many lines as we want.\nTake a look at the following example. For the four possible combinations of potentially_hazardous and explody, decide what would be printed out. Then try out the combinations and make sure you know why each combination was handled the way it was.\n\npotentially_hazardous = True\nexplody = True\n\nif potentially_hazardous and explody:\n    height = 6.01\n    length = 5.5\n    width = 14.3\n    density = 4.2\n    volume = height * width * length\n    mass = volume * density\n    energy_per_gram = 761.3\n    eyebrow_altering_potential_energy = mass * energy_per_gram\n    print(\"Total Available Kaboom (TAK) to ruin your day is\", eyebrow_altering_potential_energy)\nelif potentially_hazardous:\n    print(\"Not likely to go 'kaboom', but not something you want to casually eat, either.\")\n    print(\"I mean, unless you're feeling brave.\")\n    print(\"Even then, it's a bad idea.\")\nelif explody:\n    print(\"This is one of those things that will blow up but isn't actually hazardous.\")\n    print(\"I'm guessing it's a vinegar-and-baking-soda volcano.\")\nelse:\n    print(\"As far as we know, the material in quesion is no more\")\n    print(\"dangerous than takeout pizza.\")\n\nTotal Available Kaboom (TAK) to ruin your day is 1511396.1762899999\n\n\nDid you notice potentially_hazardous and explody? and is a boolean operator. We’ve seen the arithmetic operators already (+, -, *, /, etc.) and now here are the boolean operators. They’re named after Boolean algebra, the algebra of logic, and are used to make larger logical expressions from smaller ones. There are three boolean operators: and, or, and not.\nThe and operator evaluates to True if both of its arguments are True. The or operator evaluates to True if either or both of its arguments are true. The not operator takes only one argument and reverses it: not turns True into False and False into True.\n\nmedical_license = True\ndental_license = True\n\nif medical_license and dental_license:\n    print(\"Doctor of Medical Dentistry (DMD)\")\nelif dental_license and not medical_license:\n    print(\"Plain old dentist.\")\nelif not dental_license and medical_license:\n    print(\"Garden-variety doctor.\")\nelse:\n    print(\"No license at all. Run. Quickly.\")\n\nDoctor of Medical Dentistry (DMD)"
  },
  {
    "objectID": "theme1/PE100/PE100-03DecisionStructures.html#coming-up-next-loops",
    "href": "theme1/PE100/PE100-03DecisionStructures.html#coming-up-next-loops",
    "title": "PE100-03: Decision Structures",
    "section": "Coming Up Next: Loops",
    "text": "Coming Up Next: Loops\nAt this point, we’ve seen the most basic way to alter the flow of control in Python: the if statement. We can write Python code to solve non-trivial problems now, but there are still some things we need in order to use Python as a truly general-purpose language. In the next notebook we’re going to make our code do something over and over."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html",
    "href": "theme1/PE100/PE100-04Repetition.html",
    "title": "PE100-04: Repetition",
    "section": "",
    "text": "We started off learning Python with just simple lists of statements…\ntemperature = 100\nprint(\"It's\", temperature, \"celsius\")\nfaren_t = temperature * (9/5) + 32\nprint(\"or\", faren_t, \"in pagan units.\")\n\nIt's 100 celsius\nor 212.0 in pagan units.\nThen we added the if statement so we could control whether or not certain statements would execute or not:\nif temperature &gt;= 100:\n    print(\"Good chance it's boiling.\")\nelif temperature &gt; 3000:\n    print(\"Odds are it's plasma by now.\")\nelse:\n    print(\"You could probably run the experiment and it might even work.\")\n\nGood chance it's boiling.\nIn both of these cases, the code blocks only run once.\nThe problem is, sometimes we need things to run repeatedly. We want to look up all of the readings from an experiment or we need to compute the properties of something over dozens of temperatures each at dozens of pressures.\nPython gives us two different ways to make our programs repeat things in a loop.\nLet’s start with the while loop."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#while-loops",
    "href": "theme1/PE100/PE100-04Repetition.html#while-loops",
    "title": "PE100-04: Repetition",
    "section": "While Loops",
    "text": "While Loops\nThe syntax of a while loop looks a bit like an if statement. Take a look:\n\ninstrument = 1\nwhile instrument &lt;= 2:\n   print(\"Looking at instrument number\", instrument)\n   print(\"and then maybe we'll look at the next one.\")\n   instrument = instrument + 1\nprint(\"Done with all that looping.\")\nprint(\"...and ready to do something else now.\")\n\nLooking at instrument number 1\nand then maybe we'll look at the next one.\nLooking at instrument number 2\nand then maybe we'll look at the next one.\nDone with all that looping.\n...and ready to do something else now.\n\n\nHere’s what the above code does. First, it creates a variable named “instrument” and sets it to 1. Then it goes into the while loop. The first time through, it checks to see if instrument is less than or equal to 2. It is (because we set it to 1 just a moment ago) so the while loop will execute the code block. This block prints out two lines and then it adds 1 to instrument. That means instrument now equals 2.\nThe second time through the loop, instrument equals 2. That satisfies the conditional statement of the while loop (2 is less than or equal to 2) so the code block runs again. Two more lines are printed out and then instrument is incremented one more time.\nThe while loop runs for a third time now. This time, 3 is not less than or equal to 2, so the conditional statement is false. This means the while loop is done - it won’t run its code block again, and the flow of control will go on to the next line after the while loop. It will run the two print statements explaining that the looping is over and it can go on to other tasks.\nLet’s look at another example. Let’s print out all the powers of two that are less than 928.\n\npower=0\ntwo_to_the_power = 2**power\n\nwhile two_to_the_power &lt; 928:  # Totally not Porsche related.\n    print(\"2 to the\", power,\"equals\",two_to_the_power)\n    power = power+1\n    two_to_the_power = 2**power\nprint(\"2 to the\", power,\"is too big.\")\n\n2 to the 0 equals 1\n2 to the 1 equals 2\n2 to the 2 equals 4\n2 to the 3 equals 8\n2 to the 4 equals 16\n2 to the 5 equals 32\n2 to the 6 equals 64\n2 to the 7 equals 128\n2 to the 8 equals 256\n2 to the 9 equals 512\n2 to the 10 is too big.\n\n\nDid you notice I sneaked something in there we haven’t talked about yet? See the “#” character on the line with the while statement? That indicates the rest of the line is a comment. Python will totally ignore it. It’s handy for leaving little notes to yourself, like “why did I choose 928 there when I could have put 944?” This is very, very important when writing full-fledged, standalone programs. If you don’t leave some notes for yourself, you’ll never remember what you were thinking when you go back to that code six months from now. Also, the next person who comes along and has to change something in your code will greatly appreciate the hints.\nLeaving comments in the code isn’t as big a deal in Jupyter notebooks… you can write rather substantial notes in a Markdown cell complete with boldface, italics, and whatever other fanciness you desire. On the other hand, it’s also nice to be able to leave your comments in the just the right place in the code so it flows effortlessly through your comprehension as you read it. Let experience and personal opinion be your guide here."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#reading-information-from-the-outside-world",
    "href": "theme1/PE100/PE100-04Repetition.html#reading-information-from-the-outside-world",
    "title": "PE100-04: Repetition",
    "section": "Reading information from the outside world",
    "text": "Reading information from the outside world\nNotice that in both of those cases, we actually did know how many times the loop would run. We know that 2 to the 9th is 512 and so we know the while loop will only run that far. In fact, in every example we’ve had so far we’ve know what the output will be because we always have the same inputs. Computer software wouldn’t be terribly interesting if it could only run specific, known, canned inputs. Fortunately, Python gives us several ways to bring data into our programs.\nThe simplest way to bring data into a Python program is to edit the program and change the values we assign to variables. This is sort of the reducto ad absurdum method, but honestly it isn’t a bad way to handle very small amounts of input. It’s even easier in Jupyter notebooks since the code is just sitting there looking at us, waiting to be edited. For values that aren’t going to change very often (your name, perhaps, or the chargeback account number for using some instrument, for instance) then just assigning a value to a variable and editing it every once and a while is a fine way to go.\nAnother way to get data into a Python program is to read it in from where the user is running the program. For doing this, Python provides a function called “input” which takes an optional argument, specifically a string that is printed as a prompt. Python then waits for the user to type something as a response. When they do, that string is returned to the calling program. Here’s a simple example:\n\nyour_name = input(\"Please enter your name\")\nprint(\"Hello,\", your_name)\n\nPlease enter your name Erik\n\n\nHello, Erik\n\n\nWhen the above code runs, the prompt “Please enter your name” is displayed right below the code cell and a text entry box is placed beside it. When you enter your name, it greets you.\nIf we were running this tiny little snippet of code as a regular program, the interaction would be in the terminal emulator window that we ran the program in. Because this is running in Jupyter, though, the interaction is directly in the notebook. The prompt and the entry blank occur just below the running code cell.\nWhat will happen when we run the following?\n\nresponse = input(\"Enter a number between 4 and 8\")\nnew_value = response + 6.5\nprint(new_value)\n\nEnter a number between 4 and 8 5.25\n\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In [4], line 2\n      1 response = input(\"Enter a number between 4 and 8\")\n----&gt; 2 new_value = response + 6.5\n      3 print(new_value)\n\nTypeError: can only concatenate str (not \"float\") to str\n\n\n\nWow! Python couldn’t run that and it “threw an error”. We’ll examine Python’s error handling facilities later, but for now we’ll just assume that means it came to a screeching halt. Looking at the error message, it seems there is some problem with trying to add a real number (a floting point number) to a string."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#type-casting",
    "href": "theme1/PE100/PE100-04Repetition.html#type-casting",
    "title": "PE100-04: Repetition",
    "section": "Type Casting",
    "text": "Type Casting\ninput() prompts the user and returns the string they entered, but what if we want the user to enter a number? What do we do then? The answer is we’ll use a process known as type casting. The act of type casting is no more than converting information from one type to another.\nThere are three very useful functions for type casting: int(), float(), and str(). Let’s see them in action…\n\n# three conversions:\n\n# first, string to float:\nthe_sum = 6.554 + float(\"9.9\")\nprint(the_sum)\n\n# next, string to integer:\nthe_integer_number = int(\"34543456\")\nprint(the_integer_number)\n\n#finally, integer to string:\nhandy_string = str(2+2)\nprint(handy_string)\n\n16.454\n34543456\n4\n\n\nWhat did the above do? First, it converted the string “9.9” (literally, three characters… it’s a string) to a “float” (a floating point number, some languages will call that a real number). The second example takes a string of 8 characters and interprets them as an integer. That value is what gets returned and stored in our variable. Finally, we copmute the number 4 by adding 2+2, and then we let the str() function convert that to a single character long string having just the character “4”.\nBy now we know enough to be able to ask the user for a number and get something back that we can actually do math with.\n\nuser_response = input(\"Enter a number between 17 and 34\")\nselected_number = float(user_response)\nselected_number\n\nThere’s an even easier way, though. Just like function composition worked when you took precalculus, the results of a Python function can be used as the argument to another. Hence:\n\nselected_number = float(input(\"Enter a number between 17 and 34\"))\nselected_number\n\nEnter a number between 17 and 34 26\n\n\n26.0\n\n\nSometimes, function calls can be nested really deeply. Personally, when it comes time to debug code like that I find myself printing it out and coloring each level with a different highlighter pen."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#putting-it-together-while-loops-to-get-user-input",
    "href": "theme1/PE100/PE100-04Repetition.html#putting-it-together-while-loops-to-get-user-input",
    "title": "PE100-04: Repetition",
    "section": "Putting it together: while loops to get user input",
    "text": "Putting it together: while loops to get user input\nThe great thing about a while statement is that it can loop zero times, one, two, or twelve trillion. Best of all, we don’t have to know how many ahead of time. We could do the following:\nprint(“Computing an average.”)\nsum=0.0 counter=0 data_point = float(input(“Enter a number, or enter negative num to stop”)) while data_point &gt;= 0.0: sum = sum+data_point counter = counter+1 data_point = float(input(“Enter a number, or enter negative num to stop”))\nprint(“Average value is”, sum/counter)\nWhen we run the code above, we’re prompted to keep entering numbers until we finally enter -999. Each time it goes through the loop it keeps track of the running total of the numbers and the count of how many numbers have been entered. Once it’s done, it divides the total by the count and displays that as the average.\nLet’s step through what happens when the user enters 1, 2, 3, and -999: 1. The sum and counter variables are initialized to zero. 1. The user is prompted to enter a number, possibly a negative number to indicate no more data, and that input is type cast to a floating point number. 1. The while loop’s condition will be met any time a positive number was input (greater than or equal to zero). 1 is a positive number, so run the loop body. 1. This first time through, we’ll add the 1 that was input to our running total, which is now 1. 1. And increment the count, now equal to 1. 1. AND PROMPT THE USER FOR ANOTHER NUMBER!!! 1. Back at the while statement again, we check the condition and, yes, 2 is a positive number, so we run the loop’s code block. 1. Update the sum and count, and then… 1. PROMPT THE USER FOR ANOTHER VALUE!!! 1. Running the while statement again, the user entered 3, and 3 is positive, so the clode block will be executed. 1. Update the sum (now 6) and count (now 3). 1. Prompt for another number 1. Back at the while statement, we check and see that -999 is not a positive number, so we skip the code block and resume by running whatever follows it. 1. Having exited the while loop entirely, print out the average value by dividing sum/count.\nAll the boldface and all-capitals lines above are there to emphasize how important it is to make sure your while loop isn’t just checking the same thing over and over. If we didn’t get a new number from the user each time through, the value of data_point would never change. That would result in an infinite loop, causing Python to never be able to complete the code in that cell. If it ever happens to you, and it probably will, the “Interrupt Kernel” command on JupyterLab’s Kernel menu will stop the looping and let you get back to work.\nThe while loop is certainly versatile… it can be used any time you need to do something repeatedly. If you know how many times you need to have the code block execute, either when you write the code or when it’s running, then keep a variable that is incremented in the block every time and exit the while loop when the counter hits the right number.\nWhere while loops really shine is when it’s impossible to know ahead of time how many times the code block should run. The example above, where we keep accepting numbers until the user signals there aren’t any more, there’s no way to know how many times to execute that loop until we see a negative number. In a case like that, the while loop is the only practical solution.\nSo if while loops are so great and solve every problem, why do we need anything else? The big reason is expressiveness: they can be a little awkward to understand, especially when you’re looking at someone else’s code. Having the conditional test separated from the action that establishes when to stop makes it a little awkward to understand (or debug!) someone else’s code. This is especially true when we need to step through something by unusual increments.\nSo what are we to do in these cases?"
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#for-loops",
    "href": "theme1/PE100/PE100-04Repetition.html#for-loops",
    "title": "PE100-04: Repetition",
    "section": "For Loops",
    "text": "For Loops\nThe for loop is quite similar to the while loop. The difference is that for loops are controlled by a count whereas while loops are controlled by a condition.\nLet’s start with an example.\n\nfor the_value in range(1,4):\n    print(the_value)\n\n1\n2\n3\n\n\nThat is the simplest for loop you’ll see. Let’s look at the pieces. 1. The for statement itself 2. The name of the target variable whose value will be changing as the loop runs (“the_value” in this case”) 3. “in” - and if this reminds you of set membership then you’re on to something 4. “range()” - this is an example of an iterable, which means “something that can be stepped through”. 5. The colon… the one I forget 50% of the time. 6. The code block, in this case just a print statement.\nMost of the time, fairly close to “always”, the code block will take advantage of the target variable changing each time through. In our example, “the_value” is our target variable, as it loops through it will take on the values 1 through 3, and the code block has a print statement that uses it.\nBefore we examine the range() function, let’s take a look at another iterable. We’ll talk about lists in a later lesson, but for now we can just wave our hands around and understand enough for the moment.\n\nfor sample_weight in [123.6, 121.9, 119.4, 124.23219]:\n    print(\"The sample weighed\", sample_weight, \"grams.\")\n    if sample_weight &lt; 120:\n        print(\"Be careful! This sample might not be all you hoped for.\")\n        \n\nThe sample weighed 143.6 grams.\nThe sample weighed 141.9 grams.\nThe sample weighed 139.4 grams.\nThe sample weighed 144.23219 grams.\n\n\nYou can use the target variable as many times as you want to in the code block.\nNow let’s take a more detailed look at the range() function. In its most basic form it takes one argument - the stop value.\n\nfor i in range(4):\n    print(i)\n\n0\n1\n2\n3\n\n\nThis single-argument form starts at zero, counts up by one each time, and doesn’t include the stop value. This is different from every other programming language you’ll ever encounter. It’s just one of those things.\nWe’ve already seen the two-argument form. It takes a starting value and a stopping value, and iterates by one from the start until the last value that is less than the stop.\n\nfor i in range(7,10):\n    print(i)\n\n7\n8\n9\n\n\nAnd there’s even a three-argument form. The third argument is the amount to step by.\n\nfor i in range (12,20,3):\n    print(i)\n\n12\n15\n18\n\n\nThe step size doesn’t have to be a positive number…\n\nfor i in range(6, -3, -2):\n    print(i)\n\n6\n4\n2\n0\n-2\n\n\nIn case you’re curious, the step size cannot be zero. If you really want an infinite loop, and there are cases where it makes sense, you have to use a while loop instead.\nAs a general rule, any place where you can use an explicit value (a literal) you can use a variable. Arguments to a for loop are no exception:\n\nstart_value = int(input(\"where should we start? \"))\nend_value   = int(input(\"where should we run right up to and stop just short of it? \"))\nstep_size   = int(input(\"what should we step by? \"))\n\nfor i in range(start_value, end_value, step_size):\n    print(i)\n    \n\nwhere should we start?  13\nwhere should we run right up to and stop just short of it?  15\nwhat should we step by?  2\n\n\n13\n\n\nIf we need to do something a specific number of times, we need to pay attention to our starting and stopping conditions. I’ve messed this up so many times I know now to be careful. You’ve been warned.\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(1, how_many):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\n\n\nThey add up to 9\n\n\nNotice something wrong? If you ask it to total 3 numbers, it only prompts for two of them. There are a couple of ways to solve this. The easiest is to just use the one-argument form of range().\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(how_many):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nThat offers a little insight into why Python has it’s funny “up to but not including” semantics: zero is a perfectly legitimate number and a very natural starting point.\nThe only problem with the single-argument method is that the values that the target variable goes through include zero. This may or may not be a problem if that value is used inside the code block. If you really need to count from one instead of zero, you can increment the stopping value:\n\nhow_many = int(input(\"How many numbers would you like to total up? \"))\nsum = 0\nfor i in range(1, how_many + 1):\n    sum = sum + int(input(\"Enter a number \"))\nprint(\"They add up to\", sum)\n\nHow many numbers would you like to total up?  3\nEnter a number  4\nEnter a number  5\nEnter a number  6\n\n\nThey add up to 15\n\n\nAnd that behaved just like we expected.\nYou may have noticed a pattern already. We frequently need to compute a new value for an existing variable. What we’ve done so far has been along the lines of grand_total = grand_total + new_reading. Python gives us a shorthand way to write that. We could instead express that as grand_total += new_reading. There is no space between the plus and equals signs. The only reason this exists is to save you some typing. As you might expect, there are a few more of these Augmented Assignment Operators…\n\n\n\nOperator\nExample\nEquivalent\n\n\n\n\n+=\ncount += 1\ncount = count + 1\n\n\n-=\nx -= offset\nx = x - offset\n\n\n*=\nproduct *= val\nproduct = product * val\n\n\n/=\ny /= 3\ny = y / 3\n\n\n%=\nval %= 2\nval = val % 2\n\n\n\nOut of all of them, += is far and away the most commonly used one."
  },
  {
    "objectID": "theme1/PE100/PE100-04Repetition.html#nested-loops",
    "href": "theme1/PE100/PE100-04Repetition.html#nested-loops",
    "title": "PE100-04: Repetition",
    "section": "Nested Loops",
    "text": "Nested Loops\nYou know what’s fun to put in a loop’s code block? Another loop! Best of all, it comes in pretty handy when dealing with high-dimensional data. Plenty of algorithms rely on nested loops, too. Take a look at this:\n\nfor x in range(5):\n    for y in range(4):\n        print(\"x=\",x,\" y=\", y)\n\nx= 0  y= 0\nx= 0  y= 1\nx= 0  y= 2\nx= 0  y= 3\nx= 1  y= 0\nx= 1  y= 1\nx= 1  y= 2\nx= 1  y= 3\nx= 2  y= 0\nx= 2  y= 1\nx= 2  y= 2\nx= 2  y= 3\nx= 3  y= 0\nx= 3  y= 1\nx= 3  y= 2\nx= 3  y= 3\nx= 4  y= 0\nx= 4  y= 1\nx= 4  y= 2\nx= 4  y= 3\n\n\nWhat’s going on here? Initially, the outer loop, the one that iterates zero through four and assigns it’s value to x, runs. When it starts running its code block for the x=0 pass, the for loop for the y variable starts. ‘y’ assumes the values 0 through 3, so the first four lines printed out are for x=0, y=0, then x=0, y=1, and so on through x=0, y=3. Once that inner for loop completes, the outer for loop gets to iterate again. Now the inside for loop runs again, only this time we have x=1. That’s why the next four lines are “x=1, y=0” through “x=1, y=3”. Every time the outer loop runs another iteration, the inner loop gets to run all the way from start to finish.\nIn later lessons, we’ll have a few opportunities to play with nested loops. In fact, we’ll get to do that in the very next lesson: Functions!"
  },
  {
    "objectID": "theme1/PE100/PE100-07Exceptions.html",
    "href": "theme1/PE100/PE100-07Exceptions.html",
    "title": "PE100-06: Exceptions",
    "section": "",
    "text": "Most of the time, the code we write does exactly what we expect. Our numbers are added up, files are written and read, and users type their input in neat little boxes. Sometimes, though, something goes wrong. Maybe the disk storage space filled up, or we try to write to a file in a directory we don’t have access to (or maybe the directory doesn’t even exist). When things like this happen, the Python interpreter stops the normal flow of execution.\nTake a look at an exception:\n\nfunny_number = 1/0\n\n\n---------------------------------------------------------------------------\nZeroDivisionError                         Traceback (most recent call last)\nCell In[1], line 1\n----&gt; 1 funny_number = 1/0\n\nZeroDivisionError: division by zero\n\n\n\nWhen you run the above, Python will notice the error, stop the code from running, and point out that a “ZeroDivisionError” has occurred. Since this kind of thing wasn’t supposed to happen (division by zero is considered a Bad Thing(tm) by most people) we can say the situation we’re faced with is an exception. And indeed, Python’s error handling mechanisms are based on what are called “exceptions”.\nWhen Python saw the “division by zero” error, it stopped running the rest of the code. It created one of these Exceptions, and then it threw it. Nothing in our one-line example tried to do anything about that exception, so Python just let the program crash and it printed the helpful error messages for us.\nMost of the time, we want our code to be able to handle exceptions when they arrise. We want something that can catch these exceptions when they’re thrown. For that, we need to use Python’s try statement.\n\nTry, try again\n\ntry is how we safely wrap up a bit of code so that if something in there fails and an exception is thrown, we have a way to catch it. For example:\n\ntry:\n    denominator = int(input('Please enter the denominator'))\n    funny_number = 1000/denominator\n    print('the result was', funny_number)\nexcept ZeroDivisionError:\n    print('Looks like someone tried to divide by zero.')\nprint(\"Either we were able to do the division or else we successfully handled an exception.\")\n\n    \n\nPlease enter the denominator 0\n\n\nLooks like someone tried to divide by zero.\nEither we were able to do the division or else we successfully handled an exception.\n\n\nTry running the code above a few times. In the input area, try some different numbers each time. Maybe 4, 0, and -2. Notice that division by non-zero numbers works as expected. Notice also that division by zero now lets us print out an error message instead of crashing. Once we’re done handling the exception, the program resumes with the first line after the try/except structure.\nIn fact, there might be several except clauses if there are several kinds of exceptions that might be thrown. For example, let’s figure out how to share a pizza.\n\ntry:\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept ValueError:\n    print(\"The number of people must be a valid integer.\")\nexcept ZeroDivisionError:\n    print(\"Seriously? There are zero people sharing a pizza?\")\n\nprint(\"Whatever happened up there, this is the first line of code after\")\nprint(\"the try/except structure.\")\n\nHow many people: 0\n\n\nSeriously? There are zero people sharing a pizza?\nWhatever happened up there, this is the first line of code after\nthe try/except structure.\n\n\nAs you try different numbers of people, you can see that division by zero is, of course, handled. You can also enter things that aren’t integers. In response to the prompt, you could enter “Fred”. That can’t be converted to an integer, so the int() function throws an error. The except ValueError clause catches that exception and prints out a message.\nNotice that after either exception handler executes its code, the flow of control goes down to the next line after the try/except structure. In this case, that line is one that prints out a message saying it’s the first line of code after the try and all of the excepts.\nSometimes it’s hard to predict what exception might be thrown in a section of code. In that case, we can use just except: without any exception type. This serves as a “catch-all” handler.\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept ValueError:\n    print(\"The number of people must be a valid integer.\")\nexcept ZeroDivisionError:\n    print(\"Seriously? There are zero people sharing a pizza?\")\nexcept:\n    print(\"The catch-all handler has been awoken from its slumber.\")\n    print(\"I don't know what went wrong, except I can tell you it\")\n    print(\"wasn't a ValueError or a ZeroDivisionError, because\")\n    print(\"those would have been caught by more specific handlers\")\n    print(\"further up the list.\")\n\nThe catch-all handler has been awoken from its slumber.\nI don't know what went wrong, except I can tell you it\nwasn't a ValueError or a ZeroDivisionError, because\nthose would have been caught by more specific handlers\nfurther up the list.\n\n\nIndeed, if we’re lazy (or in a hurry) then we can get by with just a plain except clause and let the user figure it out later:\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept:\n    print(\"There was some sort of problem. I have no idea what.\")\n\nThere was some sort of problem. I have no idea what.\n\n\nUsing just a plain catch-all exception handler doesn’t give you much to work with, but it is slightly better than nothing. Your code won’t crash outright but you won’t much information about what went wrong. If only there was a way to examine that exception, to peer in and divine its secret nature…\nYep. Here you go…\n\ntry:\n    my_file = open('/tmp/ThisFileIsUnlikelyToExist', 'r')\n    people = int(input('How many people:'))\n    slices = 8/people\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nexcept Exception as err:\n    print(\"Error:\", err)\n\nError: [Errno 2] No such file or directory: '/tmp/ThisFileIsUnlikelyToExist'\n\n\nWhat we’ve done is catch any kind of exception (except Exception) and assigned it to a variable named “err”. Then we can print out err. We could even convert err to a string and search for the interesting parts (like the filename of our missing file) and do some clever error handling based on what specifically went wrong.\nPython has a few more tricks when it comes to exception handling, and these can be handy for making your code more readable.\n\n\nFancy exception handling\n\nA try/except structure can have an else clause. This clause will only be executed if no exception was thrown.\n\ntry:\n    people = int(input('How many people:'))\n    slices = 8/people\nexcept Exception as err:\n    print(\"Error:\", err)\nelse:\n    print('Each gets', format(slices, '.2f'), 'pieces.')\n\nHow many people: 0\n\n\nError: division by zero\n\n\nIf the user enters something that can be converted to an integer and is non-zero, then the program continues, finishing up the try block and executing the else block. On the other hand, if an exception of any type is thrown then the “number of pieces” message will never be printed.\nThere is also a finally clause. This one will run after everything else has happened, no matter what.\n\ntry:\n    output_file = open(\"/tmp/output\", \"w\")\n    people = int(input('How many people:'))\n    slices = 8/people\nexcept Exception as err:\n    print(\"Error:\", err)\nelse:\n    print('Each gets', format(slices, '.2f'), 'pieces.')\nfinally:\n    output_file.close()\n\nHow many people: 0\n\n\nError: division by zero\n\n\nIn the try clause, a file opening was added. In the finally clause, the file will be closed whether an exception was thrown or not.\nHow useful are else and finally clauses? It’s true they’re not absolutely necessary. Most programming languages don’t have anything like that. You can always juggle your code around and get by with just try and except. On the other hand, these two clauses can make your code easier to read and understand. Your precise intention can be discerned.\nWe’ve seen how to write Python code that catches errors without crashing. This technique works in both regular Python programs and in Jupyter Notebooks. Next up, we’ll turn back to ways of storing information. This time we’ll look at lists."
  },
  {
    "objectID": "theme1/PE100/PE100-09Strings.html",
    "href": "theme1/PE100/PE100-09Strings.html",
    "title": "PE100-09: Strings",
    "section": "",
    "text": "We’ve been using strings in each of the previous modules, but we’ve accepted them as a found artifact without really getting into them and seeing how they. This module will correct that deficiency and make all of better peopleprogrammers.\n\nReview time\nLet’s take a quick look at what we’ve done so far.\nString literals:   \"Doug McKenzie\"\nString variables:  comedic_genius = \"Mel Brooks\"\n\nComparison:        if my_name == your_name:\n                   if your_name != \"Chuck Woolery\":\nConcatenation:     full_name = first_name+last_name\nRepetition:        \"ABC\" * 20\nThere is a lot more we can do with strings. We can: * Index into them * Iterate over them * Slice them * Search them * Call methods that act on them…\nIn fact, when you look at what you can do with a string and how you do it, you suddenly realize that a string is just (conceptually) a tuple of letters.\nWe can index into a string:\n\nmy_name=\"John Belushi\"\nprint(my_name[0])\nprint(my_name[len(my_name)-1])\nprint(my_name[-1])\n\nJ\ni\ni\n\n\nStrings are iterables:\n\nfor char in my_name:\n    print(char)\n\nJ\no\nh\nn\n \nB\ne\nl\nu\ns\nh\ni\n\n\nJust like a tuple, the elements of a string are immutable. Once a string is created, the characters can’t be changed.\n\nmy_name=\"Bob\"\nmy_name[0]='R'\n\n\n---------------------------------------------------------------------------\nTypeError                                 Traceback (most recent call last)\nCell In[4], line 2\n      1 my_name=\"Bob\"\n----&gt; 2 my_name[0]='R'\n\nTypeError: 'str' object does not support item assignment\n\n\n\nWe can slice strings:\n\nmy_name=\"Michael Jordan\"\nprint(my_name[2:4])\nprint(my_name[4:])\nprint(my_name[:2])\n\nch\nael Jordan\nMi\n\n\nWe can search into a string with the in operator:\n\ntweet_msg = \"I think synchrotrons are cool.\"\nif \"synchrotrons\" in tweet_msg:\n    print(\"found one!\")\n\nfound one!\n\n\nThere are a huge variety of string methods. We’ll look at just a few here. Some of them are handy for validating inputs…\n\nbolts = input(\"How many bolts did you install?\")\nif not bolts.isdigit():\n    print(\"I was expecting something that looked like an integer\")\nelse:\n    fasteners = int(bolts)\n\nHow many bolts did you install? op\n\n\nI was expecting something that looked like an integer"
  },
  {
    "objectID": "theme1/PE100/PE100-08Lists.html",
    "href": "theme1/PE100/PE100-08Lists.html",
    "title": "PE100-08: Lists",
    "section": "",
    "text": "All of the variables we’ve seen so far store exactly one value. If you set the variable “weight” to 74.5, then 74.5 is the only value there is in “weight”. Nice and simple. If we need to save several values then we can use several variables…\n\nweight_1 = 74.5\nweight_2 = 76.7\nweight_3 = 77.1\n\nAs you can imagine, this turns tedious in a hurry. What if you had a thousand values to deal with? And even if you did all of that typing, doing any kind of non-trivial computation with it would be difficult, too. We need a way to store a bunch of values, but doing it in a way that makes it easy to manipulate the whole thing as a whole or each individual value. For doing that, Python provides us with lists.\nPython is one of the few languages that support lists deep down in the language itself. Because of that, they’re easy to work with. Let’s take a look, shall we?\n\nnames = ['Alice', 'Bob', 'Candice', 'Dan']\nnames\n\n['Alice', 'Bob', 'Candice', 'Dan']\n\n\nLists are represented with square brackets [ ] at the beginning and end, and with the values inside the brackets separated by commas.\n\nodd_numbers = [1, 3, 5, 7, 9]\ningredients = ['flour', 'lard', 'baking powder', 'milk']\n\nThe values in a list don’t all have to be the same type.\n\nplaying_card = [9, 'Diamonds']\n\nA list can have any number of values, limited only by the amount of memory in the computer that is hosting the Jupyter (or JupyterLab) server. Lists are even allowed to have no values in them.\n\nempty_list = []\n\nSo far we’ve been creating lists using literal values, but we could use variables just as easily…\n\nnimh = 16\nlithiumPrimary = 2\ncarbonZinc = 6\n\nbattery_inventory = [nimh, lithiumPrimary, carbonZinc]\nprint(battery_inventory)\n\n[16, 2, 6]\n\n\nTo find out how many elements are in a list, use the len() function:\n\nnumber_of_ingredients = len(ingredients)\nprint(number_of_ingredients)\n\nprint(len(battery_inventory))\n\n4\n3\n\n\nThere are operators that act on lists. The * operator is used for repetition…\n\nmy_list = [1, 2, 3] * 2\nprint(my_list)\nmany_zeros = [0] * 25\nprint(many_zeros)\n\n[1, 2, 3, 1, 2, 3]\n[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\n…and the + operator combines two lists:\n\nbig_list = my_list + many_zeros\nprint(big_list)\n\n[1, 2, 3, 1, 2, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n\n\nLists are iterables, just like the results of the range() function, so they can be iterated over using a for loop:\n\nfor name in ['David', 'Bill', 'Richard']:\n    print(name)\n\nDavid\nBill\nRichard\n\n\n\nfor ingr in ingredients * 3:\n    print(ingr)\n\nflour\nlard\nbaking powder\nmilk\nflour\nlard\nbaking powder\nmilk\nflour\nlard\nbaking powder\nmilk\n\n\nIt’s fairly common to iterate over a list for things like sums and averages.\n\ntotal = 0\nfor item in [4, 3, 4, 5]:\n    total += item\nprint(total)\n\n16\n\n\nThe above code steps its way over all of the values in the list. Each time it goes to a new value, it adds that value to total. When it gets to the end, all of the values have been added up. If we want an average, we don’t have to count the values ourselves. We can just use the len() function.\n\ntotal = 0\nmy_list = [4, 3, 4, 5]\nfor item in my_list:\n    total += item\navg = total / len(my_list)\nprint(avg)\n\n4.0\n\n\nSometimes you need to use a particular value in a list and you don’t want to iterate over the whole thing. For this, Python gives us indexing, letting us directly access any element of a list. The first (as it appears on screen, “leftmost”) element in numbered zero and each one after that goes up by one. The highest numbered one is therefore the length of the list minus one.\n\nprint(ingredients)\nprint()\nprint(\"Of all the biscuit ingredients,\", ingredients[0] , \"is the most important one.\")\nprint(\"The second most important one is\", ingredients[2])\n\n['flour', 'lard', 'baking powder', 'milk']\n\nOf all the biscuit ingredients, flour is the most important one.\nThe second most important one is baking powder\n\n\nLike the majority of programming languages, Python uses square brackets to indicate the index into the list. Unlike the vast majority of languages, Python allows indexes to be negative! A negative number for an index means “count backwards from the end”. my_list[-1] refers to the item at the end of the list. my_list[-3] refers to the third to last item.\n\nprint(ingredients[-1])\nprint(ingredients[-3])\n\nmilk\nlard\n\n\nWe’ve seen how to iterate over lists and also how to access individual list elements by using indexing. Python has a special indexing scheme, though, that lets us deal with small lists made from our original list. This is called List Slicing and can save you a lot of work sometimes. The overall syntax for this looks like list_name[start:end]\nAn example is definitely called for here:\n\nmy_list = [2, 4, 6, 8, 10, 12]\nprint(my_list[1:3])\n\n[4, 6]\n\n\nRemember that list indexes count from zero, and remember also that ranges in Python include the starting index (here, it’s the 1) and will continue to the last value that is smaller than the one on the right side of the colon.\nBoth the starting and the ending indexes are optional! If one of the two is missing, it will be interpreted as 0 or the list’s length, respectively.\n\nprint(my_list)\nprint()\nprint(my_list[:3])\nprint(my_list[1:])\nprint(my_list[:])\n\n[2, 4, 6, 8, 10, 12]\n\n[2, 4, 6]\n[4, 6, 8, 10, 12]\n[2, 4, 6, 8, 10, 12]\n\n\nAnd finally, the in operator is used to test list membership.\n\nlucky_numbers = [2, 7, 17, 9]\nplayer_number = int(input('Enter your favorite number'))\nif player_number in lucky_numbers:\n    print(\"Your favorite number is lucky!\")\nelse:\n    print(\"Sorry! Better luck next time!\")\n\nEnter your favorite number 8\n\n\nSorry! Better luck next time!\n\n\n\nThere’s Method to the Madness\n\nThere are two kinds of functions available for working with lists. Built-in functions are the ones that are part of Python itself. Methods, as you’ll recall from the unit on files, are special functions that are situated inside of objects and only usable with that kind of object. Python lists are objects. They’re iterable objects, in fact.\nLet’s take a look at a few of the methods available for working with lists. First up is append().\n\nprint(lucky_numbers)\nlucky_numbers.append(106)\n\nlucky_numbers\n\n[2, 7, 17, 9]\n\n\n[2, 7, 17, 9, 106]\n\n\nJust as the name implies, append() adds an element to the end of a list.\nBut what if we want to put a new element in a specific place? For that, there is insert().\n\nprint(lucky_numbers)\nlucky_numbers.insert(2, 202)\nlucky_numbers\n\n[2, 7, 17, 9, 106]\n\n\n[2, 7, 202, 17, 9, 106]\n\n\nThe insert function takes two arguments. The first is the position in the list where the insertion should happen. In the example above, it was at position 2. Remember, list indexes start at zero! The second argument is the element to insert. And when we look at the resulting list, we see that 202 is in position 2 now (which is the third position!) and all the other elements have been shifted to the right.\nWe’ve been fetching elements from the list by location number, so far. How do we find something by searching for it? The index() method does that.\n\nwhere_found = lucky_numbers.index(202)\nprint(where_found)\n\n2\n\n\nWe passed the argument 202 to the index method. It searched the list and returned the index of the first occurence. That index is 2. Makes sense because we just inserted it there a minute ago!\nIf we can insert things into a list then surely we can remove them too, right? Indeed we can with the remove() method.\n\nprint(lucky_numbers)\nlucky_numbers.remove(7)\nprint(lucky_numbers)\n\n[2, 7, 202, 17, 9, 106]\n[2, 202, 17, 9, 106]\n\n\nWatch out! remove() looks up an item, like index() does, and then removes it. It doesn’t take a position number. In other words:\n\npeople = ['David', 'Bill', 'Richard']\npeople.remove('Bill')\nprint(people)\n\n['David', 'Richard']\n\n\nYou might find yourself needing to sort the items in a list, and for that the sort() method exists:\n\nprint(lucky_numbers)\nlucky_numbers.sort()\nprint(lucky_numbers)\n\n[2, 202, 17, 9, 106]\n[2, 9, 17, 106, 202]\n\n\nFinally, there are methods to find the greatest and smallest values in a list.\n\nprint(min(lucky_numbers))\nprint(max(lucky_numbers))\n\n2\n202\n\n\nEarlier we saw the use of len() to find out how many items are in a list. This is a built-in function and works on many types of variables, not just lists. There are two more built-in functions that are useful for working with lists: min() and max().\n\nsiblings = ['David', 'Bill', 'Shirley', 'Richard', 'Laverne']\nprint(min(siblings))\nprint(max(siblings))\n\nBill\nShirley\n\n\n\n\nLists and Functions\n\nFunctions have no problem accepting lists as arguments and they can also return lists as the function’s value. There is a subtle “gotcha” when passing lists as an argument, though.\nFirst, let’s look at a simple example:\n\noriginal_list = [1, 2, 3, 9]\n\ndef sum_of_list(list_to_sum):\n    sum = 0\n    for i in list_to_sum:\n        sum = sum + i\n    return sum\n\nthe_sum = sum_of_list(original_list)\nthe_sum\n\n15\n\n\nThat worked as expected - there’s no problem passing lists into functions. What about returning lists from functions?\n\ndef pet_factory(how_many_pairs):\n    pets = ['goldfish', 'catfish'] * how_many_pairs\n    return pets\n\nmany_fish = pet_factory(5)\nprint(many_fish)\n\n['goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish', 'goldfish', 'catfish']\n\n\nEarlier, when we talked about functions in section 5, we said that if a function changes the value of one of its arguments then the effects of that change stay inside the function and aren’t visible to anything when the function exits. That statement was mostly true. If you pass a list as an argument to a function and if that function changes the list then the change made there will be visible outside. Strings, floats, and integers asre protected, but lists are more exposed.\n\noriginal_list = [1, 2, 3, 9]\n\ndef doubler(numbers):\n    for i in range(len(numbers)):\n        numbers[i]=numbers[i]*2\n\nprint(original_list)\ndoubler(original_list)\nprint(original_list)\n\n[1, 2, 3, 9]\n[2, 4, 6, 18]\n\n\nChanging the value of an argument inside of a function usually isn’t a great idea, but in the case of lists it can be useful.\n\n\nNo Funny Glasses Required\n\nThe lists we have worked with up to this point have all been one dimensional. Lists get a lot more interesting as the number of dimensions goes up.\nUnlike most programming languages, Python does not have a multi-dimensional list or array construction, per se. What Python does have is a list that is versatile enough to contain anything - and that includes containing other lists! A two-dimensional list in Python is just a “list of lists”.\nTake a look:\n\nfirst_presidents = [['George', 'Washington'], ['John', 'Adams'], ['Thomas', 'Jefferson']]\n\nAbove, on that very long line, we’ve created a list with square brackets. Inside that list, we’ve put three more lists inside square brackets of their own. So we’ve made a list of lists.\nThat long line is hard to read, isn’t it? Python won’t let us just split a long line of code across multiple lines… unless we explicitly tell it what we’re doing. That is done by ending each line with a backslash and immediately pressing enter. It looks like this:\n\nfirst_presidents = [['George', 'Washington'],\\\n                    ['John', 'Adams'],\\\n                    ['Thomas', 'Jefferson']]\n\nJupyter even goes to the trouble to line up the columns for us.\nAnyway, let’s see what we’ve created.\n\nprint(first_presidents)\n\n[['George', 'Washington'], ['John', 'Adams'], ['Thomas', 'Jefferson']]\n\n\n\nprint(first_presidents[0])\n\n['George', 'Washington']\n\n\n\nprint(first_presidents[2])\n\n['Thomas', 'Jefferson']\n\n\nWe can index into the outer array, the one that contains the smaller lists, just like we normally would. We can also index into the inner array two different ways. The long way…\n\npresident_number_one = first_presidents[0]\nfirst_name = president_number_one[0]\nfirst_name\n\n'George'\n\n\n… or we can take the shortcut:\n\nfirst_name = first_presidents[0][0]\nfirst_name\n\n'George'\n\n\nThe first zero got us to the “George”, “Washington” element, and the second zero indexed into that and gave us ‘George’. Let’s try some other combinations:\n\nnext_first_name = first_presidents[1][0]\nnext_first_name\n\n'John'\n\n\n\nanother_name = first_presidents[1][1]\nanother_name\n\n'Adams'\n\n\nIt’s easy to see how we’re indexing into this two-dimensional list. In fact, it works roughly the same way as a 2-D array in most programming languages.\nIt’s so similar, in fact, that you’re probably feeling the urge to do some Linear Algebra right now.\nDon’t. Not yet.\nPython’s multidimensional list support is exactly that: support for lists. It can be pressed into service for arrays (in the linear algebraic sense of the term) but performance is pretty bad. In Programming Elements 101 we’ll see a software library called “numpy”. It is superior for arrays where you want to do some math.\nNow let’s look at how to traverse multi-dimensional array. We’ll create a 2-D list that look like this:\n        Column 0  Column 1  Column 2  Column3\nRow 0.     A         B         C         D\nRow 1.     E         F         G         H\nRow 2.     I         J         K         L\nRow 3.     M         N         O         P\n\nletter_table = [['A', 'B', 'C', 'D'],\\\n['E', 'F', 'G', 'H'],\\\n['I', 'J', 'K', 'L'],\\\n['M', 'N', 'O', 'P']]\n\nWe can get a whole row:\n\nprint(letter_table[1])\n\n['E', 'F', 'G', 'H']\n\n\nor we can get a specific cell (the order is row, then column):\n\nprint(letter_table[2][1])\n\nJ\n\n\nWe can access the table by column, but it’s not as easy. We’ll have to write a loop that steps down a column and reads the values:\n\nfor i in range(len(letter_table)):\n    print(letter_table[i][3])\n\nD\nH\nL\nP\n\n\nWhat if we want to access all of the cells in the array? For that, nested loops work.\n\nfor row in range(len(letter_table)):\n    for col in range(len(letter_table[row])):\n        print(letter_table[row][col])\n\nA\nB\nC\nD\nE\nF\nG\nH\nI\nJ\nK\nL\nM\nN\nO\nP\n\n\n“But wait!”, I hear you say. “I need to store higher-dimensionality data!” No problem. Python will allow arbitrarily deep nesting. We can have lists of lists of lists (3 dimensions) or lists of lists of lists of lists for four dimensions. Accessing the cells is just a matter of adding more array indexes to the end of the name.\n\nfirst_vice_presidents = [[\"John\", \"Adams\"],\\\n                         [\"Thomas\", \"Jefferson\"],\\\n                         [\"Aaron\", \"Burr\"]]\n\nearly_us_leaders = [first_presidents, first_vice_presidents]\nprint(early_us_leaders[1][0][1])\nprint(early_us_leaders[1][2][0])\n\nAdams\nAaron\n\n\nIt’s easy to get confused with deeply nested lists. Three dimensions isn’t bad, four is managable, but as the structures get deeper and deeper I have to resort to drawing pictures and frequent testing every step of the way.\ntl;dr: If you’re a string theorist working in 21 dimensions or whatever, Python lists probably aren’t the way to go. You should use numpy.\n\n\nTuples\n\nA “double”, mathematically speaking, is two of something. A “triple” is three of them. If you don’t know how many, or you don’t want to specify, then it’s generically called a “tuple” (pronounced “Too pull”, according the The American Heritage Dictionary and, more importantly, everyone who has ever taught the database class).\nPython gracious provides us with tuples. Their syntax is just like a list, only using parentheses instead of square brackets. For instance:\n\nmy_tuple = (2, 8, 256)\nprint(my_tuple[1])\n\n8\n\n\nTuples have some restrictions when compared to lists. * You can’t sort them. * You can’t insert or delete from them * You can’t change the values in them\nWhy would we want tuples if they’re so similar to lists, only somewhat disabled? In a word, “speed”. They’re very fast compared to lists. That’s why some Python functions require them. The most likely time you’ll see tuples is when you’re accessing data from a database. The second most common use is when you need to return multiple values from a function.\nSince tuples have the speed advantage but lists are more versatile, it’s not unusual to see programmers use the list() and tuple() functions to convert between the two types:\n\nmy_tuple = (2, 8, 256)\nlist_version = list(my_tuple)\nlist_version\n\n[2, 8, 256]\n\n\n\nmy_list=[2, 4, 6, 8]\ntuple_version = tuple(my_list)\ntuple_version\n\n(2, 4, 6, 8)\n\n\nReturning multiple values from a function feels like cheating the first time you do it. After all, sin(x) returns exactly one number, right?\nWhat if you wrote a function that returns a complex number, like 1.105+7.3i ? That’s one number (albeit one on the complex plane) but it’s written like two pieces of data being returned.\nWhat if you got really fancy and wrote a function that returned a column vector? That would be like returning a lot of numbers all at once, wouldn’t it?\nSo returning multiple values at once isn’t that bad, is it? Especially if the values all have related meaning and “belong” together.\n\ndef get_extremes(number_list):\n    min_val = min(number_list)\n    max_val = max(number_list)\n    return (min_val, max_val)\n\nnumbers = [5, 3, 2, 7, 2, 5]\nlow, high = get_extremes(numbers)\nprint(low)\nprint(high)\n\n2\n7\n\n\nA couple of things to note. First, notice how the function creates a tuple and returns it. The parentheses indicate a tuple is being constructed and the min_val and max_val variables are put into the tuple as the first and second elements.\nSecond, look at how that tuple is returned to the caller, taken apart, and stored in a pair of variables. You’ll see the syntax first_variable, second_variable, third_variable = func() when a tuple is returned from a function. The first element of the tuple is placed in first_variable and so on.\n\n\nComing Up Next\n\nWe’ve made it to the end of this section. Take a moment, breathe, and relax… this is the longest module in the “Python and Jupyter” series. Next up we have lots of information on strings. We’ve been using strings a lot already without really looking at what they are and what they can do. It’s time to remedy that.\n(pssst. Want a hint? Strings are just tuples of letters!)"
  },
  {
    "objectID": "theme1/PE103/vcs.html",
    "href": "theme1/PE103/vcs.html",
    "title": "Version Control",
    "section": "",
    "text": "Version control systems (or VCS) are software tools that are used to track changes to source code or other collections of files.\nWikipedia has a fairly long list of version control systems, each of them varying in practical usage and in implementation details. These days Git is the most popular version control system in use. Git appears to have “won”, and so Git is what we will be discussing here."
  },
  {
    "objectID": "theme1/PE103/vcs.html#why-do-we-need-version-control",
    "href": "theme1/PE103/vcs.html#why-do-we-need-version-control",
    "title": "Version Control",
    "section": "Why do we need version control?",
    "text": "Why do we need version control?\nWhen you work on a project, you often will want to save the state of your code at various points, such that you can go back and forth between these various points. When you work on a project as part of a team, you will also want to know who wrote some part of the code, when, and why.\nWhether you work alone or in a team, using a version control system will help you achieve the above goals. Sometimes your team member is the past you (who should help you), or the future you (whom you should help).\nIn the absence of a version control system, you will often will end up with a chaotic mess which achieves the above goals in a poorer manner. You will likely resort to several almost-same-but-not-quite files variously named like so:\n\nnotebook.ipynb\nnotebook-2024-05-01.ipynb\nnotebook-2024-05-01-final.ipynb\nnotebook-working.ipynb\nnotebook-test.ipynb\nnotebook-final.ipynb\n\n(Or think of the situation depicted by this PHD Comics strip.)\n\n\nThis scheme is basically a messy reinvention of a version control system. That might work in the simple cases, but it will soon break down as you do more work on your project.\nYou want to avoid the cognitive overload of dealing with messy schemes based on file names. You want to use a version control system properly.\nConceptually, you can think of the evolution of versions over time as a directed graph:\n\nSometimes you branch out, say, when exploring a new direction:\n\nAnd you might eventually want to merge the branch to the “main” body of your graph:\n\nWith a version control system, you can to do these things in a less ad-hoc manner."
  },
  {
    "objectID": "theme1/PE103/vcs.html#version-control-in-practice-git",
    "href": "theme1/PE103/vcs.html#version-control-in-practice-git",
    "title": "Version Control",
    "section": "Version control in practice: Git",
    "text": "Version control in practice: Git\nGit is a command-line program that runs on all popular operating systems. If you use macOS or Linux, you probably have Git installed already. Here we assume that you are using the account that you have with CLASSE.\nLet us prime ourselves for Git with an XKCD strip:\n\n\n\n© Randall Munroe, Creative Commons Attibution-NonCommercial license\n\n\nAs the cartoon suggests, Git has a (perhaps well-deserved) reputation for being rather unfriendly or inscrutable. With some familiarity and practice, it can be tamed.\n\n\n\n\n\n\nNote\n\n\n\nDepending on your background, you might find that learning Git by first understanding the data model more helpful. Version control module of the MIT course “The Missing Semester of Your CS Education” and Git from the Bottom Up by John Wiegley take this route. These notes that you are currently reading, however, take the more traditional path of introducing you to the more frequently used Git commands.\n\n\n\nFirst steps with Git\nYou can start trying out git by running the below in a terminal:\n$ git help\nThis should print some common Git commands used in various situations.\n\n\n\n\n\n\nNote\n\n\n\nThe specific output from git help might vary depending on the version of Git that you are using. You can find the version of Git that you’re using by running git version.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nYou might find git help -g (or git help --guides) useful:\n$ git help -g\nThe common Git guides are:\n\n   attributes   Defining attributes per path\n   glossary     A Git glossary\n   ignore       Specifies intentionally untracked files to ignore\n   modules      Defining submodule properties\n   revisions    Specifying revisions and ranges for Git\n   tutorial     A tutorial introduction to Git (for version 1.5.1 or newer)\n   workflows    An overview of recommended workflows with Git\n\n'git help -a' and 'git help -g' lists available subcommands and some\nconcept guides. See 'git help &lt;command&gt;' or 'git help &lt;concept&gt;'\nto read about a specific subcommand or concept."
  },
  {
    "objectID": "theme1/PE103/vcs.html#getting-started-with-git",
    "href": "theme1/PE103/vcs.html#getting-started-with-git",
    "title": "Version Control",
    "section": "Getting started with Git",
    "text": "Getting started with Git\nGit commands are generally of the form git &lt;subcommand&gt;, where &lt;subcommand&gt; is for the specific operation you want to do. We will discuss them in the following sections.\nThere is also a little bit of configuration that you should do, before you are able to git add and git commit your changes. Let us start with this configuration.\n\nInitial configuration\nGit keeps track of who makes changes. For this to work, you’ll need to configure Git using git config subcommand:\n$ git config --global user.name \"Your Name\"\n$ git config --global user.email \"you@example.com\"\nThis will write configuration to a file named .gitconfig in your home directory.\n$ cat ~/.gitconfig\n[user]\n    name = Your Name\n    email = you@example.com\nOf course, you should use “real” values instead of Your Name and you@example.com.\n\n\nStarting a new repository\nLet us start with a very simple example, just for practice. We will create a brand new Git repository, and commit some changes to it.\nNow, what is a repository, and what are commits?\n\nA Git repository is essentially a directory where Git tracks your files and manages changes to them. It is a database that stores your project’s history. This history includes every version of every file in your project, and who made those changes, and when, among other things.\nA Git commit is a snapshot of your project at some point in time. You can think of them as versions.\n\nOn lnx201, let us create a new directory (with mkdir hello-world), change to that directory (with cd hello-world), create a file in that repository, and initialize a git repository there (with git init):\n$ mkdir hello-world\n$ cd hello-world/\n$ echo \"hello $USER\"\nhello ssasidharan\n$ echo \"hello $USER\" &gt; hello.txt\n$ cat hello.txt\nhello ssasidharan\n$ git init\nInitialized empty Git repository in /home/ssasidharan/hello-world/.git/\nThat created an empty repository, meaning, nothing has been added to it. As you can see, git init created a directory named .git/ inside hello-world/. This .git directory is the repository’s “database” – this is where Git stores information about your project, including version history, configuration settings, and references to commits, and more.\nThe \".\" prefix in .git/ means that it is a “hidden” directory. It won’t appear in the output of commands such as ls. You will need to use ls --all or ls -a.\nRunning git status will show an “untracked file”:\n$ git status\n# On branch master\n#\n# Initial commit\n#\n# Untracked files:\n#   (use \"git add &lt;file&gt;...\" to include in what will be committed)\n#\n#   hello.txt\nnothing added to commit but untracked files present (use \"git add\" to track)\n\n\n\n\n\n\nNote\n\n\n\nWhat does On branch master mean?\nThink of a branch as a line of development. Your work almost always happen on a branch. When you start a project, Git uses a default branch called a master. With newer versions of Git (since , the default branch is called main.\n\n\n\n\n\n\nAn “untracked file” means Git do not know about the file. We will need to add that file to Git.\n\n\nAdding changes\nLet us add hello.txt to the repository, and check the status again:\n$ git add hello.txt\n$ git status\n# On branch master\n#\n# Initial commit\n#\n# Changes to be committed:\n#   (use \"git rm --cached &lt;file&gt;...\" to unstage)\n#\n#   new file:   hello.txt\n#\nThe command git add hello.txt adds the file hello.txt to the repository. You could also have done a git add . to add all files in the directory to the repository.\nNote that git add hello.txt does not commit hello.txt to the repository; it just tells Git to pay attention to the file. With git status, we can see that Git is aware of the fact that there are some changes to be committed in the repository.\n\n\nCommitting changes\nUse git commit to actually commit the tracked file to the repository:\n$ git commit -m \"Add hello.txt\"\n[master (root-commit) 708bfca] Add hello.txt\n 1 file changed, 1 insertion(+)\n create mode 100644 hello.txt\nThe string following -m option (-m is short for --message) is a commit message. You use commit messages to describe the change in a single short line.\nNote that commit messages are not required to be single lines. On lnx201, if you run git commit without -m &lt;message&gt;, an editor will be launched where you can write a more detailed commit message. You can configure a different editor for writing commit messages, if you want to do so. As a general rule, your commit message should start with a short description of the commit in a single line, followed by a blank line, followed by a more detailed description of the commit.\nOver time, your commit messages will tell the story about how your project evolved. More details in commit messages would be quite useful when you or someone else revisit the change at some point in the future.\n\n\n\n© Randall Munroe, Creative Commons Attibution-NonCommercial license\n\n\nNow we can use git status to re-check status of the repository:\n$ git status\n# On branch master\nnothing to commit, working directory clean\nWe can use git log to view the commit history:\n$ git log\ncommit 708bfcafe32528e90e1d52fd6b94f0c44476518a\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:10:02 2024 -0400\n\n    Add hello.txt\nThe commit 708bfcafe32528e90e1d52fd6b94f0c44476518a part is what is known as a “commmit hash” or a “commit ID”. It is a 40-character hexadecimal string generated using a cryptographic hashing algorithm. Each commits get a unique commit hash, and represents the state of the repository at a given point of time.\nLet us add some more changes, and commit them:\n$ echo \"hello from $HOSTNAME\" &gt;&gt; hello.txt\n$ git status\n# On branch master\n# Changes not staged for commit:\n#   (use \"git add &lt;file&gt;...\" to update what will be committed)\n#   (use \"git checkout -- &lt;file&gt;...\" to discard changes in working directory)\n#\n#   modified:   hello.txt\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\n$ git add hello.txt\n$ git commit -m \"Update hello.txt\"\n[master 233c748] Update hello.txt\n 1 file changed, 1 insertion(+)\n$ git status\n# On branch master\nnothing to commit, working directory clean\nTry git log to show commit history again:\n$ git log\ncommit 233c748ad3dd31c11a3bc12d0cf106d7fe888fc3\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:22:29 2024 -0400\n\n    Update hello.txt\n\ncommit 708bfcafe32528e90e1d52fd6b94f0c44476518a\nAuthor: Sajith Sasidharan &lt;ssasidharan@lnx201.classe.cornell.edu&gt;\nDate:   Tue Apr 23 19:10:02 2024 -0400\n\n    Add hello.txt\n\n\nSome more terminology\nFiles that are tracked by Git can be in one of these stages:\n\nModified: you have changed the file, but has not added or committed it.\nStaged: you have done a git add on the file, thus moving into the “staging area” of the repository.\nCommitted: you have done a git commit, thus storing the file in your local Git database.\n\nLet us see what this means with a diagram:\n\nThe working tree is what you get when you initialize repository (with git init) or check out a repository (with git clone). The files you work with are here. When you modify a file, you will need to move it to the staging area before committing it.\nThe staging area stores information about what will go into your next commit. In Git terminology, staging area is also called the index.\nWhen you do git commit, your staged changes will be committed to the database in .git directory. With these two-step actions, you get more control over what set of changes you get to commit to the project.\nWith git commit -a (or git commit --all), you can stage the changes and commit them at once:\n$ echo \"third line\" &gt;&gt; hello.txt\n$ git commit -am \"Add a third line\"\nNote that git commit -am \"Add a third line\" is the same as git commit --all --message \"Add a third line\".\n\n\nReviewing changes\nWe have git status and git log in action already. Another command is git diff, which is used to find the difference between two commits:\n$ git diff 708bfcafe32528e90e1d52fd6b94f0c44476518a 233c748ad3dd31c11a3bc12d0cf106d7fe888fc3\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\nA useful shortcut is git diff HEAD~:\n$ git diff HEAD~\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\nIn Git parlance, HEAD implies the last commit on the current branch, and HEAD~ is the commit before that, and git diff HEAD~ would print the difference between the latest commit and the one before that.\nEventually, once there are more commits in the repository, you can view the difference with an arbitrary number of commits in history with git diff HEAD~~~ (or, more conveniently: git diff HEAD~3), and so on. You get the idea.\nAnother shortcut for those really long commit hashes is using a smaller prefix of them. You can find these “short hashes” with git log --abbrev-commit or git log --oneline:\n$ git log --oneline\n233c748 Update hello.txt\n708bfca Add hello.txt\n$ git diff 708bfca 233c748\ndiff --git a/hello.txt b/hello.txt\nindex c5d1025..3f4c47c 100644\n--- a/hello.txt\n+++ b/hello.txt\n@@ -1 +1,2 @@\n hello ssasidharan\n+hello from lnx201.classe.cornell.edu\n\n\nRemoving files\nTo remove files from your project, use git rm &lt;path&gt; followed by git commit:\n$ git rm hello.txt\n$ git commit -m \"Remove hello.txt\""
  },
  {
    "objectID": "theme1/PE103/vcs.html#undoing-changes",
    "href": "theme1/PE103/vcs.html#undoing-changes",
    "title": "Version Control",
    "section": "Undoing changes",
    "text": "Undoing changes\nYou have made some changes, but you don’t actually want to keep those changes. You do not want to add them or commit them. To discard all such changes in your working directory, use git reset:\n$ git reset --hard HEAD\nIf you want to keep the changes but continue working on them without committing yet, you probably want to do:\n$ git reset --soft HEAD\nOften you can do a git reset &lt;commit&gt; to restore the state of your working tree to what is reflected by &lt;commit&gt;.\nAt any point, you can use git reflog (or “reference log”) to find where you were previously:\n$  git reflog \nddec657 HEAD@{0}: commit: Remove hello.txt\nd73c5c8 HEAD@{1}: commit: Add a third line\n233c748 HEAD@{2}: commit: Update hello.txt\n708bfca HEAD@{3}: commit (initial): Add hello.txt\nThis output is useful when you want to set the state of your working tree using git reset or git checkout.\nOnce you have made a commit, you can undo that with git revert, which will create a new commit which will revert undesired changes:\n$ git revert &lt;commit&gt;\n\nIgnoring (some) files\nSome files just should not be under version control. Examples would be:\n\nAnything generated by a build process, or a compiler, or a test suite, or some such. You do not want to commit the byte-compiled .pyc files, for example.\nSecrets, or files containing secrets (such as passwords, or tokens).\nEditor configuration files.\n\nYou should tell Git to ignore these files by adding their names to a .gitignore file, with one name on a line, so that git status will ignore them. You can use wildcard patterns such as *.pyc.\nOn lnx201, you probably should ignore the .directoryhash files automatically created by the file system; when you use macOS, you want to ignore .DS_Store files. So your .gitignore should have:\n.directoryhash\n.DS_Store\nTake a look at X-CITE course’s .gitignore for another example."
  },
  {
    "objectID": "theme1/PE103/vcs.html#working-with-branches",
    "href": "theme1/PE103/vcs.html#working-with-branches",
    "title": "Version Control",
    "section": "Working with branches",
    "text": "Working with branches\nA branch is a line of development. The default branch for Git is master (or main, for newer versions of Git). If you are working on a small project by yourself, you probably can commit your changes on the default branch.\nHowever, when working on bigger projects or working with others, you will want to use branches. This will ensure that there is less “churn” in the default branch, and keep things manageable.\nYou will create a branch when you implement a feature, for example, and when you are done implementing the feature, you will merge that branch to the default branch.\n\nNothing stops you from branching off from your non-main branches and merging back to the branch either. Branching and merging are (usually!) quick and easy with Git. (Well, except when there are merge conflicts. This happens when the changes on your branches diverge from the changes on the branch that you are trying to merge to.)\n\n\n\nTo find the branch on which you currently are, use:\n$ git branch\nTo create a new branch named feature-branch, use:\n$ git branch feature-branch\nTo switch to the newly created feature-branch, do:\n$ git checkout feature-branch\nYou can also create a branch and check it out in a single command:\n$ git checkout -b feature-branch\n\nMerging branches\nWhen you are ready to merge feature-branch to main branch, you will do:\n$ git checkout main \n$ git merge feature-branch\nIf the merge is not successful, you will encounter an error message:\n$ git merge feature-branch \nAuto-merging test.txt\nCONFLICT (content): Merge conflict in test.txt\nAutomatic merge failed; fix conflicts and then commit the result.\nNow git status would indicate that you have a situation:\n$ git status \n# On branch master\n# You have unmerged paths.\n#   (fix conflicts and run \"git commit\")\n#\n# Unmerged paths:\n#   (use \"git add &lt;file&gt;...\" to mark resolution)\n#\n#   both modified:      test.txt\n#\nno changes added to commit (use \"git add\" and/or \"git commit -a\")\nGit would have added some conflict markers (&lt;&lt;&lt;&lt;&lt;&lt;&lt; and ======= and &gt;&gt;&gt;&gt;&gt;&gt;&gt;) to indicate the places where it had trouble merging.\n\n\ntest.txt\n\n&lt;&lt;&lt;&lt;&lt;&lt;&lt; HEAD\nline 1\nline 2\n=======\nline 3\n&gt;&gt;&gt;&gt;&gt;&gt;&gt; feature-branch\n\nThis is when you open a text editor, find the conflicts, and resolve them manually. It is up to you to accept what you want and reject what you do not. You should do that, and remove the conflict markers. You will probably end up with:\n\n\ntest.txt\n\nline 1\nline 2\nline 3\n\nNow you can commit this with:\n$ git commit -am \"Resolve conflict\"\nIn more complicated situations, you will probably have to use a merge tool. See git-mergetool for details."
  },
  {
    "objectID": "theme1/PE103/vcs.html#working-with-tags",
    "href": "theme1/PE103/vcs.html#working-with-tags",
    "title": "Version Control",
    "section": "Working with tags",
    "text": "Working with tags\nTags are used to mark certain points in a repository’s history as important in some manner. In the case of a release, you will want to tag the commit associated with the release with v1.0.0 or v1.0.1, for example.\nYou can list the tags present in your repository with:\n$ git tag\nYou can create a tag with:\n$ git tag 1.0.0\nYou can have a tag point at a specific commit:\n$ git tag 1.0.1 3de49cc\nTo delete a tag:\n$ git tag -d 1.0.1"
  },
  {
    "objectID": "theme1/PE103/vcs.html#working-with-remote-repositories",
    "href": "theme1/PE103/vcs.html#working-with-remote-repositories",
    "title": "Version Control",
    "section": "Working with remote repositories",
    "text": "Working with remote repositories\nThis far, we’ve talked about how you work in a local repository.\nYou will want to be able to share your work with others, and be able to check out the projects others have created. This is typically done with hosting your repository somewhere in a server. These are called remote repositories.\n\nYou will git clone remote repositories, and you will git fetch or git pull updates them from, and you will git push your changes to them. In practice, you will very likely use an account on a code hosting site such as GitHub, and push your code to a repository there.\n\n\n\n\n\n\nNote\n\n\n\nWhile working with a central remote repository is the most usual Git workflow, it is worth noting that Git enables other kinds of workflows also. See the chapter Distributed Git - Distributed Workflows from Pro Git for some examples.\n\n\nThese very notes that you are currently reading are version controlled using Git, and they are hosted at the GitHub repository at https://github.com/RENCI-NRIG/X-CITE/. You can get a local copy of that repository using git clone command:\n$ git clone https://github.com/RENCI-NRIG/X-CITE.git\nCloning into 'X-CITE'...\nremote: Enumerating objects: 948, done.\nremote: Counting objects: 100% (485/485), done.\nremote: Compressing objects: 100% (266/266), done.\nremote: Total 948 (delta 228), reused 421 (delta 172), pack-reused 463\nReceiving objects: 100% (948/948), 4.52 MiB | 0 bytes/s, done.\nResolving deltas: 100% (446/446), done.\nYou can cd into that directory now with cd X-CITE, and then run some git commands such as git status and git log there:\n$ cd X-CITE/\n$ git status\n# On branch main\nnothing to commit, working directory clean\n$ git log\ncommit fd95497e30827d52dd99855a0e1be99b3db4282e\nAuthor: Sajith Sasidharan &lt;sajith@hcoop.net&gt;\nDate:   Mon Apr 22 09:27:27 2024 -0500\n\n    Mention the trace module\n\ncommit 7d55b104b50f8b1f5b8201765ceac8541f9543df\nAuthor: Sajith Sasidharan &lt;sajith@hcoop.net&gt;\nDate:   Mon Apr 22 09:16:44 2024 -0500\n\n    Add a line\n\n[... more output, elided for brevity ...]\n\nGetting updates from a remote repository\nAfter you originally cloned a remote repository, other people might have pushed new commits or branches or tags there. Using git fetch command will retrieve those updates, but without committing them to your local repository.\n$ git fetch origin\nHere origin refers to the remote repository. You can list remote repository with git remote, and git remote -v will print some more details about them.\nYou can then do git diff origin/main to view the changes, and do git merge origin/main (or git rebase origin/main) to get those changes in your local copy.\nWhen you want to fetch updates from the remote repository and automatically merge them into your current branch in one step, you will do a git pull:\n$ git pull\nHere you are fetching and merging changes from the main branch in the remote repository to your current local branch.\n\n\nGetting updates to a remote repository\nThe inverse of a git pull is git push. You use a git push to get your changes to a remote repository:\n$ git push origin main\nIf you are trying to push to X-CITE repository, this will ask for your username and password, and even if they are correct, git push will most likely fail, because you do not have the necessary permissions to write to the remote repository.\n\n\n\n\n\n\nNote\n\n\n\nIf you want to add your changes to X-CITE repository on GitHub, you will want to fork the repository, make your changes in your fork of the repository, and send a pull request to X-CITE repository. This is a somewhat separate discussion. Start with GitHub’s documentation to learn more."
  },
  {
    "objectID": "theme1/PE103/vcs.html#software-forges",
    "href": "theme1/PE103/vcs.html#software-forges",
    "title": "Version Control",
    "section": "Software forges",
    "text": "Software forges\nA software “forge” is a hosting service that can host your Git repositories, and provide many additional services (such as bug tracking, code reviews, continuous integration, etc) that helps you collaborate with other people.\n\nGitHub.com happens to be the most popular one.\nGitlab.com\n\nMany organizations and projects choose to self-host a version of GitLab. See CLASSE GitLab, for example.\n\nCodeberg.org, which runs Forgejo software.\nSourceHut.org\nBitBucket.com\n\nSome organizations and people prefer to self-host a forge, and some people prefer no forge at all. Since Git is a distributed version control system, you should be able to collaborate with no forge at all: you can share your changes as email attachments, if you want. Git was originally developed for Linux kernel development, which uses no forge at all."
  },
  {
    "objectID": "theme1/PE103/vcs.html#exercises",
    "href": "theme1/PE103/vcs.html#exercises",
    "title": "Version Control",
    "section": "Exercises",
    "text": "Exercises\n\nCreate an account on GitHub.com (or another forge of your choice), if you do not have an account there already. Create a new repository on the forge of your choice. Push some code that you are working on to that repository.\nMake some more changes to your project, and commit them. Push those commits also to the Git repository.\nCreate a Git tag (based on today’s date, or a version number), and push the tag to your repository.\nIf you want to add a new feature to your project, create a branch, and make your changes on the branch. When you are done, merge the feature branch to your main or master branch.\nIf you find errors in these notes, or want to suggest improvements, report them by creating an issue on https://github.com/RENCI-NRIG/X-CITE/.\nIf you want to propose some changes to these notes (such a fixing a mistake or improving these notes), create a pull request against https://github.com/RENCI-NRIG/X-CITE/."
  },
  {
    "objectID": "theme1/PE103/vcs.html#references",
    "href": "theme1/PE103/vcs.html#references",
    "title": "Version Control",
    "section": "References",
    "text": "References\n\nPro Git\nGit cheat sheet\nVersion Control module of The Missing Semester of Your CS Education.\nAbout Git section of GitHub documentation."
  },
  {
    "objectID": "theme1/PE103/debugging.html",
    "href": "theme1/PE103/debugging.html",
    "title": "Debugging",
    "section": "",
    "text": "Debugging is the process of finding errors, unexpected behavior, or performance issues in software, and fixing them.\nSometimes the problems may seem inscrutable or mysterious. Often in those cases, the real problem would turn out that your mental model of how the thing works is not quite accurate. It would be helpful to deploy the Feynman Algorithm:\n\nWrite down the problem.\nThink real hard.\nWrite down the solution.\n\nObviously you have to follow these steps in sequence. But what if you do not understand the problem quite well yet?\nWhen figuring out problems with software written in Python, there are multiple tools at your disposal. You might find one of these approaches or a combination of several approaches helpful.\n\n\nYou will add print() statements at various points in your code, in order to help you understand the flow of execution and find out where issues occur.\nLet us add some print statements to our rather contrived example.\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    print(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n    print(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nprint(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n\nNow you can watch the execution of the code:\n$ python3 temperature.py\ninput in deg C: 0\noutput in deg F: 32.0\n0 deg C is 32.0 deg F\n\n\n\nPython standard library provides a logging module, which you can use to log various events in your code. One benefit of using logging is that your application’s logs can include log messages from the libraries you use (if they are set up to use logging), so you will have more information to work with.\nThe module provides enough knobs to tune things like: level of logging (you can choose to log statements based on their severity, from all messages to just the critical messages), the format of log statements, the location of log files, time stamps of log statements, etc.\nYou can set up your module to log its actions like so:\n\n\ntemperature.py\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef celsius_to_fahrenheit(celsius):\n    logger.info(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n\n    logger.info(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nAnd you should do some setup of logging module in your main method:\n\n\nmain.py\n\nimport logging\nfrom temperature import celsius_to_fahrenheit\nlogger = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(filename=\"temperature.log\", level=logging.INFO)\n    logger.info(\"Started\")\n    celsius_to_fahrenheit(0)\n    logger.info(\"Finished\")\n\nif __name__ == \"__main__\":\n    main()\n\nRunning the code above with python3 main.py will write log statements to a file named temperature.log.\n\n\ntemperature.log\n\nINFO:__main__:Started\nINFO:__main__:Started\nINFO:temperature:input in deg C: 0\nINFO:temperature:output in deg F: 32.0\nINFO:__main__:Finished\n\n\n\n\nPython standard library has a pdb module, which provides an interactive debugging tool, or a “debugger”. Debuggers allow you to examine code while it is running.\nUsing a debugger, you can set breakpoints where the execution will stop, you can print values, you can step into through the execution of methods, etc.\nYou can run a program under pdb with python3 -m pdb &lt;program.py&gt;, like so:\npython3 -m pdb temperature.py\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) help\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    c          d        h         list      q        rv       undisplay\na      cl         debug    help      ll        quit     s        unt\nalias  clear      disable  ignore    longlist  r        source   until\nargs   commands   display  interact  n         restart  step     up\nb      condition  down     j         next      return   tbreak   w\nbreak  cont       enable   jump      p         retval   u        whatis\nbt     continue   exit     l         pp        run      unalias  where\n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\n(Pdb) next\n&gt; /tmp/temperature.py(5)&lt;module&gt;()\n-&gt; print(f\"0 deg C is {celsius_to_fahrenheit(0)}\")\n(Pdb) step\n--Call--\n&gt; /tmp/temperature.py(1)celsius_to_fahrenheit()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) p celsius\n0\n(Pdb) next\n&gt; /tmp/temperature.py(2)celsius_to_fahrenheit()\n-&gt; fahrenheit = (celsius * 9 / 5) + 32\n(Pdb) next\n&gt; /tmp/temperature.py(3)celsius_to_fahrenheit()\n-&gt; return fahrenheit\n(Pdb) p fahrenheit\n32.0\n(Pdb) continue\n0 deg C is 32.0\nThe program finished and will be restarted\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) exit\nAnother typical usage to break into the debugger is to insert an import pdb; pdb.set_trace() line into your code:\n\n\ntemperature.py\n\nimport pdb; pdb.set_trace()\n\ndef celsius_to_fahrenheit(celsius):\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the code with python3 temperature-pdb.py, and use various pdb commands.\nOr you can insert a breakpoint() statement at the location you want to break into the debugger:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    breakpoint()\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the program with python3 -m temperature.py. Once the execution reaches the line with breakpoint(), you will be dropped into the pdb shell.\n\n\n\nPython’s trace module allows you to trace program execution:\n$ python3 -m trace --trace code/temperature.py \n --- modulename: temperature, funcname: &lt;module&gt;\ntemperature.py(1): def celsius_to_fahrenheit(celsius):\ntemperature.py(12): print(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n --- modulename: temperature, funcname: celsius_to_fahrenheit\ntemperature.py(9):     fahrenheit = (celsius * 9 / 5) + 32\ntemperature.py(10):     return fahrenheit\n0 deg C is 32.0 deg F\nTo learn more, run python3 -m trace --help, and read the module documentation.\n\n\n\nIt is much easier to debug code when you have tests. The tests you write should help you test individual components of your code, and isolate points of failures. You can use a combination of print() statements, logging, and pdb.\nSee Testing for some examples.\n\n\n\nGet a friend or colleagues to review your code to identify potential issues, provide feedback, and suggest improvements. Another set of eyes can often spot problems that you might have overlooked. Explaining your code line by line to someone else is often helpful in finding the flaws in it.\nIf no human is immediately available, explain your code to a rubber duck! This very powerful technique is called rubber duck debugging, or simply, “rubberducking”.\n\n\n\nIDEs such as PyCharm and VS Code have built-in debugging facilities.\n\nJupyterLab also has a built-in debugger:\n\nHow to use these are left as an exercise to the reader. ;-)"
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-print-statements",
    "href": "theme1/PE103/debugging.html#using-print-statements",
    "title": "Debugging",
    "section": "",
    "text": "You will add print() statements at various points in your code, in order to help you understand the flow of execution and find out where issues occur.\nLet us add some print statements to our rather contrived example.\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    print(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n    print(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nprint(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n\nNow you can watch the execution of the code:\n$ python3 temperature.py\ninput in deg C: 0\noutput in deg F: 32.0\n0 deg C is 32.0 deg F"
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-logging",
    "href": "theme1/PE103/debugging.html#using-logging",
    "title": "Debugging",
    "section": "",
    "text": "Python standard library provides a logging module, which you can use to log various events in your code. One benefit of using logging is that your application’s logs can include log messages from the libraries you use (if they are set up to use logging), so you will have more information to work with.\nThe module provides enough knobs to tune things like: level of logging (you can choose to log statements based on their severity, from all messages to just the critical messages), the format of log statements, the location of log files, time stamps of log statements, etc.\nYou can set up your module to log its actions like so:\n\n\ntemperature.py\n\nimport logging\nlogger = logging.getLogger(__name__)\n\ndef celsius_to_fahrenheit(celsius):\n    logger.info(f\"input in deg C: {celsius}\")\n    fahrenheit = (celsius * 9 / 5) + 32\n\n    logger.info(f\"output in deg F: {fahrenheit}\")\n    return fahrenheit\n\nAnd you should do some setup of logging module in your main method:\n\n\nmain.py\n\nimport logging\nfrom temperature import celsius_to_fahrenheit\nlogger = logging.getLogger(__name__)\n\ndef main():\n    logging.basicConfig(filename=\"temperature.log\", level=logging.INFO)\n    logger.info(\"Started\")\n    celsius_to_fahrenheit(0)\n    logger.info(\"Finished\")\n\nif __name__ == \"__main__\":\n    main()\n\nRunning the code above with python3 main.py will write log statements to a file named temperature.log.\n\n\ntemperature.log\n\nINFO:__main__:Started\nINFO:__main__:Started\nINFO:temperature:input in deg C: 0\nINFO:temperature:output in deg F: 32.0\nINFO:__main__:Finished"
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-the-pdb-module",
    "href": "theme1/PE103/debugging.html#using-the-pdb-module",
    "title": "Debugging",
    "section": "",
    "text": "Python standard library has a pdb module, which provides an interactive debugging tool, or a “debugger”. Debuggers allow you to examine code while it is running.\nUsing a debugger, you can set breakpoints where the execution will stop, you can print values, you can step into through the execution of methods, etc.\nYou can run a program under pdb with python3 -m pdb &lt;program.py&gt;, like so:\npython3 -m pdb temperature.py\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) help\n\nDocumented commands (type help &lt;topic&gt;):\n========================================\nEOF    c          d        h         list      q        rv       undisplay\na      cl         debug    help      ll        quit     s        unt\nalias  clear      disable  ignore    longlist  r        source   until\nargs   commands   display  interact  n         restart  step     up\nb      condition  down     j         next      return   tbreak   w\nbreak  cont       enable   jump      p         retval   u        whatis\nbt     continue   exit     l         pp        run      unalias  where\n\nMiscellaneous help topics:\n==========================\nexec  pdb\n\n(Pdb) next\n&gt; /tmp/temperature.py(5)&lt;module&gt;()\n-&gt; print(f\"0 deg C is {celsius_to_fahrenheit(0)}\")\n(Pdb) step\n--Call--\n&gt; /tmp/temperature.py(1)celsius_to_fahrenheit()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) p celsius\n0\n(Pdb) next\n&gt; /tmp/temperature.py(2)celsius_to_fahrenheit()\n-&gt; fahrenheit = (celsius * 9 / 5) + 32\n(Pdb) next\n&gt; /tmp/temperature.py(3)celsius_to_fahrenheit()\n-&gt; return fahrenheit\n(Pdb) p fahrenheit\n32.0\n(Pdb) continue\n0 deg C is 32.0\nThe program finished and will be restarted\n&gt; /tmp/temperature.py(1)&lt;module&gt;()\n-&gt; def celsius_to_fahrenheit(celsius):\n(Pdb) exit\nAnother typical usage to break into the debugger is to insert an import pdb; pdb.set_trace() line into your code:\n\n\ntemperature.py\n\nimport pdb; pdb.set_trace()\n\ndef celsius_to_fahrenheit(celsius):\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the code with python3 temperature-pdb.py, and use various pdb commands.\nOr you can insert a breakpoint() statement at the location you want to break into the debugger:\n\n\ntemperature.py\n\ndef celsius_to_fahrenheit(celsius):\n    breakpoint()\n    fahrenheit = (celsius * 9 / 5) + 32\n    return fahrenheit\n\nprint(f\"0 deg C in fahrenheit: {celsius_to_fahrenheit(0)}\")\n\nNow you can run the program with python3 -m temperature.py. Once the execution reaches the line with breakpoint(), you will be dropped into the pdb shell."
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-the-trace-module",
    "href": "theme1/PE103/debugging.html#using-the-trace-module",
    "title": "Debugging",
    "section": "",
    "text": "Python’s trace module allows you to trace program execution:\n$ python3 -m trace --trace code/temperature.py \n --- modulename: temperature, funcname: &lt;module&gt;\ntemperature.py(1): def celsius_to_fahrenheit(celsius):\ntemperature.py(12): print(f\"0 deg C is {celsius_to_fahrenheit(0)} deg F\")\n --- modulename: temperature, funcname: celsius_to_fahrenheit\ntemperature.py(9):     fahrenheit = (celsius * 9 / 5) + 32\ntemperature.py(10):     return fahrenheit\n0 deg C is 32.0 deg F\nTo learn more, run python3 -m trace --help, and read the module documentation."
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-unit-tests",
    "href": "theme1/PE103/debugging.html#using-unit-tests",
    "title": "Debugging",
    "section": "",
    "text": "It is much easier to debug code when you have tests. The tests you write should help you test individual components of your code, and isolate points of failures. You can use a combination of print() statements, logging, and pdb.\nSee Testing for some examples."
  },
  {
    "objectID": "theme1/PE103/debugging.html#talk-to-a-friend-or-a-rubber-duck",
    "href": "theme1/PE103/debugging.html#talk-to-a-friend-or-a-rubber-duck",
    "title": "Debugging",
    "section": "",
    "text": "Get a friend or colleagues to review your code to identify potential issues, provide feedback, and suggest improvements. Another set of eyes can often spot problems that you might have overlooked. Explaining your code line by line to someone else is often helpful in finding the flaws in it.\nIf no human is immediately available, explain your code to a rubber duck! This very powerful technique is called rubber duck debugging, or simply, “rubberducking”."
  },
  {
    "objectID": "theme1/PE103/debugging.html#using-ides",
    "href": "theme1/PE103/debugging.html#using-ides",
    "title": "Debugging",
    "section": "",
    "text": "IDEs such as PyCharm and VS Code have built-in debugging facilities.\n\nJupyterLab also has a built-in debugger:\n\nHow to use these are left as an exercise to the reader. ;-)"
  },
  {
    "objectID": "theme1/PE101/index.html",
    "href": "theme1/PE101/index.html",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "By itself, Python provides everything you need to write programs. These programs won’t have a fancy user interface and they may not run very fast, but they’ll work. If that’s all Python offered, it might have become a popular language but it wouldn’t have taken over most of the world the way it has. No, what Python has going for it is a simple way to take commonly-used chunks of code, wrap them up neatly into sharable budles, and distribute those bundles far and wide. The mechanism for doing this in Python is called packages.\nIn this training unit, PE101-01, we’re going to look at some of the packages that come with Python. These are packages that you can count on being available anywhere you can run Python. In the next unit, PE101-02, we’ll look at how to find and use packages hosted in repositories available to anyone but not necessarily already installed where you’re running your programs.\nPython is, by itself, a rather simple language. The PE100 series of units has introduced you to almost all of the language. The language is kept small by moving the “nice to have, but not really necessary” parts into their own independent packages. Let’s start with an example:\n\n\npi equals 3.141592653589793\nThere are 2652 possible outcomes when drawing two cards from a deck\nThe natural logarithm of 7.994 is 2.0786912602891316\n\n\n\n\nThere are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details.\n\n\n\nSo far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory.\n\n\n\nAs we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks.\n\nSource: PE101-01: Using Python Packages and Modules"
  },
  {
    "objectID": "theme1/PE101/index.html#packages",
    "href": "theme1/PE101/index.html#packages",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "There are literally oodles of mathematical functions already implemented for you in the math package. To see a list of them as they stand currently, see ” math - mathematical functions ” in the current Python documentation.\nTaking a look at the code above, the first thing we notice is the line import math. This tells the Python interpreter to find the package named “math” and to open it up and make its contents available to this session. The things in the package we can get to will all be named by the word “math”, a period, and then the name of the actual part of the package to use. We would say the package “math” is imported into the “math namespace”. This is the default behavior, but we can change that. Indeed:\n\n\n0.5728159131285796\n\n\nBy using the as keyword in our import statement, we’re telling Python to load the “random” package but let us refer to everything as though its name was “rand”. In a little more detail, we’re creating a namespace “rand” instead of just letting Python automatically create a namespace with the same name as the package and load everything into that space.\nTo see a current list of the packages that come with a standard Python installation, take a look at this comprehensive list. In the first few sections it will list “built-in” capabilities - this is what you can do without importing anything. The rest of the page lists the available packages. Click on any of them for details."
  },
  {
    "objectID": "theme1/PE101/index.html#modules",
    "href": "theme1/PE101/index.html#modules",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "So far we’ve seen functions and constants placed into packages and directly accessible with just the package name. If you have a large package, or a package that has lots of custom changes to manage, it can be helpful to break things up into modules. Think of a module as a “sub-package”. Package and module names are separated by periods. Let’s take a look…\n\n\nall is good.\n\n\nWe imported the “path” module from the “os” package and loaded it into a namespace called “op”. Then we were able to use that namespace to get to the exists() function. We checked to see if the “/usr/bin” directory exists. That is, as you might suspect, a critically important directory."
  },
  {
    "objectID": "theme1/PE101/index.html#coming-up-next-packages-from-the-outside-world",
    "href": "theme1/PE101/index.html#coming-up-next-packages-from-the-outside-world",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "",
    "text": "As we keep saying, one of the biggest (if not the biggest) strengths of Python is the half million packages that people have written and made publicly available. In the next unit, PE101-02: Repositories, Sharing, and Conda, we’ll take a look at how to find those packages, copy them to CHESS servers, and use them in your own notebooks."
  },
  {
    "objectID": "theme1/PE101/index.html#conda---hard-problems-made-solvable",
    "href": "theme1/PE101/index.html#conda---hard-problems-made-solvable",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Conda - hard problems made solvable",
    "text": "Conda - hard problems made solvable\nFortunately, there is Conda, a software tool and a repository of its own. The conda developers keep a subset of the half billion packages that are available and they ensure that their repository reflects a combination of versions that should work together. They do the hard work, we take advantage of it. They stay in business by selling their tools to commercial users but, being a research organization, we’re not required to pay.\nConda also has another useful trick: it can take advantage of Python’s virtual environments to let you load outside packages into a completely private space. This way, when you download and install the “instantnobelprize” package (I made that up), it’s only written to your own directories. Other users, and the system as a whole, are protected from whatever it might contain.\nSetting up Conda and using it with Jupyter notebooks takes a little bit of work and has to be done from the command line, but so often it’s worth it. If you haven’t used the command line yet, take a look at the training units in SF100 on the Linux command line and scripting.\nWhat follows is taken directly from the CLASSE wiki entry for JupyterHub with just a few modifications."
  },
  {
    "objectID": "theme1/PE101/index.html#python-environments",
    "href": "theme1/PE101/index.html#python-environments",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Python Environments",
    "text": "Python Environments\nA Python environment is a local, unique to a user, repository plus a copy of the Python interpreter itself. Having a private environment is how we can load specific versions of packages even when the server has a different one. It even lets us install specific versions of python without affecting anyone else.\nWhen you launch a new notebook, you are presented with a dropdown to select your desired python kernel. The default Python 3 kernel is a CLASSE-IT maintained conda environment in /nfs/opt/anaconda3/envs/python3"
  },
  {
    "objectID": "theme1/PE101/index.html#adding-new-environments",
    "href": "theme1/PE101/index.html#adding-new-environments",
    "title": "PE101-01: Using Python Packages and Modules",
    "section": "Adding New Environments",
    "text": "Adding New Environments\nIn addition, you can install your own python environments and have them added as an option when creating new notebooks.\nCreate your own python environment using your desired python installation. Please see LinuxSoftwareDevelopment for a list of centrally maintained python environments, and further down LinuxSoftwareDevelopment for tips on creating your own conda installation.\nInstall anything you like in the environment, but you MUST at least install ipykernel. For example\npip install ipykernel\nActivate the new environment. If using conda, this would look something like:\nsource /path/to/conda/install/bin/activate conda activate my-python-env\nAdd the virtual environment as a jupyter kernel using\npython -m ipykernel install --user --name=my-python-env --display-name \"My Python Env\"\nThis adds the kernel to ~/.local/share/jupyter/kernels/ and now it can be used by Jupyter. When you create a new notebook now, “My Python Env” will be one of your choices."
  },
  {
    "objectID": "theme2/SF200/parallel-computing.html",
    "href": "theme2/SF200/parallel-computing.html",
    "title": "Parallel Computing Concepts",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme2/SF100/getting-started.html",
    "href": "theme2/SF100/getting-started.html",
    "title": "Getting Started - Accessing the Linux Cluster with NoMachine and SSH",
    "section": "",
    "text": "In the course of your research at CHESS, you’ll need to do a lot of computation. What constitutes “a lot” is discipline-specific and varies tremendously depending on what the exact processing is. It’s entirely possible that much of your work can be completed on your laptop. On the other hand, it’s also likely that at least some, and possibly a great deal, of your computing will need to be done on the Linux cluster. In simple terms, the cluster is a collection of machines running Linux (a variant of Unix) and able to coordinate workflows, assigning the running of “stuff” to an appropriate server at the right time.\nWhile there are many servers in the cluster, one of them is special. It’s name is “LNX201”, and it’s responsible for accepting new jobs and dealing with the output. It’s also the right server to use for developing your programs. Keep in mind that dozens of people may be using LNX201 at the same time, so don’t use it for running lengthy computations. If your job runs for more than a few seconds, it’s probably time to learn how to run it on some of the worker nodes. We’ll talk about that subject in SF201 when we get our first look at batch processing. Until then, we’ll keep our examples comfortably small and LNX201 will be the perfect home.\n\nRemote Desktop Access\nWhy do we need remote access at all? My laptop has a pretty decent processor, nice graphics, and a tolerable keyboard. Why don’t I just use that? The answer, or one of the answers, is that the computers in the cluster are larger and more powerful. LNX201 has 16 CPUs (“cores”, sort of) and 128 gigabytes of RAM. Each of the cores in the server are several times faster than the ones in a laptop. In short, the compute cluster can handle jobs that would bring a laptop to its knees, fans screaming, too hot to handle.\nThere are two kinds of remote access and both have their strengths and weaknesses. The first approach we’ll look at is the “virtual desktop”. This strategy takes the full graphical experience of using a computer, potentially in a far-flung destination, and shows what is going on right in front of you. The effect is to make your computer’s screen, keyboard, and mouse act like they’re connected to the remote machine, albeit with a really, really long set of cables. The second remote access approach is to use a program on your laptop or desktop to connect over the network to a remote machine and just pass characters back and forth. It gives you, for all practical purposes, something that looks like a window for using the command line, only the computer running those commands is remote.\n\n\nVirtual Desktops - NoMachine\nCHESS uses a software system called “NoMachine” for remote Virtual Desktop access. NoMachine has oodles (a scientific term) of features and configuration settings, but if you just want a basic Virtual Desktop that is good enough for all but a few special cases then you can just play on Easy Mode: use the web client that runs inside a browser and just take the default settings.\nThe rest of this lesson assumes you have: 1. A CLASSE username and password 2. The DUO app installed on your phone and configured for access to CLASSE 3. The “Pritunl” Virtual Private Network software installed on your laptop.\nThe instructions for each of these steps are in the very first steps shown in CLASSE-IT 2025 Summer Student Guide.\nGo ahead and connect to the CLASSE internal network through the Pritunl VPN software. Then go to your web browser and put in the destination URL https://nomachine.classe.cornell.edu. In just a moment, you should be seeing the initial login screen for NoMachine itself. Put in your username and password, then click on “OK” in the lower right corner.\n\nThis starts the login process, the next part of which is using DUO for two-factor authentication:\n\nOn this screen, enter a “1” if you want to use the Duo app on your phone. Otherwise, you can enter “2” and the Duo servers will call you to verify that the person trying to log in at least controls that phone number. If you selected “1”, then in just a moment your phone will beep, you can go to the Duo app, and click the green button to approve of the login request. Completing the Duo process will take you to the next screen.\n\nIn this screenshot, I’ve already selected “lnx201” so you can spot it right away. This is the one you’ll want to use. Go on and double click it to start logging in to LNX201. You’ll see a screen listing the available session types. In principle you could confighure different graphical environments here, but in reality you should just take the default.\n\nClicking OK on that will take you a quick reminder of some commands that might be useful when you’re connected. There is a checkbox you can select when you get familiar with this and that will keep this screen from showing up again.\n\nIn the meantime, click OK to continue to the next helpful screen:\n\nAnd then OK again to see a screen with some helpful advice, to be sure, but also a chance to select how the screen resolution should be handled.\n\nOn this screen I have selected the first option, “Scale the remote desktop to fit into the window”, because I like being able to change the size of my browser window and yet still see the same amount of screen space on the remote machine. Try different options here - you won’t break anything. Clicking “OK” will take you to the final information-and-setup screen:\n\nThis screen gives options for setting up the screen resolution on the remote computer. Because LNX201 is a server and not a machine with an actual graphics card and a monitor, all the sessions use simulated (virtual) desktops and that means they can be set to any resolution you want. Selecting “Don’t resize” is a reasonable starting point. You might want to experiment with the others later. In any case, we can finally click on “OK” for this screen and connect to the desktop session!\n\nIt may take a moment for everything to start up, and another moment for the remote screen drawing to catch up, but pretty soon you’ll see a Linux desktop environment. Desktop environments have mostly converged on roughly the same concepts. The one you see here is “Xfce”, and it traces its heritage to the great Unix Workstations of the 1980s. MacOS, similarly, is derived from NeXTstep which is based on work on Unix Workstations in the 1980s. Various versions of Windows are based on… you get the idea. They’re all more alike than different.\nIronically, perhaps, the most common thing we’ll do on a remote desktop session is “start a terminal window”. In this case, the task is dead simple: click on the “Applications” menu and select the “Terminal Emulator” item.\n\nIn just a moment, the terminal window appears:\n\nJust as you would expect, we have a file browser available and it’s more or less equivalent to Windows’ Explorer or to the Mac’s Finder. Double Click on the “Home” folder…\n\nQuite a few applications are already installed in the Linux cluster, some of which can really take advantage of having graphics through a remote desktop environment. One example is Mathematica:\n\nAt this point, we’ve seen how to log in to the Linux cluster through a virtual desktop session, seen how to move around just a bit in that graphical environment, and teased ourselves with a bit of mathematical loveliness. All good things must come to an end, eventually, and this session is no exception. When you’re ready to go, click on your name in the upper right corner of the desktop, select the “Log Out…” menu item, and log off. This will cause the session to end and you’ll go back to the initial login screen for NoMachine.\n\nNow that we’ve gone through the basics, there might be (should be?) one nagging… consideration. Doesn’t this seem like a fairly complicated and not terribly fast way to use to use a Linux machine remotely if all I want to do is work in a terminal session?\nYes. Yes it is.\nFortunately, there’s another way. It’s called “SSH”.\n\n\nRemote Terminal Access with SSH\nTo have a good understanding of SSH, you need to know just a little bit about the history of Unix (and Linux is just a modern version of Unix). In the Old Days (1974-ish) Unix ran on a machine the size of home refrigerator and the server didn’t even have a keyboard. To interact with it, teletypes were plugged into serial ports on the back on the computers. Users performed all of their interaction with the machine by pressing a key on the teletype (sending a character) and waiting for the computer to send back characters to be printed. By the late 1970s, teletypes were getting rare quickly because they had been replaced by “terminals” (a keyboard, a picture tube, and enough electronics to draw the characters on the screen). The connection to the computer was still a serial cable. To this day, you can plug a USB-to-Serial converter into a Linux machine, plug in a teletype or a terminal, set up a configuration file, and use the computer like it’s 1979. It’s fun a time or two, but the novelty wears off.\nSerial cables had a length limitation and a speed one. Half a mile was easy (9600 bits per second, even!) but a mile was getting unreliable at any speed much over 1200 bits per second. Beyond a mile, longer distances required all sorts of interesting hardware and lots of special cables. Covering a whole campus required a network of some sort, and what the research and academic world standardized on was “Internet Protocol” (IP). One of the first programs ever written for IP was a program called “telnet”. This program used the network to set up a stream of characters to and from a remote machine to the local one. It was like having a serial cable, except it was literally a thousand times faster and could reach around the world. To this day, telnet is still the “official” way to use Internet Relay Chat (IRC). Plus, it makes you look cool.\nTelnet was great in the early days of networking. But we were so naive back then. Anyone who could get access to the network could eavesdrop on whatever communications was going over these telnet connections. Most of the traffic was utterly boring - email, Unix commands, that sort of thing. But there was one thing worth snooping on, and it was the first two things a user typed when they set up a connection: their username and their password. In the late 1980s and early 1990s, this was mostly a theoretical concern. The problem was the explosive growth of the Internet in the “Dot Com” era (1995-2000). Once there were a critical mass of Internet users, and once these users started using the Internet for things that were worth actual money, security started moving front and center. By this point, sending anything over the internet without encrypting it became a VBA (Very Bad Idea). Thus was born the Secure SHell (SSH).\nThe simple explanation of SSH is “it’s just telnet, except all the communication is encrypted”. This statement is true, in the sense that SSH replicates every telnet function, but SSH also does some other tricks. For our purposes, we’re going to keep it simple and just use it like a “better telnet”.\nTo use SSH on your computer, you need to bring up a “Terminal Window” (Mac) or “Command Window” (Windows). On a Mac, use Finder to browse the Applications folder. In there, open the Utilities folder, then double click on Terminal. (After a while, you might decide to just use Spotlight to search for Terminal.app, but it’s up to you). On a PC, the easiest thing to do is press and release the “Windows” key, then type “cmd” and press enter. That will bring up a command window.\nEither way, you’re now looking at a terminal window of some sort. Characters are entered and sent to programs, and programs send characters back out and the terminal window draws them on the screen. Just like a teletype, except no paper and no oily smell. Windows users will probably see something similar to C:\\&gt;. Mac users will see something more akin to escott@RENCI_LP91DX62MX ~ %. Both of these are “prompts”, meaning “you can type a command here”. In both cases the command to type is ssh lnx201.classe.cornell.edu. The lower case letters in “ssh” are important. Linux commands almost never contain uppercase letters. Go ahead, enter that ssh command, and press Enter. If this is the very first time you’ve ssh’ed into lnx201 from the machine you’re at, you’ll get some warnings just to remind you to be wary:\nThe authenticity of host 'lnx201.classe.cornell.edu (128.84.45.81)' can't be established.\nED25519 key fingerprint is SHA256:cIplmL7rqVGlAKYlTwtfml+KiSvUuBhgKuyjkPbde7E.\nThis key is not known by any other names.\nAre you sure you want to continue connecting (yes/no/[fingerprint])?\nYou can type “yes” here and press enter, and ssh will tell us it’s storing that cryptographic fingerprint. The “fingerprint”, by the way, refers to a encrypted, secure way to tell we’re really using the server we think we are. It has nothing to do with ridges and swirls on our fingertips. We’re going to assume no one has hijacked the actual name of the server and put up a fake one. There is a way to protect against this, but it’s a (minor?) hassle so it’s rarely done. Since we’re doing this the rough-and-ready way, ssh informs us it’s going to store a cryptographic “fingerprint” for the server. It will check that fingerprint every time we try to connect in the future and make sure it matches. If it doesn’t, it’ll warn us to be suspicious and not continue until we’re at least comfortable we know why.\nWhether it needs to notify us of anything or not, it will always prompt with the following:\n(escott@lnx201.classe.cornell.edu) Password:\nAt this point, type your password. Nothing will appear as you’re typing, not even any little dots. When you think about it, teletypes didn’t have any little dots.\nIf you type your password correctly, you’ll see one more prompt:\n(escott@lnx201.classe.cornell.edu) Duo two-factor login for escott\n\nEnter a passcode or select one of the following options:\n\n 1. Duo Push to XXX-XXX-3880\n 2. Phone call to XXX-XXX-3880\n\nPasscode or option (1-2):\nEnter a “1” and press enter. You’ll hear Duo chiming on your phone. Approve your login, let all of that percolate through the system for a second, and you’ll see your command line prompt on LNX201:\n[escott@lnx201 ~]$\nAt this point, you can run all of your favorite commands: ls, mkdir, the whole bunch. When you are done, don’t forget to log out. The “exit” command logs you out and breaks down the SSH connection.\n\n\nParting Shot, and a Note to the Nerds\nWe’ve just looked at two very different ways to remotely access the CLASSE Linux servers that CHESS uses. The remote desktop approach, using NoMachine, has the advantage of being relatively easy to use and of fully supporting graphical programs but at the expense of potentially quite a bit of lag time between doing something and seeing the results on the screen. Using SSH, on the other hand, will usually feel more responsive and use less network bandwidth but is for command line use only - no graphics. Which one to use is a matter of deciding what tradeoffs are acceptable. If you’re on campus then bandwidth isn’t an issue. Use both - no one will judge you. If you’re out in the boondocks and getting network connectivity through a bad cellphone connection then you might want to use SSH.\nSSH has a few more tricks up its sleeve. One is that it can copy files. When you copy files this way you use the “scp” command instead of “ssh”, but it’s really the same program behind the scenes. Another trick is that it can do “port forwarding” - a poor substitute for a VPN, but sometimes you’re in a situation where you’re not allowed to install a VPN to get to your instrument, for instance. And finally, yes, I have to acknowledge this for the nerds who’ve been waiting to ambush me… ssh can be used to run graphical programs remotely. The “-X” and “-Y” options are the relevant ones. Results are “good enough” for simple programs over very fast networks, but slower networks make the experience miserable and complicated graphical programs (Chrome? Just saying…) are awful no matter how fast the network is.\nComing up next, we’ll use these remote connections to start learning about command line tools and how to use them."
  },
  {
    "objectID": "theme2/SF100/slides.html#assumptions",
    "href": "theme2/SF100/slides.html#assumptions",
    "title": "Systems Fundamentals",
    "section": "Assumptions",
    "text": "Assumptions\n\nYou have your CLASSE accounts set up.\n\nTalk to CLASSE staff otherwise!\n\nOptions:\n\nUse ssh\nUse NoMachine\nUse JupyterLab"
  },
  {
    "objectID": "theme2/SF100/slides.html#use-ssh",
    "href": "theme2/SF100/slides.html#use-ssh",
    "title": "Systems Fundamentals",
    "section": "Use SSH",
    "text": "Use SSH\n\nssh ${username}@lnx201.classe.cornell.edu\n\nUse terminal or iTerm on macOS.\nUse whichever terminal you want on Linux.\nUse PuTTY (https://putty.org/) on Windows.\n\n\n\nUse terminal or iTerm on macOS. There are other choices too, but these seem popular.\nUse whatever terminal you want on Linux. You know what you are doing, and you know how to figure out stuff.\nUse PuTTY on Windows. I am not up to speed on Windows."
  },
  {
    "objectID": "theme2/SF100/slides.html#use-nomachine",
    "href": "theme2/SF100/slides.html#use-nomachine",
    "title": "Systems Fundamentals",
    "section": "Use NoMachine",
    "text": "Use NoMachine\n\nhttps://wiki.classe.cornell.edu/Computing/NoMachine\n\n\nThere is a client that you can install.\nThere is also web access, which I have used a few times. Seemed shaky."
  },
  {
    "objectID": "theme2/SF100/slides.html#use-jupyterlab",
    "href": "theme2/SF100/slides.html#use-jupyterlab",
    "title": "Systems Fundamentals",
    "section": "Use JupyterLab",
    "text": "Use JupyterLab\n\nhttps://jupyter01.classe.cornell.edu/\n\n\nUse the terminal icon on the launcher.\nOr use File -&gt; New -&gt; Terminal"
  },
  {
    "objectID": "theme2/SF100/slides.html#linux",
    "href": "theme2/SF100/slides.html#linux",
    "title": "Systems Fundamentals",
    "section": "Linux",
    "text": "Linux\n\nA popular operating system.\n\n(Actually an OS kernel, plus userland from various other projects. But those are details…)\n\nUnix-like, which traces back to 1969, therefore has accumulated quirks.\n\nExpect “hysterical raisins”."
  },
  {
    "objectID": "theme2/SF100/slides.html#lnx201",
    "href": "theme2/SF100/slides.html#lnx201",
    "title": "Systems Fundamentals",
    "section": "lnx201",
    "text": "lnx201\n\nThe Linux host we’ll be using is lnx201.classe.cornell.edu.\nRuns a distribution called Scientific Linux.\nGood enough for general use.\nDo not run anything resource heavy on lnx201.\n\n“Heavy” in terms CPU, memory, network usage etc.\nlnx201 is a shared resource.\n\nThere is a Compute Farm to run heavy things.\n\n\n\nI am not the expert on lnx201 or compute farm. I just happen to present today!"
  },
  {
    "objectID": "theme2/SF100/slides.html#the-command-line",
    "href": "theme2/SF100/slides.html#the-command-line",
    "title": "Systems Fundamentals",
    "section": "The command line",
    "text": "The command line\n\n\nYou will type commands in a shell, at the shell prompt, hit enterenter key, and then things happen.\n\n\n\nAll of this is a text user interface.\nAs opposed to clicking on GUI widgets.\nThe prompt is that piece of text that begins with [ and ends with ] $. They can look different too."
  },
  {
    "objectID": "theme2/SF100/slides.html#commands",
    "href": "theme2/SF100/slides.html#commands",
    "title": "Systems Fundamentals",
    "section": "Commands",
    "text": "Commands\n\nCommands are either programs or shell builtins.\nUse one of these commands to read documentation:\n\nman ${command}\ninfo ${command}\nor ${command} --help (sometimes!)\n\n\n\n\nPrograms can be compiled or scripts.\nThey live somewhere in your $PATH."
  },
  {
    "objectID": "theme2/SF100/slides.html#the-shell",
    "href": "theme2/SF100/slides.html#the-shell",
    "title": "Systems Fundamentals",
    "section": "The shell",
    "text": "The shell\n\nA program that accepts commands, and passes those commands to the OS to execute.\nA popular shell is bash, which is the default on lnx201.\n\n\nOf course there are other shells too: zsh (default on newer versions of macOS, fish, ksh, etc."
  },
  {
    "objectID": "theme2/SF100/slides.html#bash",
    "href": "theme2/SF100/slides.html#bash",
    "title": "Systems Fundamentals",
    "section": "Bash",
    "text": "Bash\n\n“Bourne-again shell”\n\nBased on an earlier Bourne shell, thus the “again”.\nDeveloped by the GNU project.\nOn lnx201, /bin/bash is the program.\n\nFor documentation: info bash or man bash.\n\n\n\nGNU people made a lot of the software early on.\n\nAlso Unix people, BSD people, X11 people, and a whole bunch of other people.\nThen the Linux kernel came along, became successful, and people started to call the whole combination of all these things “Linux”."
  },
  {
    "objectID": "theme2/SF100/slides.html#bash-niceties-history-and-completion",
    "href": "theme2/SF100/slides.html#bash-niceties-history-and-completion",
    "title": "Systems Fundamentals",
    "section": "Bash niceties: history and completion",
    "text": "Bash niceties: history and completion\n\nYou do not have to re-type commands that you have used in the past!\n\nUse upup and downdown arrow keys to go back and forth in your command history.\nUse Ctrl-RCtrl-R (Control+R) to “search” command history.\nUse history command to list your shell history.\n\nUse tabtab key for command completion, after typing a few characters."
  },
  {
    "objectID": "theme2/SF100/slides.html#some-helpful-commands",
    "href": "theme2/SF100/slides.html#some-helpful-commands",
    "title": "Systems Fundamentals",
    "section": "Some helpful commands",
    "text": "Some helpful commands\n\n\n\n\n\n\n\n\nCommand\nTask\nExample Syntax\n\n\n\n\nls\nlist the files in a directory\nls [/tmp]\n\n\ncd\nmove into a directory\ncd [/tmp]\n\n\npwd\nshow curent working directory\npwd\n\n\ncp\ncopy a file to into another directory, or make a copy with a different name\ncp [file.txt] [/tmp/file.txt]\n\n\ncp -r\ncopy a folder to into another directory\ncp [file.txt] [/tmp/file.txt]\n\n\nmv\nrename or move a file into another directory\nmv [file.txt] [file1.txt]\n\n\nrm\ndelete a file\nrm [file.txt]\n\n\nrm -r\nremove a directory, recursively\nrm -r [dir]\n\n\nmkdir\ncreate a directory\nmkdir [dir]\n\n\nfind\nfind a file\nfind [/tmp] -name [file]\n\n\ngrep\nsearch for a text pattern inside a file\ngrep [text] [/tmp/file.txt]\n\n\nless\nto view the text of a text file, one screen at a time\nless [/tmp/file.txt]\n\n\nexit\nexit and logout of a Terminal (Terminal-xfce4) session\nexit\n\n\n\nSource: https://wiki.classe.cornell.edu/Computing/SummerStudentOrientation#Linux_Command_Line_40ls_44_cd_44_mv_44_find_44_grep_etc_41"
  },
  {
    "objectID": "theme2/SF100/slides.html#directory-navigation",
    "href": "theme2/SF100/slides.html#directory-navigation",
    "title": "Systems Fundamentals",
    "section": "Directory navigation",
    "text": "Directory navigation\n[ssasidharan@lnx201 ~]$ tree -d -L 1 /\n/\n├── bin -&gt; usr/bin\n├── boot\n├── cdat\n├── cifs\n├── cvmfs\n├── dev\n├── etc\n├── home\n├── lib -&gt; usr/lib\n├── lib64 -&gt; usr/lib64\n├── media\n├── misc\n├── mnt\n├── net\n├── nfs\n├── opt\n├── proc\n├── root\n├── run\n├── sbin -&gt; usr/sbin\n├── srv\n├── sys\n├── tmp\n├── usr\n└── var\n\n25 directories\n\n\nDirectories and files are organized in a tree like structure.\nWell, an inverted tree, maybe.\nAt the bottom (or top?), you have the “/” directory."
  },
  {
    "objectID": "theme2/SF100/slides.html#your-home-directory",
    "href": "theme2/SF100/slides.html#your-home-directory",
    "title": "Systems Fundamentals",
    "section": "Your home directory",
    "text": "Your home directory\n\nYou have a “home” directory.\n\nYou can write your files and create directories here.\n\nUsually, and on lnx201, this will be /home/$USER\n\nAlso known as $HOME\n\n\n\n\nThe stuff that follows $ are environment variables aka env vars. They are sort of key-value pairs managed by the shell.\nThere are a set of standard env vars such as $USER, $HOME, $SHELL etc.\nDon’t write too much on your home directory.\nThere are better places top store large amounts of data.\nI am not the expert here. Ask around, or wait for the next presenter."
  },
  {
    "objectID": "theme2/SF100/slides.html#those-whatever-things",
    "href": "theme2/SF100/slides.html#those-whatever-things",
    "title": "Systems Fundamentals",
    "section": "Those $WHATEVER things",
    "text": "Those $WHATEVER things\n\nWhat are $HOME, $USER, $PATH, $SHELL, etc.?\nThey are called environment variables, or env vars.\nEnv vars are pieces of information maintained by the shell.\nPrograms can use them during execution.\nUse printenv or env command to list them."
  },
  {
    "objectID": "theme2/SF100/slides.html#the-current-working-directory",
    "href": "theme2/SF100/slides.html#the-current-working-directory",
    "title": "Systems Fundamentals",
    "section": "The current working directory",
    "text": "The current working directory\n\n\nAt any time in the shell, you are “inside” a single directory, called the current working directory.\n\nWhen you do ls, files in current working directory will be listed, etc.\n\nWhen you log in, your current working directory will be your home directory: /home/$USER aka $HOME.\nYou will use cd (change directory) to move around.\nUse the command pwd to find where you are.\n\nOr echo $PWD."
  },
  {
    "objectID": "theme2/SF100/slides.html#absolute-and-relative-paths",
    "href": "theme2/SF100/slides.html#absolute-and-relative-paths",
    "title": "Systems Fundamentals",
    "section": "Absolute and relative paths",
    "text": "Absolute and relative paths\nFile/folder names are also referred to as paths.\n\nAbsolute path names begin with the root directory, /.\n\nExample: /home/ssasidharan/Documents/hello.txt\n\nRelative paths start with the working directory.\n\nExample: ./Documents/hello.txt (or just Documents/hello.txt) when I’m in my home directory."
  },
  {
    "objectID": "theme2/SF100/slides.html#some-fun-facts-about-file-names",
    "href": "theme2/SF100/slides.html#some-fun-facts-about-file-names",
    "title": "Systems Fundamentals",
    "section": "Some fun facts about file names",
    "text": "Some fun facts about file names\n\nNames that begin with “.” are “hidden”.\n\nThey are omitted from directory listing when you do ls.\nDo ls -a (or ls --all) to list them.\n\n. and .. are special directory names.\n\n. stands for the current directory.\n.. stands for the directory above the current directory."
  },
  {
    "objectID": "theme2/SF100/slides.html#some-more-fun-facts-about-file-names",
    "href": "theme2/SF100/slides.html#some-more-fun-facts-about-file-names",
    "title": "Systems Fundamentals",
    "section": "Some more fun facts about file names",
    "text": "Some more fun facts about file names\n\nFile and directory names are case sensitive.\n\nDepends on filesystem, but that is a detail.\n\nIt is better to avoid spaces in file names, because they are a hassle.\n\nUse _ (underscore character) instead (example: file_name), or CamelCase (example: FileName).\nQuote paths within \" and \" if they happen to have spaces, or “escape” each space with \\."
  },
  {
    "objectID": "theme2/SF100/slides.html#wildcards",
    "href": "theme2/SF100/slides.html#wildcards",
    "title": "Systems Fundamentals",
    "section": "Wildcards",
    "text": "Wildcards\nSome characters are given special treatment:\n\n* matches any set of characters.\n\n[ssasidharan@lnx201 ~]$ ls /usr/bin/ab*\n/usr/bin/ab  /usr/bin/abs2rel\n\n? matches any one character.\n\n[ssasidharan@lnx201 ~]$ ls /usr/bin/?abc*\n/usr/bin/kabc2mutt  /usr/bin/kabcclient"
  },
  {
    "objectID": "theme2/SF100/slides.html#standard-input-output-and-error",
    "href": "theme2/SF100/slides.html#standard-input-output-and-error",
    "title": "Systems Fundamentals",
    "section": "Standard input, output, and error",
    "text": "Standard input, output, and error\n\nInput is read from standard input (or stdin).\nOutput is written to standard output (or stdout).\nError messages are written to standard error (or stderr).\n\n\n\n\n\n\n\nNote\n\n\nThey are files too: /dev/stdin, /dev/stdout, and /dev/stderr."
  },
  {
    "objectID": "theme2/SF100/slides.html#io-redirection",
    "href": "theme2/SF100/slides.html#io-redirection",
    "title": "Systems Fundamentals",
    "section": "I/O redirection",
    "text": "I/O redirection\nYou can redirect stdout to a file with &gt; operator:\n[ssasidharan@lnx201 ~]$ ls -l &gt; ls-output.txt\nOr append with &gt;&gt;:\n[ssasidharan@lnx201 ~]$ ls -l &gt;&gt; ls-output.txt\nTo direct a file to a programs input, use &lt; operator:\n[ssasidharan@lnx201 ~]$ cat &lt; sonnet18.txt\nShall I compare thee to a summer’s day?"
  },
  {
    "objectID": "theme2/SF100/slides.html#pipes",
    "href": "theme2/SF100/slides.html#pipes",
    "title": "Systems Fundamentals",
    "section": "Pipes",
    "text": "Pipes\n\nUsing the | (“pipe”) operator, you can “chain” programs such that one programs output is another programs input:\n\n[ssasidharan@lnx201 ~]$ ls -l /bin/ | less\n\nYou can create longer pipes:\n\n[ssasidharan@lnx201 ~]$ ls /bin /sbin | sort | uniq | wc\n   4289    4288   46820"
  },
  {
    "objectID": "theme2/SF100/slides.html#you-belong-to-groups",
    "href": "theme2/SF100/slides.html#you-belong-to-groups",
    "title": "Systems Fundamentals",
    "section": "You belong (to groups)",
    "text": "You belong (to groups)\nYour account belongs to several groups:\n[ssasidharan@lnx201 ~]$ id\nuid=63499(ssasidharan) gid=262(chess) groups=262(chess),750(classeuser)\n[ssasidharan@lnx201 ~]$ groups\nchess classeuser"
  },
  {
    "objectID": "theme2/SF100/slides.html#permissions-and-ownership",
    "href": "theme2/SF100/slides.html#permissions-and-ownership",
    "title": "Systems Fundamentals",
    "section": "Permissions and ownership",
    "text": "Permissions and ownership\nDo a “long” file listing (with ls -l) and behold:\n$ ls -l\ntotal 8\ndrwxr-xr-x  2 ssasidharan chess   44 May  8 10:42 bin\ndrwxr-xr-x  2 ssasidharan chess  144 Mar 12 00:27 CLASSE_shortcuts\ndrwxr-xr-x  2 ssasidharan chess   52 Apr  2 00:27 Desktop\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Documents\nlrwxrwxrwx  1 ssasidharan chess   31 Mar 26 15:21 Downloads -&gt; /cdat/tem/ssasidharan/Downloads\n-rw-r--r--  1 ssasidharan chess   54 Jun  2 12:38 hello.sh\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Music\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Pictures\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Public\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Templates\ndrwxr-xr-x  2 ssasidharan chess   28 Apr  2 00:27 Videos\nWhat do those characters mean?"
  },
  {
    "objectID": "theme2/SF100/slides.html#changing-permissions",
    "href": "theme2/SF100/slides.html#changing-permissions",
    "title": "Systems Fundamentals",
    "section": "Changing permissions",
    "text": "Changing permissions\n\nUse chmod command to change file mode bits (the first column in the previous listing).\n\n[ssasidharan@lnx201 ~]$ chmod +x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rwxr-xr-x 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n[ssasidharan@lnx201 ~]$ chmod -x test.sh\n[ssasidharan@lnx201 ~]$ ls -l test.sh\n-rw-r--r-- 1 ssasidharan chess 0 Mar 28 13:39 test.sh\n\nUse chown and chgrp commands to change owner and group (the third and fourth columns in the previous listing).\n\nProbably not immediately useful; just know that they exist."
  },
  {
    "objectID": "theme2/SF100/slides.html#listing-processes",
    "href": "theme2/SF100/slides.html#listing-processes",
    "title": "Systems Fundamentals",
    "section": "Listing processes",
    "text": "Listing processes\n\nList running processes using ps command:\n\n[ssasidharan@lnx201 ~]$ ps\n    PID TTY          TIME CMD\n 694411 pts/81   00:00:00 ps\n3479688 pts/81   00:00:00 bash\nThe four columns:\n\nPID is process id.\nTTY is the terminal associated with the process.\nTIME is the elapsed CPU time for the process.\nCMD is the command that created the process.\n\n\n(Also see: top and htop.)"
  },
  {
    "objectID": "theme2/SF100/slides.html#background-and-foreground-processes",
    "href": "theme2/SF100/slides.html#background-and-foreground-processes",
    "title": "Systems Fundamentals",
    "section": "Background and foreground processes",
    "text": "Background and foreground processes\n\nSome processes run in the foreground:\n\nThey read input, write output, etc.\nThey are “attached” to a terminal.\n\nBackground processes, well, run in the background. Send things to the background with &:\n\n[ssasidharan@lnx201 ~]$ sleep 100 &\n[1] 949751\n\nBring a background process to foreground using fg command, and terminate it using Ctrl-CCtrl-C:\n\n[ssasidharan@lnx201 ~]$ fg 2\nsleep 100\n^C"
  },
  {
    "objectID": "theme2/SF100/slides.html#terminating-processes",
    "href": "theme2/SF100/slides.html#terminating-processes",
    "title": "Systems Fundamentals",
    "section": "Terminating processes",
    "text": "Terminating processes\n\nkill PID command to end one process.\nkillall command to end many processes.\n\nYou can’t kill other user’s processes."
  },
  {
    "objectID": "theme2/SF100/slides.html#text-editors",
    "href": "theme2/SF100/slides.html#text-editors",
    "title": "Systems Fundamentals",
    "section": "Text editors",
    "text": "Text editors\nMany choices! Use:\n\nEmacs\nVim\nNano\nJupyterLab"
  },
  {
    "objectID": "theme2/SF100/slides.html#terminal-multiplexers",
    "href": "theme2/SF100/slides.html#terminal-multiplexers",
    "title": "Systems Fundamentals",
    "section": "Terminal multiplexers",
    "text": "Terminal multiplexers\nscreen and tmux are two options. Here’s tmux.\n\n\n\nYou can “multiplex” your terminal.\nDifferent shells in different “windows”.\nYou can split panes vertically or horizontally.\nYou can detach and re-attach, and resume your work."
  },
  {
    "objectID": "theme2/SF100/slides.html#hello-world",
    "href": "theme2/SF100/slides.html#hello-world",
    "title": "Systems Fundamentals",
    "section": "Hello world!",
    "text": "Hello world!\n\n\nhello.sh\n\n1#! /bin/bash\n\n2# A simple script.\n\n3echo \"Hello $USER!\"\n\n\n1\n\nThe “shebang”\n\n2\n\nA comment.\n\n3\n\nAn actual line of code.\n\n\n\nMake the thing executable with chmod +x hello.sh\nRun the thing with ./hello.sh"
  },
  {
    "objectID": "theme2/SF100/slides.html#other-things",
    "href": "theme2/SF100/slides.html#other-things",
    "title": "Systems Fundamentals",
    "section": "Other things",
    "text": "Other things\nBash supports a programming language with:\n\nif statements\nfor, while, until statements\nfunctions\net cetera.\nTake a peek at /etc/bashrc and $HOME/.bashrc for a taste\n\nBUT\n\nThis is not really in scope of this presentation."
  },
  {
    "objectID": "theme2/SF100/slides.html#a-cheat-sheet",
    "href": "theme2/SF100/slides.html#a-cheat-sheet",
    "title": "Systems Fundamentals",
    "section": "A cheat sheet",
    "text": "A cheat sheet\n\n(Via Stephen Turner.)"
  },
  {
    "objectID": "theme2/SF100/slides.html#resources-elsewhere",
    "href": "theme2/SF100/slides.html#resources-elsewhere",
    "title": "Systems Fundamentals",
    "section": "Resources elsewhere",
    "text": "Resources elsewhere\n\nThe Linux Command Line, A Complete Introduction by William E. Shotts, Jr. The book is freely available under a Creative Commons license, and contains a good discussion about shell scripting.\nThe Unix Programming Environment by Brian W. Kernighan and Rob Pike. Old classic, still useful. Places things in a historical context.\nShell Tools and Scripting module of MIT “The Missing Semester of Your CS Education” class."
  },
  {
    "objectID": "theme2/SF100/slides.html#other-resources",
    "href": "theme2/SF100/slides.html#other-resources",
    "title": "Systems Fundamentals",
    "section": "Other resources",
    "text": "Other resources\n\nThe Internet is pretty great. Use it.\nHowever: DO NOT trust everything you read on the Internet.\n\nDo not copy and paste commands from the Internet indiscriminately.\nUnderstand how things work, and then use it."
  },
  {
    "objectID": "theme2/SF201/batch-systems-and-compute-farms.html",
    "href": "theme2/SF201/batch-systems-and-compute-farms.html",
    "title": "Batch Systems and Compute Farms",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme5/CF200/curating-data.html",
    "href": "theme5/CF200/curating-data.html",
    "title": "Curating Data, Code, Workflows, and Publishing",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress.\nYou have run your experiments, collected data, wrote code to analyze the data, and documented your work. One next possible step that you should take is sharing the fruits of your labor with the world. The fruits of your labor includes, among other things, the data you have collected.\nHowever, you can’t simply file your results away just about anywhere, promise to make them available on demand, and declare victory. There are some principles and processes to follow.\nOne such set of principles is FAIR data principles."
  },
  {
    "objectID": "theme5/CF200/curating-data.html#fair-principles-and-practices",
    "href": "theme5/CF200/curating-data.html#fair-principles-and-practices",
    "title": "Curating Data, Code, Workflows, and Publishing",
    "section": "FAIR principles and practices",
    "text": "FAIR principles and practices\nFAIR is still evolving, and there is no one canonical implementation of FAIR. Organizations implement their individual approaches to FAIR principles based on their needs and constraints.\nAlthough FAIR itself does not make concrete recommendations about implementation, some common practices have been evolving.\n\nMaking data findable\n\nData should have globally unique and persistent identifiers. You will need to use DOIs (Digital Object Identifiers), ARKs (Archival Resource Keys), or other permanent identifier systems for your datasets. People also get stable identifiers – ORCID is one such system.\n\n\nData should be described with rich metadata. Document your data comprehensively with standardized metadata schemas relevant to your field (e.g., DataCite, Dublin Core).\n\n\nMetadata should clearly include the identifier of the data it describes.\nData should be registered or indexed in a searchable resource. You will need to deposit data in appropriate domain or institutional repositories that are indexed by search engines.\n\n\n\nMaking data accessible\n\nData should be retrievable by their identifier using a standardized protocol. Select repositories that provide web access protocols (HTTP/HTTPS) and web APIs (REST). The protocol should be open, free, and universally implementable.\nThe protocol should allow for authentication and authorization where necessary. When needed, use standard authentication protocols rather than proprietary solutions.\nEven if data has restrictions, ensure the conditions for access are clearly specified.\nMetadata should remain accessible even when the data is no longer available.\n\n\n\nMaking data interoperable\n\nData should use formal, accessible, shared, and broadly applicable language for knowledge representation. Use standard formats: store data in non-proprietary, widely-used formats. For example: for structured data, you should use CSV, JSON or XML, rather than Excel sheets.\nData should use vocabularies that follow FAIR principles. Use established ontologies and terminologies from your field.\nData should include qualified references to other data. When referencing other datasets, use their persistent identifiers.\nFollow semantic web standards. Consider RDF data models and linked data approaches for complex datasets.\n\n\n\nMaking data reusable\n\nData should have accurate and relevant attributes.\nData should be released with a clear and accessible license. Apply explicit, machine-readable licenses such as Creative Commons or Open Data Commons.\nData should include detailed information about data origin, collection methods, and processing steps.\nData should adhere to domain-specific data standards and reporting guidelines. Document data quality assessment methods and results."
  },
  {
    "objectID": "theme5/CF101/dmp-best-practices.html",
    "href": "theme5/CF101/dmp-best-practices.html",
    "title": "Best practices for developing DMP",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme4/XS200/metadata.html",
    "href": "theme4/XS200/metadata.html",
    "title": "Metadata for Data Fidelity and Systematic Checks",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme4/XS102/large-scale-data-analysis.html",
    "href": "theme4/XS102/large-scale-data-analysis.html",
    "title": "Large-scale Data Analysis: from Images to Science Parameters to Interpretation",
    "section": "",
    "text": "Note\n\n\n\nThis page is work in progress."
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html",
    "href": "theme4/XS101/linux-exercises.html",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Wherever you see &lt;your CLASSE username&gt; below, substitute your own CLASSE username.\n\n\nOpen a terminal on lnx201 using one of the following options: 1. ssh from your computer’s terminal (for Mac and Linux users) - Type ssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu 3. PuTTY (for Windows users) - See https://wiki.classe.cornell.edu/Computing/WinTunnelVncSSH 5. Use CLASSE’s NoMachine service - Browse to https://nomachine.classe.cornell.edu - Respond to Duo prompt - Click on lnx201 - Create a new desktop or custom session → Create a new virtual desktop - Click on Terminal icon in bottom menu bar - To log out: Applications → Log Out 1. Use CLASSE’s JupyterHub service - Browse to https://jupyterhub.classe.cornell.edu - Respond to Duo prompt - Server Options: Select a job profile: CLASSE Compute Farm, click Start - Launcher tab: Terminal - OR File → New → Terminal - To log out: File → Log Out\n\n\n\nAfter logging in, you will be in your home directory. Start navigating in the same terminal window as before. Run the following commands: 1. pwd - pwd = print working directory. This commend tells you what the current working directory is 2. ls - This command lists the contents of your current working directory 3. ls -la - This command lists the contents of the current working directory with the additional options - -l tells ls to show details about each file’s type, permissions, size, etc. (lowercase “l” = “long”) - -a tells ls to list hidden files, too (“a” = “all”) - Individual options to ls like -l and -a can be shortened to -la - How to read the output of ls -l: https://docs.nersc.gov/filesystems/unix-file-permissions/\n\n\n\nDo not make a habit of working in your home directory /home/&lt;your CLASSE username&gt;!\nInstead, use your CHESS user directory: /nfs/chess/user/&lt;your CLASSE username&gt;.\n\nType cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\ncd = change directory\nIf your CHESS user directory does not exist, first type mkdir /nfs/chess/user/&lt;your CLASSE username&gt;\n\nOR, use the premade symbolic link in your home directory:\n\nType cd /home/&lt;your CLASSE username&gt;/CLASSE_shortcuts/chess_&lt;your CLASSE username&gt;\n\nRepeat Exercise 2 and observe how your CHESS user directory differs from your home directory\nReturn to your home directory\n\nType cd /home/&lt;your CLASSE username&gt;\nOR type cd ~\n\nThe tilde symbol ~ is shorthand for your home directory\n\n\n\n\n\n\nTab completion is an extremely useful feature of the Linux command line that helps you type commands much faster. It works with commands and file paths. Simply start typing one or two characters of a command or file path, then hit the tab key. The command line will automatically fill in additional characters if it can. If it cannot fill in your command or file path all the way, hit the tab key twice to see the available options. (Unfortunately tab completion is not available in the JupyterHub terminal.)\nFor example: 1. Type cd ~/CL, then hit the tab key - The command line should automatically complete what you typed to cd ~/CLASSE_shortcuts/ (unless you have another file or directory that begins with “CL”) - Hit return. Now you are in the CLASSE_shortcuts directory within your home directory - Type ls to see the directory contents 2. Type cd ch, then hit the tab key - This time, the command line can only partially complete what you’ve typed. Hit the tab key twice to show the available options, one of which should be chess_&lt;your CLASSE username&gt;. - Now, type the first letter or two of your CLASSE username and hit tab again. - If the command line was able to complete your username, then hit return to go to your CHESS user directory - If not, keep typing letters followed by tab until your username is complete, then hit return\n\n\n\nIn Linux (UNIX), a pipe (|) is used to connect the output of one command to the input of another. This allows for powerful chaining of small utilities to perform complex tasks, following the UNIX philosophy:\n&gt; “Do one thing well.”\nFor example:\ncommand1 | command2\nmeans “take the output of command1 and pass it as input to command2”.\nFor the following set of exercises please review Linux commands like echo, cat, env, grep, more and less.\n\n\n# print string; the flag -e will properly handle backslash escapes `\\n` \necho -e \"apple\\nbanana\\ncherry\"\n\n# now combine commands via pipe\necho -e \"apple\\nbanana\\ncherry\" | grep 'an'\nWhat will be printed?\nExpected:\nbanana\n\n\n\nenv | grep SHELL\nList all environment variables, then print only the one that specifies your current shell.\n\n\n\nCombine Linux tools introduced above and redirect the output to produce a new file.\n# find who you are\nenv | grep USER\n\n# make new area on /tmp (default temporary area on ANY Linux node)\nmkdir /tmp/$USER\n\n# create new file in /tmp/$USER area\necho \"my data\" &gt; /tmp/$USER/file.dat\n\nWhat would be content of /tmp/$USER/file.dat?\nCan you print content of the /tmp/$USER/file.dat?\n\nHint: cat tool can be used both for viewing and creating files\n# create new file\ncat &gt; /tmp/$USER/file.txt &lt;&lt; EOF\none\ntwo\nthree\nEOF\n\n# view the file\ncat /tmp/$USER/file.txt\n\n# chain together multiple commands with pipe\ncat /tmp/$USER/file.txt | grep t | wc -l\n\n\n\nLinux offer two pagination tools less and more, which can be combine with the pipe concept:\n# use paginators, less and more\nls /etc | less\nUse either tool to view and navigate large files or lengthy output from a command."
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html#exercise-1-logging-in",
    "href": "theme4/XS101/linux-exercises.html#exercise-1-logging-in",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Open a terminal on lnx201 using one of the following options: 1. ssh from your computer’s terminal (for Mac and Linux users) - Type ssh &lt;your CLASSE username&gt;@lnx201.classe.cornell.edu 3. PuTTY (for Windows users) - See https://wiki.classe.cornell.edu/Computing/WinTunnelVncSSH 5. Use CLASSE’s NoMachine service - Browse to https://nomachine.classe.cornell.edu - Respond to Duo prompt - Click on lnx201 - Create a new desktop or custom session → Create a new virtual desktop - Click on Terminal icon in bottom menu bar - To log out: Applications → Log Out 1. Use CLASSE’s JupyterHub service - Browse to https://jupyterhub.classe.cornell.edu - Respond to Duo prompt - Server Options: Select a job profile: CLASSE Compute Farm, click Start - Launcher tab: Terminal - OR File → New → Terminal - To log out: File → Log Out"
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html#exercise-2-basic-commands",
    "href": "theme4/XS101/linux-exercises.html#exercise-2-basic-commands",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "After logging in, you will be in your home directory. Start navigating in the same terminal window as before. Run the following commands: 1. pwd - pwd = print working directory. This commend tells you what the current working directory is 2. ls - This command lists the contents of your current working directory 3. ls -la - This command lists the contents of the current working directory with the additional options - -l tells ls to show details about each file’s type, permissions, size, etc. (lowercase “l” = “long”) - -a tells ls to list hidden files, too (“a” = “all”) - Individual options to ls like -l and -a can be shortened to -la - How to read the output of ls -l: https://docs.nersc.gov/filesystems/unix-file-permissions/"
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html#exercise-3-chess-user-directory",
    "href": "theme4/XS101/linux-exercises.html#exercise-3-chess-user-directory",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Do not make a habit of working in your home directory /home/&lt;your CLASSE username&gt;!\nInstead, use your CHESS user directory: /nfs/chess/user/&lt;your CLASSE username&gt;.\n\nType cd /nfs/chess/user/&lt;your CLASSE username&gt;\n\ncd = change directory\nIf your CHESS user directory does not exist, first type mkdir /nfs/chess/user/&lt;your CLASSE username&gt;\n\nOR, use the premade symbolic link in your home directory:\n\nType cd /home/&lt;your CLASSE username&gt;/CLASSE_shortcuts/chess_&lt;your CLASSE username&gt;\n\nRepeat Exercise 2 and observe how your CHESS user directory differs from your home directory\nReturn to your home directory\n\nType cd /home/&lt;your CLASSE username&gt;\nOR type cd ~\n\nThe tilde symbol ~ is shorthand for your home directory"
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html#exercise-4-tab-completion",
    "href": "theme4/XS101/linux-exercises.html#exercise-4-tab-completion",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "Tab completion is an extremely useful feature of the Linux command line that helps you type commands much faster. It works with commands and file paths. Simply start typing one or two characters of a command or file path, then hit the tab key. The command line will automatically fill in additional characters if it can. If it cannot fill in your command or file path all the way, hit the tab key twice to see the available options. (Unfortunately tab completion is not available in the JupyterHub terminal.)\nFor example: 1. Type cd ~/CL, then hit the tab key - The command line should automatically complete what you typed to cd ~/CLASSE_shortcuts/ (unless you have another file or directory that begins with “CL”) - Hit return. Now you are in the CLASSE_shortcuts directory within your home directory - Type ls to see the directory contents 2. Type cd ch, then hit the tab key - This time, the command line can only partially complete what you’ve typed. Hit the tab key twice to show the available options, one of which should be chess_&lt;your CLASSE username&gt;. - Now, type the first letter or two of your CLASSE username and hit tab again. - If the command line was able to complete your username, then hit return to go to your CHESS user directory - If not, keep typing letters followed by tab until your username is complete, then hit return"
  },
  {
    "objectID": "theme4/XS101/linux-exercises.html#exercise-5-use-pipe-to-combine-commands",
    "href": "theme4/XS101/linux-exercises.html#exercise-5-use-pipe-to-combine-commands",
    "title": "Linux Exercises for CHESS Users",
    "section": "",
    "text": "In Linux (UNIX), a pipe (|) is used to connect the output of one command to the input of another. This allows for powerful chaining of small utilities to perform complex tasks, following the UNIX philosophy:\n&gt; “Do one thing well.”\nFor example:\ncommand1 | command2\nmeans “take the output of command1 and pass it as input to command2”.\nFor the following set of exercises please review Linux commands like echo, cat, env, grep, more and less.\n\n\n# print string; the flag -e will properly handle backslash escapes `\\n` \necho -e \"apple\\nbanana\\ncherry\"\n\n# now combine commands via pipe\necho -e \"apple\\nbanana\\ncherry\" | grep 'an'\nWhat will be printed?\nExpected:\nbanana\n\n\n\nenv | grep SHELL\nList all environment variables, then print only the one that specifies your current shell.\n\n\n\nCombine Linux tools introduced above and redirect the output to produce a new file.\n# find who you are\nenv | grep USER\n\n# make new area on /tmp (default temporary area on ANY Linux node)\nmkdir /tmp/$USER\n\n# create new file in /tmp/$USER area\necho \"my data\" &gt; /tmp/$USER/file.dat\n\nWhat would be content of /tmp/$USER/file.dat?\nCan you print content of the /tmp/$USER/file.dat?\n\nHint: cat tool can be used both for viewing and creating files\n# create new file\ncat &gt; /tmp/$USER/file.txt &lt;&lt; EOF\none\ntwo\nthree\nEOF\n\n# view the file\ncat /tmp/$USER/file.txt\n\n# chain together multiple commands with pipe\ncat /tmp/$USER/file.txt | grep t | wc -l\n\n\n\nLinux offer two pagination tools less and more, which can be combine with the pipe concept:\n# use paginators, less and more\nls /etc | less\nUse either tool to view and navigate large files or lengthy output from a command."
  }
]